[{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/","tags":["K8S","转载"],"title":"01_K8S_概念入门","uri":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/"},{"categories":["转载","K8S"],"content":"01_K8S_概念入门 k8s概念入门 ","date":"2020-10-01","objectID":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/:0:0","tags":["K8S","转载"],"title":"01_K8S_概念入门","uri":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/"},{"categories":["转载","K8S"],"content":"1. 四组基本概念 Pod/Pod控制器 Name/Namespace Lable/Label选择器 Service/Ingress ","date":"2020-10-01","objectID":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/:1:0","tags":["K8S","转载"],"title":"01_K8S_概念入门","uri":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/"},{"categories":["转载","K8S"],"content":"1.1 POD和POD控制器 kubernetes 的pod控制器 Podk8s里能够被运行的最小逻辑单元1个POD里面可以运行多个容器(SideCar 边车模式)POD中的容器共享 UTS/NAT/IPC 名称空间POD和容器颗粒理解为豌豆荚和豌豆 Pod控制器Pod控制器是Pod启动的一种模板用来保证在K8S里启动的Pod始终按预期运行包括副本数\\生命周期\\健康检查等 常用的Pod控制器: 控制器名称 用途简述 Deployment 用于管理无状态应用,支持滚动更新和回滚 DaemonSet 确保集群中的每一个节点上只运行一个特定的pod副本 ReplicaSet 确保pod副本数量符合用户期望的数量状态 StatefulSet 管理有状态应用 Job 有状态，一次性任务 Cronjob(定时任务) 有状态，周期性任务 ","date":"2020-10-01","objectID":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/:1:1","tags":["K8S","转载"],"title":"01_K8S_概念入门","uri":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/"},{"categories":["转载","K8S"],"content":"1.2 Name/Namespace NameK8S使用’资源’来定义每一种逻辑概念(功能)每种’资源’都应该有自己的’名称'‘名称’通常定义在’资源’的元数据(metadata)信息中 资源的配置信息包括: API版本(apiVersion) 类别(kind) 元数据(metadata) 定义清单(spec) 状态(status) Namespace名称空间用于隔离K8S内各种资源,类似K8S内部的虚拟分组同一个名称空间中,相同资源的名称不能相同默认的名称空间为default，kube-system，kube-public查询特定资源,要带上相应的名称空间 ","date":"2020-10-01","objectID":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/:1:2","tags":["K8S","转载"],"title":"01_K8S_概念入门","uri":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/"},{"categories":["转载","K8S"],"content":"1.3 Lable/Label选择器 Lable标签的作用是便于分类管理资源对象标签与资源之间是多对多的关系给一个资源多个标签,可以实现不同维度的管理 Lable选择器可以使用标签选择器过滤指定的标签标签选择器有基于等值关系(等于,不等于)和基于集合关系(属于,存在)的两种许多资源都支持内嵌标签选择器字段:matchLables或matchExpressions ","date":"2020-10-01","objectID":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/:1:3","tags":["K8S","转载"],"title":"01_K8S_概念入门","uri":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/"},{"categories":["转载","K8S"],"content":"1.4 Service/Ingress Service(重点)：POD会分配IP地址,但IP会随着POD销毁而消失多个同类型POD,IP或端口必然不同,但却相同的服务Service用来提供相同服务POD的对外访问接口Service通过标签选择器来确定作用于哪些PODService只能提供L4层的调度,即:IP+端口 Ingress(重点)：Ingress也是用来暴露POD的对外访问接口Igress提供L7层的调度,即http/httpsIgress可以调度不同业务域,不同URL路径的流量 ","date":"2020-10-01","objectID":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/:1:4","tags":["K8S","转载"],"title":"01_K8S_概念入门","uri":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/"},{"categories":["转载","K8S"],"content":"2. 核心组件与核心附件 核心组件配置存储中心 etcd服务 主控节点（master） kube-apiserver服务 kube-controller-manager服务 kube-scheduler服务 运算节点（node） kube-kubelet服务 kube-proxy服务 CLI客户端kubectl命令行工具 核心附件CNI网络插件（flannel/calico） 服务发现插件（coredns） 服务暴露插件（traefik） GUI管理插件（dashboard） ","date":"2020-10-01","objectID":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/:2:0","tags":["K8S","转载"],"title":"01_K8S_概念入门","uri":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/"},{"categories":["转载","K8S"],"content":"2.1 核心组件功能 配置存储中心-etcdetcd是一个非关系型数据库，作用类似于zookeeper注册中心用于各种服务的注册和数据缓存 kube-apiserver(master)提供季军管理的REST API接口，包括鉴权、数据校验、集群状态变更负责其他模块之间的数据交互，承担通信枢纽的功能和etcd通信，是资源配额控制的入口提供玩备的集群控制机制 kube-controller-manager由一系列控制器组成,通过apiserver监控整个集群的状态,确保集群处于预期的工作状态是管理所有控制器的控制器 kube-scheduler主要是接收调度POD到合适的node节点上通过apiserver，从etcd中获取资源信息进行调度只负责调度工作，启动工作是node节点上的kubelet负责调度策略：预算策略（predict）、优选策略（priorities） kube-kubelet定时从apiserver获取节点上POD的期望状态（如副本数量、网络类型、存储空间、容器类型等）然后调用容器平台接口达到这个状态提供POD节点具体使用的网络定时汇报当前节点状态给apiserver，以供调度复制镜像和容器的创建和清理工作 kube-proxy是K8S在每个节点上运行网络的代理，service资源的载体不直接为POD节点提供网络,而是提供POD间的集群网络建立了POD网络和集群网络的关系（clusterIp-\u003epodIp)负责建立、删除、更新调度规则与apiserver通信，以更新自己和获取其他kube-proxy的的调度规则常用的调度模式：Iptables(不推荐)、Ipvs(推荐) ","date":"2020-10-01","objectID":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/:2:1","tags":["K8S","转载"],"title":"01_K8S_概念入门","uri":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/"},{"categories":["转载","K8S"],"content":"2.2 K8S的三条网络 节点网络实际网络，就是宿主机网络建议地址段：﻿10.4.7.0/24﻿建议通过不同的IP端,区分不同的业务、机房或数据中心 Pod 网络实际网络，容器运行的网络建议﻿172.7.21.0/24﻿ ,并建议POD网段与节点IP绑定如: 节点IP为﻿10.4.7.21﻿，则POD网络为﻿172.7.21.0/24﻿ service网络虚拟网络，也叫集群网络(cluster server),用于内部集群间通信构建于POD网络之上, 主要是解决服务发现和负载均衡通过kube-proxy连接POD网络和service网络建议地址段为：﻿192.168.0.0/16﻿ ","date":"2020-10-01","objectID":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/:2:2","tags":["K8S","转载"],"title":"01_K8S_概念入门","uri":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/"},{"categories":["转载","K8S"],"content":"3. K8S流程图 说明: 主控节点和node节点只是逻辑上的概念,物理上可以部署在一起 ","date":"2020-10-01","objectID":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/:3:0","tags":["K8S","转载"],"title":"01_K8S_概念入门","uri":"/01_k8s_%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/"},{"categories":["K8S","转载"],"content":"02_K8S二进制部署实践-1.15.5 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:0:0","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"1. 部署架构 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:1:0","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"1.1 架构图 架构说明: etcd至少3台组成一个高可用集群 两台proxy组成高可用代理对外提供VIP 两台机器共同承担master和node节点功能 运维主机非K8S套件,但为K8S服务 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:1:1","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"1.2 安装方式选择 Minikube 预览使用,仅供学习 二进制安装(生产首选,新手推荐) kubeadmin安装 简单,用k8s跑k8s自己,熟手推荐 新手不推荐的原因是容易知其然不知其所以然 出问题后找不到解决办法 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:1:2","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"2. 部署准备 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:2:0","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"2.1 准备工作 准备5台2C/2g/50g虚拟机,网络10.4.7.0/24 预装centos7.4,做完基础优化 安装部署bind9,部署自建DNS系统 准备自签证书环境 安装部署docker和harbor仓库 机器列表 主机名 IP地址 用途 hdss7-11 10.4.7.11 proxy1 hdss7-12 10.4.7.12 proxy2 hdss7-21 10.4.7.21 master1 hdss7-22 10.4.7.22 master2 hdss7-200 10.4.7.200 运维主机 基本部署软件 [root@hdss7-11 ~]# hostname hdss7-11 [root@hdss7-11 ~]# getenforce Disabled [root@hdss7-11 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 TYPE=Ethernet BOOTPROTO=none NAME=eth0 DEVICE=eth0 ONBOOT=yes IPADDR=10.4.7.11 NETMASK=255.255.255.0 GATEWAY=10.4.7.254 DNS1=10.4.7.254 [root@hdss7-11 ~]# yum install wget net-tools telnet tree nmap sysstat lrzsz dos2unix -y ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:2:1","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"2.2 部署DNS服务bind9 2.2.1 安装配置DNS服务 在7.11上部署bind的DNS服务 yum install bind bind-utils -y 修改并校验配置文件 [root@hdss7-11 ~]# vim /etc/named.conf listen-on port 53 { 10.4.7.11; }; allow-query { any; }; forwarders { 10.4.7.254; }; #上一层DNS地址(网关或公网DNS) recursion yes; dnssec-enable no; dnssec-validation no [root@hdss7-11 ~]# named-checkconf 2.2.2 增加自定义域和对于配置 在域配置中增加自定义域 cat \u003e\u003e/etc/named.rfc1912.zones \u003c\u003c'EOF' # 添加自定义主机域 zone \"host.com\" IN { type master; file \"host.com.zone\"; allow-update { 10.4.7.11; }; }; # 添加自定义业务域 zone \"zq.com\" IN { type master; file \"zq.com.zone\"; allow-update { 10.4.7.11; }; }; EOF host.com和zq.com都是我们自定义的域名,一般用host.com做为主机域 zq.com为业务域,业务不同可以配置多个 为自定义域host.com创建配置文件 cat \u003e/var/named/host.com.zone \u003c\u003c'EOF' $ORIGIN host.com. $TTL 600 ; 10 minutes @ IN SOA dns.host.com. dnsadmin.host.com. ( 2020041601 ; serial 10800 ; refresh (3 hours) 900 ; retry (15 minutes) 604800 ; expire (1 week) 86400 ; minimum (1 day) ) NS dns.host.com. $TTL 60 ; 1 minute dns A 10.4.7.11 HDSS7-11 A 10.4.7.11 HDSS7-12 A 10.4.7.12 HDSS7-21 A 10.4.7.21 HDSS7-22 A 10.4.7.22 HDSS7-200 A 10.4.7.200 EOF 为自定义域zq.com创建配置文件 cat \u003e/var/named/zq.com.zone \u003c\u003c'EOF' $ORIGIN zq.com. $TTL 600 ; 10 minutes @ IN SOA dns.zq.com. dnsadmin.zq.com. ( 2020041601 ; serial 10800 ; refresh (3 hours) 900 ; retry (15 minutes) 604800 ; expire (1 week) 86400 ; minimum (1 day) ) NS dns.zq.com. $TTL 60 ; 1 minute dns A 10.4.7.11 EOF host.com域用于主机之间通信,所以要先增加上所有主机 zq.com域用于后面的业务解析用,因此不需要先添加主机 2.2.3 启动并验证DNS服务 再次检查配置并启动dns服务 [root@hdss7-11 ~]# named-checkconf [root@hdss7-11 ~]# systemctl start named [root@hdss7-11 ~]# ss -lntup|grep 53 udp UNCONN 0 0 10.4.7.11:53 udp UNCONN 0 0 :::53 tcp LISTEN 0 10 10.4.7.11:53 tcp LISTEN 0 128 127.0.0.1:953 tcp LISTEN 0 10 :::53 tcp LISTEN 0 128 ::1:953 # 验证结果 [root@hdss7-11 ~]# dig -t A hdss7-11.host.com @10.4.7.11 +short 10.4.7.11 [root@hdss7-11 ~]# dig -t A hdss7-21.host.com @10.4.7.11 +short 10.4.7.21 2.2.4 所有主机修改网络配置 5台K8S主机都需要按如下方式修改网络配置 # 修改dns并添加搜索域 sed -i 's#^DNS.*#DNS1=10.4.7.11#g' /etc/sysconfig/network-scripts/ifcfg-eth0 echo \"search=host.com\" \u003e\u003e/etc/sysconfig/network-scripts/ifcfg-eth0 systemctl restart network # 检查DNS配置 ~]# cat /etc/resolv.conf # Generated by NetworkManager search host.com nameserver 10.4.7.11 ~]# dig -t A hdss7-21.host.com +short 10.4.7.21 # 一定记得检查dns配置文件中是否有search信息 windows宿主机也要改 wmnet8网卡更改DNS：10.4.7.11 # ping通才行,否则检查 ping hdss7-200.host.com ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:2:2","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"2.3 自签发证书环境准备 操作在7.200这个运维机上完成 2.3.1 下载安装cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/bin/cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/bin/cfssl-json wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/bin/cfssl-certinfo chmod +x /usr/bin/cfssl* 2.3.2 生成ca证书文件 mkdir /opt/certs cat \u003e/opt/certs/ca-csr.json \u003c\u003cEOF { \"CN\": \"zqcd\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"chengdu\", \"L\": \"chengdu\", \"O\": \"zq\", \"OU\": \"ops\" } ], \"ca\": { \"expiry\": \"175200h\" } } EOF CN: Common Name，浏览器使用该字段验证网站是否合法，一般写的是域名。非常重要。浏览器使用该字段验证网站是否合法 C: Country， 国家 ST: State，州，省 L: Locality，地区，城市 O: Organization Name，组织名称，公司名称 OU: Organization Unit Name，组织单位名称，公司部门 2.3.3 生成ca证书 cd /opt/certs cfssl gencert -initca ca-csr.json | cfssl-json -bare ca [root@hdss7-200 certs]# ll total 16 -rw-r--r-- 1 root root 989 Apr 16 20:53 cacsr -rw-r--r-- 1 root root 324 Apr 16 20:52 ca-csr.json -rw------- 1 root root 1679 Apr 16 20:53 ca-key.pem -rw-r--r-- 1 root root 1330 Apr 16 20:53 ca.pem ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:2:3","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"2.4 docker环境准备 2.4.1 安装并配置docker curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun mkdir /etc/docker/ cat \u003e/etc/docker/daemon.json \u003c\u003cEOF { \"graph\": \"/data/docker\", \"storage-driver\": \"overlay2\", \"insecure-registries\": [\"registry.access.redhat.com\",\"quay.io\",\"harbor.zq.com\"], \"registry-mirrors\": [\"https://q2gr04ke.mirror.aliyuncs.com\"], \"bip\": \"172.7.21.1/24\", \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"live-restore\": true } EOF 注意:bip要根据宿主机ip变化 hdss7-21.host.com bip 172.7.21.1/24 hdss7-22.host.com bip 172.7.22.1/24 hdss7-200.host.com bip 172.7.200.1/24 2.4.2 启动docker mkdir -p /data/docker systemctl start docker systemctl enable docker docker --version ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:2:4","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"2.5 部署harbor私有仓库 下载地址:https://github.com/goharbor/harbor/releases/download/v1.8.5/harbor-offline-installer-v1.8.5.tgz 2.5.1 下载并解压 tar xf harbor-offline-installer-v1.8.5.tgz -C /opt/ cd /opt/ mv harbor/ harbor-v1.8.5 ln -s /opt/harbor-v1.8.5/ /opt/harbor 2.5.2 编辑配置文件 [root@hdss7-200 opt]# vi /opt/harbor/harbor.yml # 以下是修改项,手动在配置文件中更改 hostname: harbor.zq.com http: port: 180 harbor_admin_password:Harbor12345 data_volume: /data/harbor log: level: info rotate_count: 50 rotate_size:200M location: /data/harbor/logs [root@hdss7-200 opt]# mkdir -p /data/harbor/logs 2.5.3 使用docker-compose启动harbor [root@hdss7-200 opt]cd /opt/harbor/ yum install docker-compose -y sh /opt/harbor/install.sh docker-compose ps docker ps -a 2.5.4 使用dns解析harbor 在7.11DNS服务上操作 [root@hdss7-11 ~]# vi /var/named/zq.com.zone 2020032002 ; serial #每次修改DNS解析后,都要滚动此ID harbor A 10.4.7.200 [root@hdss7-11 ~]# systemctl restart named [root@hdss7-11 ~]# dig -t A harbor.zq.com +short 10.4.7.200 2.5.5 使用nginx反向代理harbor 回到7.200运维机上操作 [root@hdss7-200 harbor]# yum install nginx -y [root@hdss7-200 harbor]# vi /etc/nginx/conf.d/harbor.zq.com.conf server { listen 80; server_name harbor.zq.com; client_max_body_size 1000m; location / { proxy_pass http://127.0.0.1:180; } } [root@hdss7-200 harbor]# nginx -t [root@hdss7-200 harbor]# systemctl start nginx [root@hdss7-200 harbor]# systemctl enable nginx 浏览器输入：harbor.zq.com 用户名：admin 密码：Harbor12345 新建项目：public 访问级别：公开 2.5.6 提前准备pauser/nginx基础镜像 pauser镜像是k8s启动pod时,预先用来创建相关资源(如名称空间)的 nginx镜像是k8s部署好以后,我们测试pod创建所用的 docker login harbor.zq.com -uadmin -pHarbor12345 docker pull kubernetes/pause docker pull nginx:1.17.9 docker tag kubernetes/pause:latest harbor.zq.com/public/pause:latest docker tag nginx:1.17.9 harbor.zq.com/public/nginx:v1.17.9 docker push harbor.zq.com/public/pause:latest docker push harbor.zq.com/public/nginx:v1.17.9 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:2:5","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"2.6 准备nginx文件服务 创建一个nginx虚拟主机,用来提供文件访问访问,主要依赖nginx的autoindex属性 2.6.1 创建文件访问 在7.200上 # 创建配置 cat \u003e/etc/nginx/conf.d/k8s-yaml.zq.com.conf \u003c\u003cEOF server { listen 80; server_name k8s-yaml.zq.com; location / { autoindex on; default_type text/plain; root /data/k8s-yaml; } } EOF # 启动nginx mkdir -p /data/k8s-yaml/coredns nginx -t nginx -s reload 2.6.2 添加域名解析 在7.11的bind9域名服务器上,增加DNS记录 vi /var/named/zq.com.zone # 在最后添加一条解析记录 k8s-yaml A 10.4.7.200 # 同时滚动serial为 @ IN SOA dns.zq.com. dnsadmin.zq.com. ( 2019061803 ; serial 重启服务并验证: systemctl restart named [root@hdss7-11 ~]# dig -t A k8s-yaml.zq.com +short 10.4.7.200 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:2:6","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"3. 部署master节点-etcd服务 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:3:0","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"3.1 部署etcd集群 分别在12/21/22 上安装ectd服务,11节点作为备选节点 3.1.1 创建生成CA证书的JSON配置文件 在7.200上操作 一个配置里面包含了server端,clinet端和双向(peer)通信所需要的配置,后面创建证书的时候会传入不同的参数调用不同的配置 cat \u003e/opt/certs/ca-config.json \u003c\u003cEOF { \"signing\": { \"default\": { \"expiry\": \"175200h\" }, \"profiles\": { \"server\": { \"expiry\": \"175200h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\" ] }, \"client\": { \"expiry\": \"175200h\", \"usages\": [ \"signing\", \"key encipherment\", \"client auth\" ] }, \"peer\": { \"expiry\": \"175200h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ] } } } } EOF 证书时间统一为10年,不怕过期 证书类型 client certificate：客户端使用，用于服务端认证客户端,例如etcdctl、etcd proxy、fleetctl、docker客户端 server certificate：服务端使用，客户端以此验证服务端身份,例如docker服务端、kube-apiserver peer certificate：双向证书，用于etcd集群成员间通信 3.1.3.创建生成自签发请求(csr)的json配置文件 注意: 需要将所有可能用来部署etcd的机器,都加入到hosts列表中 否则后期重新加入不在列表中的机器,需要更换所有etcd服务的证书 cat \u003e/opt/certs/etcd-peer-csr.json \u003c\u003cEOF { \"CN\": \"k8s-etcd\", \"hosts\": [ \"127.0.0.1\", \"10.4.7.11\", \"10.4.7.12\", \"10.4.7.21\", \"10.4.7.22\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"beijing\", \"L\": \"beijing\", \"O\": \"zq\", \"OU\": \"ops\" } ] } EOF 3.1.4.生成etcd证书文件 cd /opt/certs/ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \\ -config=ca-config.json -profile=peer \\ etcd-peer-csr.json |cfssl-json -bare etcd-peer [root@hdss7-200 certs]# ll total 36 -rw-r--r-- 1 root root 837 Apr 19 15:35 ca-config.json -rw-r--r-- 1 root root 989 Apr 16 20:53 ca.csr -rw-r--r-- 1 root root 324 Apr 16 20:52 ca-csr.json -rw------- 1 root root 1679 Apr 16 20:53 ca-key.pem -rw-r--r-- 1 root root 1330 Apr 16 20:53 ca.pem -rw-r--r-- 1 root root 1062 Apr 19 15:35 etcd-peer.csr -rw-r--r-- 1 root root 363 Apr 19 15:35 etcd-peer-csr.json -rw------- 1 root root 1679 Apr 19 15:35 etcd-peer-key.pem -rw-r--r-- 1 root root 1419 Apr 19 15:35 etcd-peer.pem ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:3:1","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"3.2 安装启动etcd集群 以7.12做为演示,另外2台机器大同小异,不相同的配置都会特别说明 3.2.1 创建etcd用户和安装软件 etcd地址:https://github.com/etcd-io/etcd/tags 建议使用3.1版本,更高版本有问题 useradd -s /sbin/nologin -M etcd wget https://github.com/etcd-io/etcd/archive/v3.1.20.tar.gz tar xf etcd-v3.1.20-linux-amd64.tar.gz -C /opt/ cd /opt/ mv etcd-v3.1.20-linux-amd64/ etcd-v3.1.20 ln -s /opt/etcd-v3.1.20/ /opt/etcd 3.2.2 创建目录，拷贝证书文件 创建证书目录、数据目录、日志目录 mkdir -p /opt/etcd/certs /data/etcd /data/logs/etcd-server chown -R etcd.etcd /opt/etcd-v3.1.20/ chown -R etcd.etcd /data/etcd/ chown -R etcd.etcd /data/logs/etcd-server/ 拷贝生成的证书文件 cd /opt/etcd/certs scp hdss7-200:/opt/certs/ca.pem . scp hdss7-200:/opt/certs/etcd-peer.pem . scp hdss7-200:/opt/certs/etcd-peer-key.pem . chown -R etcd.etcd /opt/etcd/certs 也可以先创建一个NFS,直接从NFS中拷贝 3.2.3 创建etcd服务启动脚本 参数说明: https://blog.csdn.net/kmhysoft/article/details/71106995 cat \u003e/opt/etcd/etcd-server-startup.sh \u003c\u003c'EOF' #!/bin/sh ./etcd \\ --name etcd-server-7-12 \\ --data-dir /data/etcd/etcd-server \\ --listen-peer-urls https://10.4.7.12:2380 \\ --listen-client-urls https://10.4.7.12:2379,http://127.0.0.1:2379 \\ --quota-backend-bytes 8000000000 \\ --initial-advertise-peer-urls https://10.4.7.12:2380 \\ --advertise-client-urls https://10.4.7.12:2379,http://127.0.0.1:2379 \\ --initial-cluster etcd-server-7-12=https://10.4.7.12:2380,etcd-server-7-21=https://10.4.7.21:2380,etcd-server-7-22=https://10.4.7.22:2380 \\ --ca-file ./certs/ca.pem \\ --cert-file ./certs/etcd-peer.pem \\ --key-file ./certs/etcd-peer-key.pem \\ --client-cert-auth \\ --trusted-ca-file ./certs/ca.pem \\ --peer-ca-file ./certs/ca.pem \\ --peer-cert-file ./certs/etcd-peer.pem \\ --peer-key-file ./certs/etcd-peer-key.pem \\ --peer-client-cert-auth \\ --peer-trusted-ca-file ./certs/ca.pem \\ --log-output stdout EOF [root@hdss7-12 ~]# chmod +x /opt/etcd/etcd-server-startup.sh 注意:以上启动脚本,有几个配置项在每个服务器都有所不同 --name #节点名字 --listen-peer-urls #监听其他节点所用的地址 --listen-client-urls #监听etcd客户端的地址 --initial-advertise-peer-urls #与其他节点交互信息的地址 --advertise-client-urls #与etcd客户端交互信息的地址 3.2.4 使用supervisor启动etcd 安装supervisor软件 yum install supervisor -y systemctl start supervisord systemctl enable supervisord 创建supervisor管理etcd的配置文件 配置说明参考: https://www.jianshu.com/p/53b5737534e8 cat \u003e/etc/supervisord.d/etcd-server.ini \u003c\u003cEOF [program:etcd-server] ; 显示的程序名,类型my.cnf,可以有多个 command=sh /opt/etcd/etcd-server-startup.sh numprocs=1 ; 启动进程数 (def 1) directory=/opt/etcd ; 启动命令前切换的目录 (def no cwd) autostart=true ; 是否自启 (default: true) autorestart=true ; 是否自动重启 (default: true) startsecs=30 ; 服务运行多久判断为成功(def. 1) startretries=3 ; 启动重试次数 (default 3) exitcodes=0,2 ; 退出状态码 (default 0,2) stopsignal=QUIT ; 退出信号 (default TERM) stopwaitsecs=10 ; 退出延迟时间 (default 10) user=etcd ; 运行用户 redirect_stderr=true ; 是否重定向错误输出到标准输出(def false) stdout_logfile=/data/logs/etcd-server/etcd.stdout.log stdout_logfile_maxbytes=64MB ; 日志文件大小 (default 50MB) stdout_logfile_backups=4 ; 日志文件滚动个数 (default 10) stdout_capture_maxbytes=1MB ; 设定capture管道的大小(default 0) ;子进程还有子进程,需要添加这个参数,避免产生孤儿进程 killasgroup=true stopasgroup=true EOF 启动etcd服务并检查 supervisorctl update supervisorctl status netstat -lntup|grep etcd 3.2.5 部署启动集群其他机器 略 3.2.6 检查集群状态 [root@hdss7-12 certs]# /opt/etcd/etcdctl cluster-health member 988139385f78284 is healthy: got healthy result from http://127.0.0.1:2379 member 5a0ef2a004fc4349 is healthy: got healthy result from http://127.0.0.1:2379 member f4a0cb0a765574a8 is healthy: got healthy result from http://127.0.0.1:2379 [root@hdss7-12 certs]# /opt/etcd/etcdctl member list 988139385f78284: name=etcd-server-7-22 peerURLs=https://10.4.7.22:2380 clientURLs=http://127.0.0.1:2379,https://10.4.7.22:2379 isLeader=false 5a0ef2a004fc4349: name=etcd-server-7-21 peerURLs=https://10.4.7.21:2380 clientURLs=http://127.0.0.1:2379,https://10.4.7.21:2379 isLeader=false f4a0cb0a765574a8: name=etcd-server-7-12 peerURLs=https://10.4.7.12:2380 clientURLs=http://127.0.0.1:2379,https://10.4.7.12:2379 isLeader=true ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:3:2","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"4. 部署mater节点 kube-apiserver服务 下载页面: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.15.md 下载地址: https://dl.k8s.io/v1.15.5/kubernetes-server-linux-amd64.tar.gz https://dl.k8s.io/v1.15.5/kubernetes-client-linux-amd64.tar.gz https://dl.k8s.io/v1.15.5/kubernetes-node-linux-amd64.tar.gz ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:4:0","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"4.1 签发client端证书 证书签发都在7.200上操作 此证书的用途是apiserver和etcd之间通信所用 4.1.1 创建生成证书csr的json配置文件 cat \u003e/opt/certs/client-csr.json \u003c\u003cEOF { \"CN\": \"k8s-node\", \"hosts\": [ ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"beijing\", \"L\": \"beijing\", \"O\": \"zq\", \"OU\": \"ops\" } ] } EOF 4.1.2 生成client证书文件 cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=client \\ client-csr.json |cfssl-json -bare client [root@hdss7-200 certs]# ll|grep client -rw-r--r-- 1 root root 993 Apr 20 21:30 client.csr -rw-r--r-- 1 root root 280 Apr 20 21:30 client-csr.json -rw------- 1 root root 1675 Apr 20 21:30 client-key.pem -rw-r--r-- 1 root root 1359 Apr 20 21:30 client.pem ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:4:1","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"4.2 签发kube-apiserver证书 此证书的用途是apiserver对外提供的服务的证书 4.2.1 创建生成证书csr的json配置文件 此配置中的hosts包含所有可能会部署apiserver的列表 其中10.4.7.10是反向代理的vip地址 cat \u003e/opt/certs/apiserver-csr.json \u003c\u003cEOF { \"CN\": \"k8s-apiserver\", \"hosts\": [ \"127.0.0.1\", \"192.168.0.1\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\", \"10.4.7.10\", \"10.4.7.21\", \"10.4.7.22\", \"10.4.7.23\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"beijing\", \"L\": \"beijing\", \"O\": \"zq\", \"OU\": \"ops\" } ] } EOF 4.2.2 生成kube-apiserver证书文件 cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=server \\ apiserver-csr.json |cfssl-json -bare apiserver [root@hdss7-200 certs]# ll|grep apiserver -rw-r--r-- 1 root root 1249 Apr 20 21:31 apiserver.csr -rw-r--r-- 1 root root 566 Apr 20 21:31 apiserver-csr.json -rw------- 1 root root 1675 Apr 20 21:31 apiserver-key.pem -rw-r--r-- 1 root root 1590 Apr 20 21:31 apiserver.pem ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:4:2","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"4.3 下载安装kube-apiserver 以7.21为例 # 上传并解压缩 tar xf kubernetes-server-linux-amd64-v1.15.2.tar.gz -C /opt cd /opt mv kubernetes/ kubernetes-v1.15.2 ln -s /opt/kubernetes-v1.15.2/ /opt/kubernetes # 清理源码包和docker镜像 cd /opt/kubernetes rm -rf kubernetes-src.tar.gz cd server/bin rm -f *.tar rm -f *_tag # 创建命令软连接到系统环境变量下 ln -s /opt/kubernetes/server/bin/kubectl /usr/bin/kubectl ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:4:3","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"4.4 部署apiserver服务 4.4.1 拷贝证书文件 拷贝证书文件到/opt/kubernetes/server/bin/cert目录下 # 创建目录 mkdir -p /opt/kubernetes/server/bin/cert cd /opt/kubernetes/server/bin/cert # 拷贝三套证书 scp hdss7-200:/opt/certs/ca.pem . scp hdss7-200:/opt/certs/ca-key.pem . scp hdss7-200:/opt/certs/client.pem . scp hdss7-200:/opt/certs/client-key.pem . scp hdss7-200:/opt/certs/apiserver.pem . scp hdss7-200:/opt/certs/apiserver-key.pem . 4.4.2 创建audit配置 audit日志审计规则配置是k8s要求必须要有得配置,可以不理解,直接用 mkdir /opt/kubernetes/server/conf cat \u003e/opt/kubernetes/server/conf/audit.yaml \u003c\u003c'EOF' apiVersion: audit.k8s.io/v1beta1 # This is required. kind: Policy # Don't generate audit events for all requests in RequestReceived stage. omitStages: - \"RequestReceived\" rules: # Log pod changes at RequestResponse level - level: RequestResponse resources: - group: \"\" # Resource \"pods\" doesn't match requests to any subresource of pods, # which is consistent with the RBAC policy. resources: [\"pods\"] # Log \"pods/log\", \"pods/status\" at Metadata level - level: Metadata resources: - group: \"\" resources: [\"pods/log\", \"pods/status\"] # Don't log requests to a configmap called \"controller-leader\" - level: None resources: - group: \"\" resources: [\"configmaps\"] resourceNames: [\"controller-leader\"] # Don't log watch requests by the \"system:kube-proxy\" on endpoints or services - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" # core API group resources: [\"endpoints\", \"services\"] # Don't log authenticated requests to certain non-resource URL paths. - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" # Log the request body of configmap changes in kube-system. - level: Request resources: - group: \"\" # core API group resources: [\"configmaps\"] # This rule only applies to resources in the \"kube-system\" namespace. # The empty string \"\" can be used to select non-namespaced resources. namespaces: [\"kube-system\"] # Log configmap and secret changes in all other namespaces at the Metadata level. - level: Metadata resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] # Log all other resources in core and extensions at the Request level. - level: Request resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. # A catch-all rule to log all other requests at the Metadata level. - level: Metadata # Long-running requests like watches that fall under this rule will not # generate an audit event in RequestReceived. omitStages: - \"RequestReceived\" EOF 4.4.3 创建apiserver启动脚本 cat \u003e/opt/kubernetes/server/bin/kube-apiserver.sh \u003c\u003c'EOF' #!/bin/bash ./kube-apiserver \\ --apiserver-count 2 \\ --audit-log-path /data/logs/kubernetes/kube-apiserver/audit-log \\ --audit-policy-file ../conf/audit.yaml \\ --authorization-mode RBAC \\ --client-ca-file ./cert/ca.pem \\ --requestheader-client-ca-file ./cert/ca.pem \\ --enable-admission-plugins NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota \\ --etcd-cafile ./cert/ca.pem \\ --etcd-certfile ./cert/client.pem \\ --etcd-keyfile ./cert/client-key.pem \\ --etcd-servers https://10.4.7.12:2379,https://10.4.7.21:2379,https://10.4.7.22:2379 \\ --service-account-key-file ./cert/ca-key.pem \\ --service-cluster-ip-range 192.168.0.0/16 \\ --service-node-port-range 3000-29999 \\ --target-ram-mb=1024 \\ --kubelet-client-certificate ./cert/client.pem \\ --kubelet-client-key ./cert/client-key.pem \\ --log-dir /data/logs/kubernetes/kube-apiserver \\ --tls-cert-file ./cert/apiserver.pem \\ --tls-private-key-file ./cert/apiserver-key.pem \\ --v 2 EOF # 授权 chmod +x /opt/kubernetes/server/bin/kube-apiserver.sh 4.4.4 创建supervisor启动apiserver的配置 安装supervisor软件 yum install supervisor -y systemctl start supervisord systemctl enable supervisord cat \u003e/etc/supervisord.d/kube-apiserver.ini \u003c\u003cEOF [program:kube-apiserver] ; 显示的程序名,类似my.cnf,可以有多个 command=sh /opt/kubernetes/server/bi","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:4:4","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"4.5 部署controller-manager服务 apiserve、controller-manager、kube-scheduler三个服务所需的软件在同一套压缩包里面的，因此后两个服务不需要在单独解包 而且这三个服务是在同一个主机上，互相之间通过http://127.0.0.1,也不需要证书 4.5.1 创建controller-manager启动脚本 cat \u003e/opt/kubernetes/server/bin/kube-controller-manager.sh \u003c\u003c'EOF' #!/bin/sh ./kube-controller-manager \\ --cluster-cidr 172.7.0.0/16 \\ --leader-elect true \\ --log-dir /data/logs/kubernetes/kube-controller-manager \\ --master http://127.0.0.1:8080 \\ --service-account-private-key-file ./cert/ca-key.pem \\ --service-cluster-ip-range 192.168.0.0/16 \\ --root-ca-file ./cert/ca.pem \\ --v 2 EOF # 授权 chmod +x /opt/kubernetes/server/bin/kube-controller-manager.sh 4.5.2 创建supervisor配置 cat \u003e/etc/supervisord.d/kube-conntroller-manager.ini \u003c\u003cEOF [program:kube-controller-manager] ; 显示的程序名 command=sh /opt/kubernetes/server/bin/kube-controller-manager.sh numprocs=1 ; 启动进程数 (def 1) directory=/opt/kubernetes/server/bin autostart=true ; 是否自启 (default: true) autorestart=true ; 是否自动重启 (default: true) startsecs=30 ; 服务运行多久判断为成功(def. 1) startretries=3 ; 启动重试次数 (default 3) exitcodes=0,2 ; 退出状态码 (default 0,2) stopsignal=QUIT ; 退出信号 (default TERM) stopwaitsecs=10 ; 退出延迟时间 (default 10) user=root ; 运行用户 redirect_stderr=true ; 重定向错误输出到标准输出(def false) stdout_logfile=/data/logs/kubernetes/kube-controller-manager/controller.stdout.log stdout_logfile_maxbytes=64MB ; 日志文件大小 (default 50MB) stdout_logfile_backups=4 ; 日志文件滚动个数 (default 10) stdout_capture_maxbytes=1MB ; 设定capture管道的大小(default 0) ;子进程还有子进程,需要添加这个参数,避免产生孤儿进程 killasgroup=true stopasgroup=true EOF 4.5.3 启动服务并检查 mkdir -p /data/logs/kubernetes/kube-controller-manager supervisorctl update supervisorctl status 4.5.4 部署启动所有集群 没有不同的地方,所以略 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:4:5","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"4.6 部署kube-scheduler服务 4.6.1 创建启动脚本 cat \u003e/opt/kubernetes/server/bin/kube-scheduler.sh \u003c\u003c'EOF' #!/bin/sh ./kube-scheduler \\ --leader-elect \\ --log-dir /data/logs/kubernetes/kube-scheduler \\ --master http://127.0.0.1:8080 \\ --v 2 EOF # 授权 chmod +x /opt/kubernetes/server/bin/kube-scheduler.sh 4.6.2 创建supervisor配置 cat \u003e/etc/supervisord.d/kube-scheduler.ini \u003c\u003cEOF [program:kube-scheduler] command=sh /opt/kubernetes/server/bin/kube-scheduler.sh numprocs=1 ; 启动进程数 (def 1) directory=/opt/kubernetes/server/bin autostart=true ; 是否自启 (default: true) autorestart=true ; 是否自动重启 (default: true) startsecs=30 ; 服务运行多久判断为成功(def. 1) startretries=3 ; 启动重试次数 (default 3) exitcodes=0,2 ; 退出状态码 (default 0,2) stopsignal=QUIT ; 退出信号 (default TERM) stopwaitsecs=10 ; 退出延迟时间 (default 10) user=root ; 运行用户 redirect_stderr=true ; 重定向错误输出到标准输出(def false) stdout_logfile=/data/logs/kubernetes/kube-scheduler/scheduler.stdout.log stdout_logfile_maxbytes=64MB ; 日志文件大小 (default 50MB) stdout_logfile_backups=4 ; 日志文件滚动个数 (default 10) stdout_capture_maxbytes=1MB ; 设定capture管道的大小(default 0) ;子进程还有子进程,需要添加这个参数,避免产生孤儿进程 killasgroup=true stopasgroup=true EOF 4.6.3 启动服务并检查 mkdir -p /data/logs/kubernetes/kube-scheduler supervisorctl update supervisorctl status 4.6.4 部署启动所有集群 没有不同的地方,所以略 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:4:6","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"4.7 检查master节点部署情况 [root@hdss7-21 bin]# kubectl get cs NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-1 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:4:7","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"5. 部署4层反代去代理apiserver master节点上的3套服务部署完成后,需要使用反向代理去统一两个apiservser的对外端口 这里使用nginx+keepalived的高可用架构部署在7.11和7.12两台机器上 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:5:0","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"5.1 部署nginx四层反代 使用7443端口代理apiserver的6443端口,使用keepalived管理VIP10.4.7.10 5.1.1 yum安装程序 yum install nginx keepalived -y 5.1.2 配置NGINX 四层代理不能写在默认的conf.d目录下,因为这个目录默认是数据http模块的include 所以要么把四层代理写到主配置文件最下面,要么模仿七层代理创建一个四层代理文件夹 # 1. 在nginx配置文件中增加四层代理配置文件夹 mkdir /etc/nginx/tcp.d/ echo 'include /etc/nginx/tcp.d/*.conf;' \u003e\u003e/etc/nginx/nginx.conf # 写入代理配置 cat \u003e/etc/nginx/tcp.d/apiserver.conf \u003c\u003cEOF stream { upstream kube-apiserver { server 10.4.7.21:6443 max_fails=3 fail_timeout=30s; server 10.4.7.22:6443 max_fails=3 fail_timeout=30s; } server { listen 7443; proxy_connect_timeout 2s; proxy_timeout 900s; proxy_pass kube-apiserver; } } EOF 5.1.3 启动nginx nginx -t systemctl start nginx systemctl enable nginx ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:5:1","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"5.2 配置keepalived 5.2.1 创建端口监测脚本 创建脚本 cat \u003e/etc/keepalived/check_port.sh \u003c\u003c'EOF' #!/bin/bash #keepalived 监控端口脚本 #使用方法：等待keepalived传入端口参数,检查改端口是否存在并返回结果 CHK_PORT=$1 if [ -n \"$CHK_PORT\" ];then PORT_PROCESS=`ss -lnt|grep $CHK_PORT|wc -l` if [ $PORT_PROCESS -eq 0 ];then echo \"Port $CHK_PORT Is Not Used,End.\" exit 1 fi else echo \"Check Port Cant Be Empty!\" fi EOF 给与脚本执行权限 chmod +x /etc/keepalived/check_port.sh 5.2.2 创建keepalived主配置文件 主机定义为10.4.7.11,从机定义为10.4.7.12 注意:主配置文件添加了nopreempt参数,非抢占式,意味着VIP发生漂移后,主重新启动后也不会夺回VIP,目的是为了稳定性 cat \u003e/etc/keepalived/keepalived.conf \u003c\u003c'EOF' ! Configuration File for keepalived global_defs { router_id 10.4.7.11 } vrrp_script chk_nginx { script \"/etc/keepalived/check_port.sh 7443\" interval 2 weight -20 } vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 251 priority 100 advert_int 1 mcast_src_ip 10.4.7.11 nopreempt authentication { auth_type PASS auth_pass 11111111 } track_script { chk_nginx } virtual_ipaddress { 10.4.7.10 } } EOF 5.2.3 创建keepalived从配置文件 cat \u003e/etc/keepalived/keepalived.conf \u003c\u003c'EOF' ! Configuration File for keepalived global_defs { router_id 10.4.7.12 } vrrp_script chk_nginx { script \"/etc/keepalived/check_port.sh 7443\" interval 2 weight -20 } vrrp_instance VI_1 { state BACKUP interface eth0 virtual_router_id 251 mcast_src_ip 10.4.7.12 priority 90 advert_int 1 authentication { auth_type PASS auth_pass 11111111 } track_script { chk_nginx } virtual_ipaddress { 10.4.7.10 } } EOF 5.3.4 启动keepalived并验证 systemctl start keepalived systemctl enable keepalived ip addr|grep '10.4.7.10' ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:5:2","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"6. 部署node节点 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:6:0","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"6.1 签发kubelet证书 签发证书,都在7.200上 6.1.1 创建生成证书csr的json配置文件 cd /opt/certs/ cat \u003e/opt/certs/kubelet-csr.json \u003c\u003cEOF { \"CN\": \"k8s-kubelet\", \"hosts\": [ \"127.0.0.1\", \"10.4.7.10\", \"10.4.7.21\", \"10.4.7.22\", \"10.4.7.23\", \"10.4.7.24\", \"10.4.7.25\", \"10.4.7.26\", \"10.4.7.27\", \"10.4.7.28\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"beijing\", \"L\": \"beijing\", \"O\": \"zq\", \"OU\": \"ops\" } ] } EOF 6.1.2 生成kubelet证书文件 cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=server \\ kubelet-csr.json | cfssl-json -bare kubelet [root@hdss7-200 certs]# ll |grep kubelet -rw-r--r-- 1 root root 1115 Apr 22 22:17 kubelet.csr -rw-r--r-- 1 root root 452 Apr 22 22:17 kubelet-csr.json -rw------- 1 root root 1679 Apr 22 22:17 kubelet-key.pem -rw-r--r-- 1 root root 1460 Apr 22 22:17 kubelet.pem ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:6:1","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"6.2 创建kubelet服务 6.2.1 拷贝证书至node节点 cd /opt/kubernetes/server/bin/cert scp hdss7-200:/opt/certs/kubelet.pem . scp hdss7-200:/opt/certs/kubelet-key.pem . 6.2.2 创建kubelet配置 创建kubelet的配置文件kubelet.kubeconfig比较麻烦,需要四步操作才能完成 (1) set-cluster(设置集群参数) 使用ca证书创建集群myk8s,使用的apiserver信息是10.4.7.10这个VIP cd /opt/kubernetes/server/conf/ kubectl config set-cluster myk8s \\ --certificate-authority=/opt/kubernetes/server/bin/cert/ca.pem \\ --embed-certs=true \\ --server=https://10.4.7.10:7443 \\ --kubeconfig=kubelet.kubeconfig (2) set-credentials(设置客户端认证参数) 使用client证书创建用户k8s-node kubectl config set-credentials k8s-node \\ --client-certificate=/opt/kubernetes/server/bin/cert/client.pem \\ --client-key=/opt/kubernetes/server/bin/cert/client-key.pem \\ --embed-certs=true \\ --kubeconfig=kubelet.kubeconfig (3) set-context(绑定namespace) 创建myk8s-context,关联集群myk8s和用户k8s-node kubectl config set-context myk8s-context \\ --cluster=myk8s \\ --user=k8s-node \\ --kubeconfig=kubelet.kubeconfig (4) use-context 使用生成的配置文件向apiserver注册,注册信息会写入etcd,所以只需要注册一次即可 kubectl config use-context myk8s-context --kubeconfig=kubelet.kubeconfig (5) 查看生成的kubelet.kubeconfig [root@hdss7-21 conf]# cat kubelet.kubeconfig apiVersion: v1 clusters: - cluster: certificate-authority-data: xxxxxxxx server: https://10.4.7.10:7443 name: myk8s contexts: - context: cluster: myk8s user: k8s-node name: myk8s-context current-context: myk8s-context kind: Config preferences: {} users: - name: k8s-node user: client-certificate-data: xxxxxxxx client-key-data: xxxxxxxx 可以看出来,这个配置文件里面包含了集群名字,用户名字,集群认证的公钥,用户的公私钥等 6.2.3 创建k8s-node.yaml配置文件 cat \u003e/opt/kubernetes/server/conf/k8s-node.yaml \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: k8s-node roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: k8s-node EOF 使用RBAC鉴权规则,创建了一个ClusterRoleBinding的资源 此资源中定义了一个user叫k8s-node 给k8s-node用户绑定了角色ClusterRole,角色名为system:node 使这个用户具有成为集群运算节点角色的权限 由于这个用户名,同时也是kubeconfig中指定的用户, 所以通过kubeconfig配置启动的kubelet节点,就能够成为node节点 6.2.4 应用资源配置 应用资源配置,并查看结果 # 应用资源配置 kubectl create -f /opt/kubernetes/server/conf/k8s-node.yaml # 查看集群角色和角色属性 [root@hdss7-21 conf]# kubectl get clusterrolebinding k8s-node NAME AGE k8s-node 13s [root@hdss7-21 conf]# kubectl get clusterrolebinding k8s-node -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: creationTimestamp: \"2020-04-22T14:38:09Z\" name: k8s-node resourceVersion: \"21217\" selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/k8s-node uid: 597ffb0f-f92d-4eb5-aca2-2fe73397e2e4 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: k8s-node #此时只是创建了相应的资源,还没有具体的node,如下验证 [root@hdss7-21 conf]# kubectl get nodes No resources found. 6.2.5 创建kubelet启动脚本 –hostname-override参数每个node节点都一样,是节点的主机名,注意修改 cat \u003e/opt/kubernetes/server/bin/kubelet.sh \u003c\u003c'EOF' #!/bin/sh ./kubelet \\ --hostname-override hdss7-21.host.com \\ --anonymous-auth=false \\ --cgroup-driver systemd \\ --cluster-dns 192.168.0.2 \\ --cluster-domain cluster.local \\ --runtime-cgroups=/systemd/system.slice \\ --kubelet-cgroups=/systemd/system.slice \\ --fail-swap-on=\"false\" \\ --client-ca-file ./cert/ca.pem \\ --tls-cert-file ./cert/kubelet.pem \\ --tls-private-key-file ./cert/kubelet-key.pem \\ --image-gc-high-threshold 20 \\ --image-gc-low-threshold 10 \\ --kubeconfig ../conf/kubelet.kubeconfig \\ --log-dir /data/logs/kubernetes/kube-kubelet \\ --pod-infra-container-image harbor.zq.com/public/pause:latest \\ --root-dir /data/kubelet EOF # 创建目录\u0026授权 chmod +x /opt/kubernetes/server/bin/kubelet.sh mkdir -p /data/logs/kubernetes/kube-kubelet mkdir -p /data/kubelet 6.2.6 创建supervisor配置 cat \u003e/etc/supervisord.d/kube-kubelet.ini \u003c\u003cEOF [program:kube-kubelet] command=sh /opt/kubernetes/server/bin/kubelet.sh numprocs=1 ; 启动进程数 (def 1) directory=/opt/kubernetes/server/bin autostart=true ; 是否自启 (default: true) autorestart=true ;","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:6:2","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"6.3 创建kube-proxy服务 签发证书在7.200上 6.3.1 签发kube-proxy证书 (1) 创建生成证书csr的json配置文件 cd /opt/certs/ cat \u003e/opt/certs/kube-proxy-csr.json \u003c\u003cEOF { \"CN\": \"system:kube-proxy\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"beijing\", \"L\": \"beijing\", \"O\": \"zq\", \"OU\": \"ops\" } ] } EOF (2) 生成kube-proxy证书文件 cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=client \\ kube-proxy-csr.json |cfssl-json -bare kube-proxy-client (3) 检查生成的证书文件 [root@hdss7-200 certs]# ll |grep proxy -rw-r--r-- 1 root root 1005 Apr 22 22:54 kube-proxy-client.csr -rw------- 1 root root 1675 Apr 22 22:54 kube-proxy-client-key.pem -rw-r--r-- 1 root root 1371 Apr 22 22:54 kube-proxy-client.pem -rw-r--r-- 1 root root 267 Apr 22 22:54 kube-proxy-csr.json 6.3.2 拷贝证书文件至各节点 cd /opt/kubernetes/server/bin/cert scp hdss7-200:/opt/certs/kube-proxy-client.pem . scp hdss7-200:/opt/certs/kube-proxy-client-key.pem . 6.3.3 创建kube-proxy配置 同样是四步操作,类似kubelet (1) set-cluster cd /opt/kubernetes/server/conf/ kubectl config set-cluster myk8s \\ --certificate-authority=/opt/kubernetes/server/bin/cert/ca.pem \\ --embed-certs=true \\ --server=https://10.4.7.10:7443 \\ --kubeconfig=kube-proxy.kubeconfig (2) set-credentials kubectl config set-credentials kube-proxy \\ --client-certificate=/opt/kubernetes/server/bin/cert/kube-proxy-client.pem \\ --client-key=/opt/kubernetes/server/bin/cert/kube-proxy-client-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig (3) set-context kubectl config set-context myk8s-context \\ --cluster=myk8s \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig (4) use-context kubectl config use-context myk8s-context --kubeconfig=kube-proxy.kubeconfig 6.3.4 加载ipvs模块以备kube-proxy启动用 # 创建开机ipvs脚本 cat \u003e/etc/ipvs.sh \u003c\u003c'EOF' #!/bin/bash ipvs_mods_dir=\"/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs\" for i in $(ls $ipvs_mods_dir|grep -o \"^[^.]*\") do /sbin/modinfo -F filename $i \u0026\u003e/dev/null if [ $? -eq 0 ];then /sbin/modprobe $i fi done EOF # 执行脚本开启ipvs sh /etc/ipvs.sh # 验证开启结果 [root@hdss7-21 conf]# lsmod |grep ip_vs ip_vs_wrr 12697 0 ip_vs_wlc 12519 0 ......略 6.3.5 创建kube-proxy启动脚本 同上,–hostname-override参数在不同的node节点上不一样,需修改 cat \u003e/opt/kubernetes/server/bin/kube-proxy.sh \u003c\u003c'EOF' #!/bin/sh ./kube-proxy \\ --hostname-override hdss7-21.host.com \\ --cluster-cidr 172.7.0.0/16 \\ --proxy-mode=ipvs \\ --ipvs-scheduler=nq \\ --kubeconfig ../conf/kube-proxy.kubeconfig EOF # 授权 chmod +x /opt/kubernetes/server/bin/kube-proxy.sh 6.3.6 创建kube-proxy的supervisor配置 cat \u003e/etc/supervisord.d/kube-proxy.ini \u003c\u003c'EOF' [program:kube-proxy] command=sh /opt/kubernetes/server/bin/kube-proxy.sh numprocs=1 ; 启动进程数 (def 1) directory=/opt/kubernetes/server/bin autostart=true ; 是否自启 (default: true) autorestart=true ; 是否自动重启 (default: true) startsecs=30 ; 服务运行多久判断为成功(def. 1) startretries=3 ; 启动重试次数 (default 3) exitcodes=0,2 ; 退出状态码 (default 0,2) stopsignal=QUIT ; 退出信号 (default TERM) stopwaitsecs=10 ; 退出延迟时间 (default 10) user=root ; 运行用户 redirect_stderr=true ; 重定向错误输出到标准输出(def false) stdout_logfile=/data/logs/kubernetes/kube-proxy/proxy.stdout.log stdout_logfile_maxbytes=64MB ; 日志文件大小 (default 50MB) stdout_logfile_backups=4 ; 日志文件滚动个数 (default 10) stdout_capture_maxbytes=1MB ; 设定capture管道的大小(default 0) ;子进程还有子进程,需要添加这个参数,避免产生孤儿进程 killasgroup=true stopasgroup=true EOF 6.3.7 启动服务并检查 mkdir -p /data/logs/kubernetes/kube-proxy supervisorctl update supervisorctl status [root@hdss7-21 conf]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 192.168.0.1 \u003cnone\u003e 443/TCP 47h # 检查ipvs,是否新增了配置 yum install ipvsadm -y [root@hdss7-21 conf]# ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.0.1:443 nq -\u003e 10.4.7.21:6443 Masq 1 0 0 -\u003e 10.4.7.22:6443 Masq 1 0 0 6.3.8 部署所有节点 首先需拷贝kube-proxy.kubeconfig 到 hdss7-22.host.com的conf目录下 # 拷贝证书文件 cd /opt/kubernetes/server/bin/cert scp hdss7-200:/opt/certs/kube-proxy-client.pem . scp hdss7-20","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:6:3","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"7. 验证kubernetes集群 ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:7:0","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"7.1 在任意一个节点上创建一个资源配置清单 cat \u003e/root/nginx-ds.yaml \u003c\u003c'EOF' apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: nginx-ds spec: template: metadata: labels: app: nginx-ds spec: containers: - name: my-nginx image: harbor.zq.com/public/nginx:v1.17.9 ports: - containerPort: 80 EOF ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:7:1","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"7.2 应用资源配置，并检查 7.2.1 应用资源配置 kubectl create -f /root/nginx-ds.yaml [root@hdss7-22 conf]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-ds-j777c 1/1 Running 0 8s nginx-ds-nwsd6 1/1 Running 0 8s 7.2.2 在另一台node节点上检查 kubectl get pods kubectl get pods -o wide curl 172.7.22.2 7.2.3 查看kubernetes是否搭建好 [root@hdss7-22 conf]# kubectl get cs NAME STATUS MESSAGE ERROR etcd-0 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} controller-manager Healthy ok scheduler Healthy ok [root@hdss7-21 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION hdss7-21.host.com Ready master,node 6d1h v1.15.5 hdss7-22.host.com Ready \u003cnone\u003e 6d1h v1.15.5 [root@hdss7-22 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-ds-j777c 1/1 Running 0 6m45s nginx-ds-nwsd6 1/1 Running 0 6m45s ","date":"2020-10-01","objectID":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/:7:2","tags":["K8S","转载"],"title":"02_K8S_二进制部署实践","uri":"/02_k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"},{"categories":["K8S","转载"],"content":"03_K8S_管理k8s核心资源的三种基本方法 ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:0:0","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["K8S","转载"],"content":"1. 方法分类 陈述式–主要依赖命令行工具kubectl进行管理 优点 可以满足90%以上的使用场景 对资源的增、删、查操作比较容易 缺点 命令冗长，复杂，难以记忆 特定场景下，无法实现管理需求 对资源的修改麻烦，需要patch来使用json串更改。 声明式-主要依赖统一资源配置清单进行管理 GUI式-主要依赖图形化操作界面进行管理 ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:1:0","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["K8S","转载"],"content":"2. kubectl命令行工具 kubectl中文命令说明 ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:2:0","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["K8S","转载"],"content":"2.0 增加kubectl自动补全 二进制安装的k8s,kubectl工具没有自动补全功能(其他方式安装的未验证),可以使用以下方式开启命令自动补全 source \u003c(kubectl completion bash) ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:2:1","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["K8S","转载"],"content":"2.1 get 查 2.1.1 查看名称空间namespace ~]# kubectl get namespaces NAME STATUS AGE default Active 4d11h kube-node-lease Active 4d11h kube-public Active 4d11h kube-system Active 4d11h # namespaces可以简写为ns kubectl get ns # 但不是所有资源都可以简写,所以我还是习惯tab补全名 2.1.2 查看namespace中的资源 get all查询所有资源 kubectl get all # 默认是查询default名称空间的资源,查询其他名称空间,需要加 -n namespaces kubectl get all -n kube-public 一般要养成习惯,get任何资源的时候,都要加上-n参数指定名称空间 get pods查询所有pod podsecuritypolicies.extensions ~]# kubectl get pods -n default NAME READY STATUS RESTARTS AGE nginx-ds-p66qh 1/1 Running 0 2d10h get nodes查询所有node节点 ~]# kubectl get nodes -n default NAME STATUS ROLES AGE VERSION hdss7-21.host.com Ready master,node 2d12h v1.15.5 hdss7-22.host.com NotReady \u003cnone\u003e 2d12h v1.15.5 2.1.3 -o yaml查看资源配置清单详细信息 -o yaml 可以查看yaml格式的资源配置清单详情 # 查看POD的清单 ~]# kubectl -n kube-public get pod nginx-dp-568f8dc55-jk6nb -o yaml # 查看deploy的清单 ~]# kubectl -n kube-public get deploy nginx-dp -o yaml # 查看service的清单 ~]# kubectl -n kube-public get service -o yaml -n kube-public ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:2:2","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["K8S","转载"],"content":"2.2 创建删除名称空间 create namespace创建名称空间 ~]# kubectl create namespace app namespace/app created ~]# kubectl get namespaces NAME STATUS AGE app Active 16s default Active 4d11h kube-node-lease Active 4d11h kube-public Active 4d11h kube-system Active 4d11h delete namespaces删除名称空间 ~]# kubectl delete namespaces app namespace \"app\" deleted ~]# kubectl get namespaces NAME STATUS AGE default Active 4d11h kube-node-lease Active 4d11h kube-public Active 4d11h kube-system Active 4d11h ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:2:3","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["K8S","转载"],"content":"2.3 管理POD控制器和POD 以deployment类型的POD控制为例,关于POD控制器类型,请参考官网 创建POD控制器 kubectl create deployment nginx-dp --image=harbor.zq.com/public/nginx:v1.17.9 -n kube-public ~]# kubectl get deployments -n kube-public NAME READY UP-TO-DATE AVAILABLE AGE nginx-dp 1/1 1 1 18s ~]# kubectl get pod -n kube-public NAME READY STATUS RESTARTS AGE nginx-dp-568f8dc55-9qt4j 1/1 Running 0 7m50s -o wide查看扩展信息 # 查看POD控制器信息,比基础信息多出了镜像来源,选择器等 kubectl get deployments -o wide -n kube-public # 查看POD信息,比基础信息多出了POD的IP地址,节点位置等, kubectl get pod -o wide -n kube-public describe查看资源详细信息 # 查看POD控制器详细信息 kubectl describe deployments nginx-dp -n kube-public # 查看POD详细信息 kubectl describe pod nginx-dp-568f8dc55-9qt4j -n kube-public exec进入某个POD kubectl -n kube-public exec -it nginx-dp-568f8dc55-9qt4j bash 用法与docker exec类似 scale 扩容POD kubectl -n kube-public scale deployments nginx-dp --replicas=2 delete删除POD和POD控制器 ~]# kubectl -n kube-public delete pods nginx-dp-568f8dc55-9qt4j pod \"nginx-dp-568f8dc55-9qt4j\" deleted ~]# kubectl -n kube-public get pods NAME READY STATUS RESTARTS AGE nginx-dp-568f8dc55-hnrxr 1/1 Running 0 13s ~]# kubectl -n kube-public delete deployments nginx-dp deployment.extensions \"nginx-dp\" deleted ~]# kubectl -n kube-public get pods No resources found. 在POD控制器存在的情况下,删除了POD,会由POD控制器再创建出新的POD 删除POD控制器后,对应的POD也会一并删除 ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:2:4","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["K8S","转载"],"content":"2.4 service资源管理 从上面的POD删除重建的过程可知,虽然POD会被POD控制器拉起,但是存放的NODE或POD的IP都是不确定的,那怎么对外稳定的提供服务呢 这就需要引入service的功能了,它相当于一个反向代理,不管后端POD怎么变化,server提供的服务都不会变化,可以为pod资源提供稳定的接入点 2.4.1 创建service资源 ~]# kubectl -n kube-public create deployment nginx-dp --image=harbor.zq.com/public/nginx:v1.17.9 deployment.apps/nginx-dp created ~]# kubectl -n kube-public expose deployment nginx-dp --port=80 service/nginx-dp exposed ~]# kubectl -n kube-public get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-dp ClusterIP 192.168.94.73 \u003cnone\u003e 80/TCP 45s 可以看到创建了一个VIP192.168.94.73,查看LVS信息,可以看到转发条目 [root@hdss7-21 ~]# ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) ...... TCP 192.168.94.73:80 nq -\u003e 172.7.21.3:80 Masq 1 0 0 2.4.2 扩容POD看service怎么调度 ~]# kubectl -n kube-public scale deployment nginx-dp --replicas=2 deployment.extensions/nginx-dp scaled ~]# ipvsadm -Ln ...... TCP 192.168.94.73:80 nq -\u003e 172.7.21.3:80 Masq 1 0 0 -\u003e 172.7.22.4:80 Masq 1 0 0 ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:2:5","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["K8S","转载"],"content":"2.5 explain查看属性的定义和用法 查看service资源下metadata的定义及用法 kubectl explain service.metadata ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:2:6","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["K8S","转载"],"content":"3 统一资源配置清单 统一资源配置清单,就是一个yaml格式的文件,文件中按指定格式定义了所需内容,然后通过命令行工具kubectl应用即可 ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:3:0","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["K8S","转载"],"content":"3.1 语法格式 kubectl create/apply/delete -f /path_to/xxx.yaml 3.1.1 学习方法 忌一来就无中生有自己写,容易把自己憋死 先看官方或别人写的,能读懂即可 别人的读懂了能改改内容即可 遇到不懂的用kubectl explain查帮助 ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:3:1","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["K8S","转载"],"content":"3.2 初略用法 3.2.1 查看已有资源的资源配置清单 kubuctl get svc nginx-dp -o yaml -n kube-pubic 必须存在的四个部分为: apiVersion kind metadata spec 资源配置清单中有许多项目,如果想查看资源配置清单中某一项的意义或该项下面可以配置的内容,可以使用explain来获取 kubectl explain service.kind 3.2.2 创建并应用资源配置清单 创建yaml配置文件 cat \u003enginx-ds-svc.yaml \u003c\u003cEOF apiVersion: v1 kind: Service metadata: labels: app: nginx-ds name: nginx-ds namespace: default spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: nginx-ds type: ClusterIP EOF 应用配置创建资源 kubectl create -f nginx-ds-svc.yaml # 或 kubectl apply -f nginx-ds-svc.yaml # 查看结果 [root@hdss7-21 ~]# kubectl get -f nginx-ds-svc.yaml NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-ds ClusterIP 192.168.48.225 \u003cnone\u003e 80/TCP 24s create和apply的区别 create命令和apply命令都会根据配置文件创建资源,但是: create命令只会新建,如果资源文件已使用过,则会提示错误 如果资源不存在,apply命令会新建,如果已存在,则会根据配置修改 如果是create命令新建的资源,使用apply修改时会提示 [root@hdss7-21 ~]# kubectl apply -f nginx-ds-svc.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply 意思是如果要用apply修改,就应该用apply命令创建,或者create创建时加--save-config参数 所以养成使用apply命令的习惯即可 3.2.3 修改资源配置清单 在线修改 使用edit命令,会打开一个在线的yaml格式文档,直接修改该文档后,修改立即生效 kubectl edit svc nginx-ds -n default # 查看结果 [root@hdss7-21 ~]# kubectl get service nginx-ds NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-ds ClusterIP 192.168.48.225 \u003cnone\u003e 8080/TCP 2m37s 离线修改 离线修改就是修改原来的yaml文件,然后使用apply命令重新应用配置即可 #将对外暴露的端口改为881 sed -i 's#port: 80#port: 881#g' nginx-ds-svc.yaml # edit修改过资源,再用apply修改,会报错 [root@hdss7-21 ~]# kubectl apply -f nginx-ds-svc.yaml The Service \"nginx-ds\" is invalid: * spec.ports[0].name: Required value * spec.ports[1].name: Required value # 加上--force强制修改选项 [root@hdss7-21 ~]# kubectl apply -f nginx-ds-svc.yaml --force service/nginx-ds configured # 查看结果 [root@hdss7-21 ~]# kubectl get service nginx-ds NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-ds ClusterIP 192.168.142.98 \u003cnone\u003e 881/TCP 8s 3.2.4 删除资源配置清单 陈述式删除 即:直接删除创建好的资源 kubectl delete svc nginx-ds -n default 声明式删除 即:通过制定配置文件的方式,删除用该配置文件创建出的资源 kubectl delete -f nginx-ds-svc.yaml ","date":"2020-10-01","objectID":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/:3:2","tags":["K8S","转载"],"title":"03_K8S_管理k8s核心资源的三种基本方法","uri":"/03_k8s_%E7%AE%A1%E7%90%86k8s%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%89%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/04_k8s_flannel/","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"04_K8S_核心网络插件Flannel k8s虽然设计了网络模型,然后将实现方式交给了CNI网络插件,而CNI网络插件的主要目的,就是实现POD资源能够跨宿主机进行通信 常见的网络插件有flannel,calico,canal,但是最简单的flannel已经完全满足我们的要求,故不在考虑其他网络插件 网络插件Flannel介绍：https://www.kubernetes.org.cn/3682.html ","date":"2020-10-01","objectID":"/04_k8s_flannel/:0:0","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"1. flannel功能概述 ","date":"2020-10-01","objectID":"/04_k8s_flannel/:1:0","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"1.1 flannel运转流程 首先 flannel利用Kubernetes API或者etcd用于存储整个集群的网络配置，其中最主要的内容为设置集群的网络地址空间。 例如，设定整个集群内所有容器的IP都取自网段“10.1.0.0/16”。 接着 flannel在每个主机中运行flanneld作为agent，它会为所在主机从集群的网络地址空间中，获取一个小的网段subnet，本主机内所有容器的IP地址都将从中分配。 例如，设定本主机内所有容器的IP地址网段“10.1.2.0/24”。 然后 flanneld再将本主机获取的subnet以及用于主机间通信的Public IP，同样通过kubernetes API或者etcd存储起来。 最后 flannel利用各种backend mechanism，例如udp，vxlan等等，跨主机转发容器间的网络流量，完成容器间的跨主机通信。 ","date":"2020-10-01","objectID":"/04_k8s_flannel/:1:1","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"1.2 flannel的网络模型 1.2.1 flannel支持3种网络模型 host-gw网关模型 {\"Network\": \"xxx\", \"Backend\": {\"Type\": \"host-gw\"}} 主要用于宿主机在同网段的情况下POD间的通信,即不跨网段通信. 此时flannel的功能很简单,就是在每个宿主机上创建了一条通网其他宿主机的网关路由 完全没有性能损耗,效率极高 vxlan隧道模型 {\"Network\": \"xxx\", \"Backend\": {\"Type\": \"vxlan\"}} 主要用于宿主机不在同网段的情况下POD间通信,即跨网段通信. 此时flannel会在宿主机上创建一个flannel.1的虚拟网卡,用于和其他宿主机间建立VXLAN隧道 跨宿主机通信时,需要经由flannel.1设备封包、解包，因此效率不高 混合模型 {\"Network\": \"xxx\", \"Backend\": {\"Type\": \"vxlan\",\"Directrouting\": true}} 在既有同网段宿主机，又有跨网段宿主机的情况下，选择混合模式 flannel会根据通信双方的网段情况，自动选择是走网关路由通信还是通过VXLAN隧道通信 1.2.2 实际工作中的模型选择 很多人不推荐部署K8S的使用的flannel做网络插件,不推荐的原因是是flannel性能不高,然而 flannel性能不高是指它的VXLAN隧道模型,而不是gw模型 规划K8S集群的时候,应规划多个K8S集群来管理不同的业务 同一个K8S集群的宿主机,就应该规划到同一个网段 既然是同一个网段的宿主机通信,使用的就应该是gw模型 gw模型只是创建了网关路由,通信效率极高 因此,建议工作中使用flannel,且用gw模型 ","date":"2020-10-01","objectID":"/04_k8s_flannel/:1:2","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"2. 部署flannel插件 ","date":"2020-10-01","objectID":"/04_k8s_flannel/:2:0","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"2.1 在etcd中写入网络信息 以下操作在任意etcd节点中执行都可以 /opt/etcd/etcdctl set /coreos.com/network/config '{\"Network\": \"172.7.0.0/16\", \"Backend\": {\"Type\": \"host-gw\"}}' # 查看结果 [root@hdss7-12 ~]# /opt/etcd/etcdctl get /coreos.com/network/config {\"Network\": \"172.7.0.0/16\", \"Backend\": {\"Type\": \"host-gw\"}} ","date":"2020-10-01","objectID":"/04_k8s_flannel/:2:1","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"2.2 部署准备 2.2.1 下载软件 wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz mkdir /opt/flannel-v0.11.0 tar xf flannel-v0.11.0-linux-amd64.tar.gz -C /opt/flannel-v0.11.0/ ln -s /opt/flannel-v0.11.0/ /opt/flannel 2.2.2 拷贝证书 因为要和apiserver通信，所以要配置client证书,当然ca公钥自不必说 cd /opt/flannel mkdir cert scp hdss7-200:/opt/certs/ca.pem cert/ scp hdss7-200:/opt/certs/client.pem cert/ scp hdss7-200:/opt/certs/client-key.pem cert/ 2.2.3 配置子网信息 cat \u003e/opt/flannel/subnet.env \u003c\u003cEOF FLANNEL_NETWORK=172.7.0.0/16 FLANNEL_SUBNET=172.7.21.1/24 FLANNEL_MTU=1500 FLANNEL_IPMASQ=false EOF 注意:subnet子网网段信息,每个宿主机都要修改 ","date":"2020-10-01","objectID":"/04_k8s_flannel/:2:2","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"2.3 启动flannel服务 2.3.1 创建flannel启动脚本 cat \u003e/opt/flannel/flanneld.sh \u003c\u003c'EOF' #!/bin/sh ./flanneld \\ --public-ip=10.4.7.21 \\ --etcd-endpoints=https://10.4.7.12:2379,https://10.4.7.21:2379,https://10.4.7.22:2379 \\ --etcd-keyfile=./cert/client-key.pem \\ --etcd-certfile=./cert/client.pem \\ --etcd-cafile=./cert/ca.pem \\ --iface=eth0 \\ --subnet-file=./subnet.env \\ --healthz-port=2401 EOF # 授权 chmod u+x flanneld.sh 注意: public-ip为节点IP,注意按需修改 iface为网卡,若本机网卡不是eth0,注意修改 2.3.2 创建supervisor启动脚本 cat \u003e/etc/supervisord.d/flannel.ini \u003c\u003cEOF [program:flanneld] command=sh /opt/flannel/flanneld.sh numprocs=1 directory=/opt/flannel autostart=true autorestart=true startsecs=30 startretries=3 exitcodes=0,2 stopsignal=QUIT stopwaitsecs=10 user=root redirect_stderr=true stdout_logfile=/data/logs/flanneld/flanneld.stdout.log stdout_logfile_maxbytes=64MB stdout_logfile_backups=4 stdout_capture_maxbytes=1MB ;子进程还有子进程,需要添加这个参数,避免产生孤儿进程 killasgroup=true stopasgroup=true EOF supervisor的各项配置不再备注,有需要的看K8S二进制安装中的备注 2.3.3 启动flannel服务并验证 启动服务 mkdir -p /data/logs/flanneld supervisorctl update supervisorctl status 验证路由 [root@hdss7-22 ~]# route -n|egrep -i '172.7|des' Destination Gateway Genmask Flags Metric Ref Use Iface 172.7.21.0 10.4.7.21 255.255.255.0 UG 0 0 0 eth0 172.7.22.0 0.0.0.0 255.255.255.0 U 0 0 0 docker0 [root@hdss7-21 ~]# route -n|egrep -i '172.7|des' Destination Gateway Genmask Flags Metric Ref Use Iface 172.7.21.0 0.0.0.0 255.255.255.0 U 0 0 0 docker0 172.7.22.0 10.4.7.22 255.255.255.0 UG 0 0 0 eth0 验证通信结果 [root@hdss7-21 ~]# ping 172.7.22.2 PING 172.7.22.2 (172.7.22.2) 56(84) bytes of data. 64 bytes from 172.7.22.2: icmp_seq=1 ttl=63 time=0.538 ms 64 bytes from 172.7.22.2: icmp_seq=2 ttl=63 time=0.896 ms [root@hdss7-22 ~]# ping 172.7.21.2 PING 172.7.21.2 (172.7.21.2) 56(84) bytes of data. 64 bytes from 172.7.21.2: icmp_seq=1 ttl=63 time=0.805 ms 64 bytes from 172.7.21.2: icmp_seq=2 ttl=63 time=1.14 ms ","date":"2020-10-01","objectID":"/04_k8s_flannel/:2:3","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"3 优化iptables规则 ","date":"2020-10-01","objectID":"/04_k8s_flannel/:3:0","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"3.1 前因后果 3.1.1 优化原因说明 我们使用的是gw网络模型,而这个网络模型只是创建了一条到其他宿主机下POD网络的路由信息. 因而我们可以猜想: 从外网访问到B宿主机中的POD,源IP应该是外网IP 从A宿主机访问B宿主机中的POD,源IP应该是A宿主机的IP 从A的POD-A01中,访问B中的POD,源IP应该是POD-A01的容器IP 此情形可以想象是一个路由器下的2个不同网段的交换机下的设备通过路由器(gw)通信 然后遗憾的是: 前两条毫无疑问成立 第3条理应成立,但实际不成立 不成立的原因是: Docker容器的跨网络隔离与通信，借助了iptables的机制 因此虽然K8S我们使用了ipvs调度,但是宿主机上还是有iptalbes规则 而docker默认生成的iptables规则为： 若数据出网前，先判断出网设备是不是本机docker0设备(容器网络) 如果不是的话，则进行SNAT转换后再出网，具体规则如下 [root@hdss7-21 ~]# iptables-save |grep -i postrouting|grep docker0 -A POSTROUTING -s 172.7.21.0/24 ! -o docker0 -j MASQUERADE 由于gw模式产生的数据,是从eth0流出,因而不在此规则过滤范围内 就导致此跨宿主机之间的POD通信,使用了该条SNAT规则 解决办法是: 修改此IPTABLES规则,增加过滤目标:过滤目的地是宿主机网段的流量 3.1.2 问题复现 在7-21宿主机中,访问172.7.22.2 curl 172.7.22.2 在7-21宿主机启动busybox容器,进入并访问172.7.22.2 docker pull busybox docker run --rm -it busybox bash / # wget 172.7.22.2 查看7-22宿主机上启动的nginx容器日志 [root@hdss7-22 ~]# kubectl logs nginx-ds-j777c --tail=2 10.4.7.21 - - [xxx] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.29.0\" \"-\" 10.4.7.21 - - [xxx] \"GET / HTTP/1.1\" 200 612 \"-\" \"Wget\" \"-\" 第一条日志为对端宿主机访问日志 第二条日志为对端容器访问日志 可以看出源IP都是宿主机的IP ","date":"2020-10-01","objectID":"/04_k8s_flannel/:3:1","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"3.2 具体优化过程 3.2.1 先查看iptables规则 [root@hdss7-21 ~]# iptables-save |grep -i postrouting|grep docker0 -A POSTROUTING -s 172.7.21.0/24 ! -o docker0 -j MASQUERADE 3.2.2 安装iptables并修改规则 yum install iptables-services -y iptables -t nat -D POSTROUTING -s 172.7.21.0/24 ! -o docker0 -j MASQUERADE iptables -t nat -I POSTROUTING -s 172.7.21.0/24 ! -d 172.7.0.0/16 ! -o docker0 -j MASQUERADE # 验证规则并保存配置 [root@hdss7-21 ~]# iptables-save |grep -i postrouting|grep docker0 -A POSTROUTING -s 172.7.21.0/24 ! -d 172.7.0.0/16 ! -o docker0 -j MASQUERADE [root@hdss7-21 ~]# iptables-save \u003e /etc/sysconfig/iptables 3.2.3 注意docker重启后操作 docker服务重启后,会再次增加该规则,要注意在每次重启docker服务后,删除该规则 验证: 修改后会影响到docker原本的iptables链的规则，所以需要重启docker服务 [root@hdss7-21 ~]# systemctl restart docker [root@hdss7-21 ~]# iptables-save |grep -i postrouting|grep docker0 -A POSTROUTING -s 172.7.21.0/24 ! -o docker0 -j MASQUERADE -A POSTROUTING -s 172.7.21.0/24 ! -d 172.7.0.0/16 ! -o docker0 -j MASQUERADE # 可以用iptables-restore重新应用iptables规则,也可以直接再删 [root@hdss7-21 ~]# iptables-restore /etc/sysconfig/iptables [root@hdss7-21 ~]# iptables-save |grep -i postrouting|grep docker0 -A POSTROUTING -s 172.7.21.0/24 ! -d 172.7.0.0/16 ! -o docker0 -j MASQUERADE 3.2.4 结果验证 # 对端启动容器并访问 [root@hdss7-21 ~]# docker run --rm -it busybox sh / # wget 172.7.22.2 # 本端验证日志 [root@hdss7-22 ~]# kubectl logs nginx-ds-j777c --tail=1 172.7.21.3 - - [xxxx] \"GET / HTTP/1.1\" 200 612 \"-\" \"Wget\" \"-\" ","date":"2020-10-01","objectID":"/04_k8s_flannel/:3:2","tags":["K8S","转载"],"title":"04_K8S_核心网络插件Flannel","uri":"/04_k8s_flannel/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/","tags":["K8S","转载"],"title":"05_K8S_核心插件coredns服务","uri":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"05_K8S_核心插件CoreDNS服务 ","date":"2020-10-01","objectID":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/:0:0","tags":["K8S","转载"],"title":"05_K8S_核心插件coredns服务","uri":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"1 coredns用途 coredns github地址 coredns都做了什么：Kubernetes内部域名解析原理、弊端及优化方式 coredns在K8S中的用途,主要是用作服务发现，也就是服务(应用)之间相互定位的过程。 ","date":"2020-10-01","objectID":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/:1:0","tags":["K8S","转载"],"title":"05_K8S_核心插件coredns服务","uri":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"1.1 为什么需要服务发现 在K8S集群中，POD有以下特性： 服务动态性强 容器在k8s中迁移会导致POD的IP地址变化 更新发布频繁 版本迭代快，新旧POD的IP地址会不同 支持自动伸缩 大促或流量高峰需要动态伸缩，IP地址会动态增减 service资源解决POD服务发现： 为了解决pod地址变化的问题，需要部署service资源，用service资源代理后端pod，通过暴露service资源的固定地址(集群IP)，来解决以上POD资源变化产生的IP变动问题 那service资源的服务发现呢？ service资源提供了一个不变的集群IP供外部访问，但 IP地址毕竟难以记忆 service资源可能也会被销毁和创建 能不能将service资源名称和service暴露的集群网络IP对于 类似域名与IP关系，则只需记服务名就能自动匹配服务IP 岂不就达到了service服务的自动发现 在k8s中，coredns就是为了解决以上问题。 ","date":"2020-10-01","objectID":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/:1:1","tags":["K8S","转载"],"title":"05_K8S_核心插件coredns服务","uri":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"2 coredns的部署 从coredns开始，我们使用声明式向k8s中交付容器的方式，来部署服务 ","date":"2020-10-01","objectID":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/:2:0","tags":["K8S","转载"],"title":"05_K8S_核心插件coredns服务","uri":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"2.1 获取coredns的docker镜像 以下操作可以在任意节点上完成,推荐在7.200上做,因为接下来制作coredns的k8s配置清单也是在运维主机7.200上创建后,再到node节点上应用 docker pull docker.io/coredns/coredns:1.6.1 docker tag coredns:1.6.1 harbor.zq.com/public/coredns:v1.6.1 docker push harbor.zq.com/public/coredns:v1.6.1 ","date":"2020-10-01","objectID":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/:2:1","tags":["K8S","转载"],"title":"05_K8S_核心插件coredns服务","uri":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"2.2 创建coredns的资源配置清单 以下资源配置清单,都是参考官网改出来的 mkdir -p /data/k8s-yaml/coredns 2.2.1 rbac集群权限清单 cat \u003e/data/k8s-yaml/coredns/rbac.yaml \u003c\u003cEOF apiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: Reconcile name: system:coredns rules: - apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: EnsureExists name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system EOF 2.2.2 configmap配置清单 cat \u003e/data/k8s-yaml/coredns/cm.yaml \u003c\u003cEOF apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors log health ready kubernetes cluster.local 192.168.0.0/16 #service资源cluster地址 forward . 10.4.7.11 #上级DNS地址 cache 30 loop reload loadbalance } EOF 2.2.3 depoly控制器清单 cat \u003e/data/k8s-yaml/coredns/dp.yaml \u003c\u003cEOF apiVersion: apps/v1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: coredns kubernetes.io/name: \"CoreDNS\" spec: replicas: 1 selector: matchLabels: k8s-app: coredns template: metadata: labels: k8s-app: coredns spec: priorityClassName: system-cluster-critical serviceAccountName: coredns containers: - name: coredns image: harbor.zq.com/public/coredns:v1.6.1 args: - -conf - /etc/coredns/Corefile volumeMounts: - name: config-volume mountPath: /etc/coredns ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile EOF ","date":"2020-10-01","objectID":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/:2:2","tags":["K8S","转载"],"title":"05_K8S_核心插件coredns服务","uri":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"2.2.4 service资源清单 cat \u003e/data/k8s-yaml/coredns/svc.yaml \u003c\u003cEOF apiVersion: v1 kind: Service metadata: name: coredns namespace: kube-system labels: k8s-app: coredns kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"CoreDNS\" spec: selector: k8s-app: coredns clusterIP: 192.168.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 - name: metrics port: 9153 protocol: TCP EOF ","date":"2020-10-01","objectID":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/:2:3","tags":["K8S","转载"],"title":"05_K8S_核心插件coredns服务","uri":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"2.3 创建资源并验证 在任意NODE节点执行配置都可以,先 2.3.1 验证服务能够访问` [root@hdss7-21 ~]# dig -t A harbor.zq.com +short 10.4.7.200 2.3.2 创建资源: kubectl create -f http://k8s-yaml.zq.com/coredns/rbac.yaml kubectl create -f http://k8s-yaml.zq.com/coredns/cm.yaml kubectl create -f http://k8s-yaml.zq.com/coredns/dp.yaml kubectl create -f http://k8s-yaml.zq.com/coredns/svc.yaml 2.3.3. 查看创建情况: kubectl get all -n kube-system kubectl get svc -o wide -n kube-system dig -t A www.baidu.com @192.168.0.2 +short 2.3.4 使用dig测试解析 [root@hdss7-21 ~]# dig -t A www.baidu.com @192.168.0.2 +short www.a.shifen.com. 39.156.66.18 39.156.66.14 [root@hdss7-21 ~]# dig -t A harbor.zq.com @192.168.0.2 +short 10.4.7.200 coredns已经能解析外网域名了,因为coredns的配置中,写了他的上级DNS为10.4.7.11,如果它自己解析不出来域名,会通过递归查询一级级查找 但coredns我们不是用来做外网解析的,而是用来做service名和serviceIP的解析 2.3.5 创建一个service资源来验证 先查看kube-public名称空间有没有pod ~]# kubectl get pod -n kube-public No resources found. # 之前我调试问题已经清理了所有的POD,所以没有 如果没有则先创建pod kubectl create deployment nginx-dp --image=harbor.zq.com/public/nginx:v1.17.9 -n kube-public ~]# kubectl get deployments -n kube-public NAME READY UP-TO-DATE AVAILABLE AGE nginx-dp 1/1 1 1 35s ~]# kubectl get pod -n kube-public NAME READY STATUS RESTARTS AGE nginx-dp-568f8dc55-rxvx2 1/1 Running 0 56s 给pod创建一个service kubectl expose deployment nginx-dp --port=80 -n kube-public ~]# kubectl -n kube-public get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-dp ClusterIP 192.168.63.255 \u003cnone\u003e 80/TCP 11s 验证是否可以解析 ~]# dig -t A nginx-dp @192.168.0.2 +short # 发现无返回数据,难道解析不了 # 其实是需要完整域名:服务名.名称空间.svc.cluster.local. ~]# dig -t A nginx-dp.kube-public.svc.cluster.local. @192.168.0.2 +short 192.168.63.255 可以看到我们没有手动添加任何解析记录，我们nginx-dp的service资源的IP，已经被解析了： 进入到pod内部再次验证 ~]# kubectl -n kube-public exec -it nginx-dp-568f8dc55-rxvx2 /bin/bash -qjwmz:/# apt update \u0026\u0026 apt install curl -qjwmz:/# ping nginx-dp PING nginx-dp.kube-public.svc.cluster.local (192.168.191.232): 56 data bytes 64 bytes from 192.168.191.232: icmp_seq=0 ttl=64 time=0.184 ms 64 bytes from 192.168.191.232: icmp_seq=1 ttl=64 time=0.225 ms 为什么在容器中不用加全域名? -qjwmz:/# cat /etc/resolv.conf nameserver 192.168.0.2 search kube-public.svc.cluster.local svc.cluster.local cluster.local host.com options ndots:5 当我进入到pod内部以后，会发现我们的dns地址是我们的coredns地址，以及搜索域中已经添加了搜索域:kube-public.svc.cluster.local 我们解决了在集群内部解析的问题，要想在集群外部访问我们的服务还需要igerss服务暴露功能 现在，我们已经解决了在集群内部解析的问题，但是我们怎么做到在集群外部访问我们的服务呢？ ","date":"2020-10-01","objectID":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/:2:4","tags":["K8S","转载"],"title":"05_K8S_核心插件coredns服务","uri":"/05_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6coredns%E6%9C%8D%E5%8A%A1/"},{"categories":["Markdown"],"content":"Hugo 和 LoveIt 中的 Emoji 的用法指南.","date":"2019-10-01","objectID":"/emoji-support/","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Emoji 可以通过多种方式在 Hugo 项目中启用. emojify 方法可以直接在模板中调用, 或者使用行内 Shortcodes. 要全局使用 emoji, 需要在你的网站配置中设置 enableEmoji 为 true, 然后你就可以直接在文章中输入 emoji 的代码. 它们以冒号开头和结尾，并且包含 emoji 的 代码: 去露营啦! :tent: 很快就回来. 真开心! :joy: 呈现的输出效果如下: 去露营啦! ⛺ 很快就回来. 真开心! 😂 以下符号清单是 emoji 代码的非常有用的参考. ","date":"2019-10-01","objectID":"/emoji-support/:0:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"表情与情感 ","date":"2019-10-01","objectID":"/emoji-support/:1:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"笑脸表情 图标 代码 图标 代码 😀 grinning 😃 smiley 😄 smile 😁 grin 😆 laughing satisfied 😅 sweat_smile 🤣 rofl 😂 joy 🙂 slightly_smiling_face 🙃 upside_down_face 😉 wink 😊 blush 😇 innocent ","date":"2019-10-01","objectID":"/emoji-support/:1:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"爱意表情 图标 代码 图标 代码 😍 heart_eyes 😘 kissing_heart 😗 kissing ☺️ relaxed 😚 kissing_closed_eyes 😙 kissing_smiling_eyes ","date":"2019-10-01","objectID":"/emoji-support/:1:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"吐舌头表情 图标 代码 图标 代码 😋 yum 😛 stuck_out_tongue 😜 stuck_out_tongue_winking_eye 😝 stuck_out_tongue_closed_eyes 🤑 money_mouth_face ","date":"2019-10-01","objectID":"/emoji-support/:1:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"带手的表情 图标 代码 图标 代码 🤗 hugs 🤔 thinking ","date":"2019-10-01","objectID":"/emoji-support/:1:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"中性表情 图标 代码 图标 代码 🤐 zipper_mouth_face 😐 neutral_face 😑 expressionless 😶 no_mouth 😏 smirk 😒 unamused 🙄 roll_eyes 😬 grimacing 🤥 lying_face ","date":"2019-10-01","objectID":"/emoji-support/:1:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"困倦的表情 图标 代码 图标 代码 😌 relieved 😔 pensive 😪 sleepy 🤤 drooling_face 😴 sleeping ","date":"2019-10-01","objectID":"/emoji-support/:1:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"不适的表情 图标 代码 图标 代码 😷 mask 🤒 face_with_thermometer 🤕 face_with_head_bandage 🤢 nauseated_face 🤧 sneezing_face 😵 dizzy_face ","date":"2019-10-01","objectID":"/emoji-support/:1:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"戴帽子的表情 图标 代码 图标 代码 🤠 cowboy_hat_face ","date":"2019-10-01","objectID":"/emoji-support/:1:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"戴眼镜的表情 图标 代码 图标 代码 😎 sunglasses 🤓 nerd_face ","date":"2019-10-01","objectID":"/emoji-support/:1:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"担心的表情 图标 代码 图标 代码 😕 confused 😟 worried 🙁 slightly_frowning_face ☹ frowning_face 😮 open_mouth 😯 hushed 😲 astonished 😳 flushed 😦 frowning 😧 anguished 😨 fearful 😰 cold_sweat 😥 disappointed_relieved 😢 cry 😭 sob 😱 scream 😖 confounded 😣 persevere 😞 disappointed 😓 sweat 😩 weary 😫 tired_face ","date":"2019-10-01","objectID":"/emoji-support/:1:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"否定的表情 图标 代码 图标 代码 😤 triumph 😡 pout rage 😠 angry 😈 smiling_imp 👿 imp 💀 skull ☠️ skull_and_crossbones ","date":"2019-10-01","objectID":"/emoji-support/:1:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"特殊打扮的表情 图标 代码 图标 代码 💩 hankey poop shit 🤡 clown_face 👹 japanese_ogre 👺 japanese_goblin 👻 ghost 👽 alien 👾 space_invader 🤖 robot ","date":"2019-10-01","objectID":"/emoji-support/:1:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"猫脸表情 图标 代码 图标 代码 😺 smiley_cat 😸 smile_cat 😹 joy_cat 😻 heart_eyes_cat 😼 smirk_cat 😽 kissing_cat 🙀 scream_cat 😿 crying_cat_face 😾 pouting_cat ","date":"2019-10-01","objectID":"/emoji-support/:1:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"猴脸表情 图标 代码 图标 代码 🙈 see_no_evil 🙉 hear_no_evil 🙊 speak_no_evil ","date":"2019-10-01","objectID":"/emoji-support/:1:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"情感 图标 代码 图标 代码 💋 kiss 💌 love_letter 💘 cupid 💝 gift_heart 💖 sparkling_heart 💗 heartpulse 💓 heartbeat 💞 revolving_hearts 💕 two_hearts 💟 heart_decoration ❣️ heavy_heart_exclamation 💔 broken_heart ❤️ heart 💛 yellow_heart 💚 green_heart 💙 blue_heart 💜 purple_heart 🖤 black_heart 💯 100 💢 anger 💥 boom collision 💫 dizzy 💦 sweat_drops 💨 dash 🕳️ hole 💣 bomb 💬 speech_balloon 👁️‍🗨️ eye_speech_bubble 🗯️ right_anger_bubble 💭 thought_balloon 💤 zzz ","date":"2019-10-01","objectID":"/emoji-support/:1:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人与身体 ","date":"2019-10-01","objectID":"/emoji-support/:2:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"张开手掌的手势 图标 代码 图标 代码 👋 wave 🤚 raised_back_of_hand 🖐️ raised_hand_with_fingers_splayed ✋ hand raised_hand 🖖 vulcan_salute ","date":"2019-10-01","objectID":"/emoji-support/:2:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"部分手指的手势 图标 代码 图标 代码 👌 ok_hand ✌️ v 🤞 crossed_fingers 🤘 metal 🤙 call_me_hand ","date":"2019-10-01","objectID":"/emoji-support/:2:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"一根手指的手势 图标 代码 图标 代码 👈 point_left 👉 point_right 👆 point_up_2 🖕 fu middle_finger 👇 point_down ☝️ point_up ","date":"2019-10-01","objectID":"/emoji-support/:2:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"握紧的手势 图标 代码 图标 代码 👍 +1 thumbsup 👎 -1 thumbsdown ✊ fist fist_raised 👊 facepunch fist_oncoming punch 🤛 fist_left 🤜 fist_right ","date":"2019-10-01","objectID":"/emoji-support/:2:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"两只手 图标 代码 图标 代码 👏 clap 🙌 raised_hands 👐 open_hands 🤝 handshake 🙏 pray ","date":"2019-10-01","objectID":"/emoji-support/:2:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"握住东西的手势 图标 代码 图标 代码 ✍️ writing_hand 💅 nail_care 🤳 selfie ","date":"2019-10-01","objectID":"/emoji-support/:2:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"身体部位 图标 代码 图标 代码 💪 muscle 👂 ear 👃 nose 👀 eyes 👁️ eye 👅 tongue 👄 lips ","date":"2019-10-01","objectID":"/emoji-support/:2:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人 图标 代码 图标 代码 👶 baby 👦 boy 👧 girl :blonde_man: blonde_man person_with_blond_hair 👨 man 👩 woman 👱‍♀️ blonde_woman 👴 older_man 👵 older_woman ","date":"2019-10-01","objectID":"/emoji-support/:2:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"身体动作 图标 代码 图标 代码 🙍‍♀️ frowning_woman person_frowning 🙍‍♂️ frowning_man 🙎‍♀️ person_with_pouting_face pouting_woman 🙎‍♂️ pouting_man 🙅‍♀️ ng_woman no_good no_good_woman 🙅‍♂️ ng_man no_good_man 🙆‍♀️ ok_woman 🙆‍♂️ ok_man 💁‍♀️ information_desk_person sassy_woman tipping_hand_woman 💁‍♂️ sassy_man tipping_hand_man 🙋‍♀️ raising_hand raising_hand_woman 🙋‍♂️ raising_hand_man 🙇‍♂️ bow bowing_man 🙇‍♀️ bowing_woman 🤦‍♂️ man_facepalming 🤦‍♀️ woman_facepalming 🤷‍♂️ man_shrugging 🤷‍♀️ woman_shrugging ","date":"2019-10-01","objectID":"/emoji-support/:2:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物角色 图标 代码 图标 代码 👨‍⚕️ man_health_worker 👩‍⚕️ woman_health_worker 👨‍🎓 man_student 👩‍🎓 woman_student 👨‍🏫 man_teacher 👩‍🏫 woman_teacher 👨‍⚖️ man_judge 👩‍⚖️ woman_judge 👨‍🌾 man_farmer 👩‍🌾 woman_farmer 👨‍🍳 man_cook 👩‍🍳 woman_cook 👨‍🔧 man_mechanic 👩‍🔧 woman_mechanic 👨‍🏭 man_factory_worker 👩‍🏭 woman_factory_worker 👨‍💼 man_office_worker 👩‍💼 woman_office_worker 👨‍🔬 man_scientist 👩‍🔬 woman_scientist 👨‍💻 man_technologist 👩‍💻 woman_technologist 👨‍🎤 man_singer 👩‍🎤 woman_singer 👨‍🎨 man_artist 👩‍🎨 woman_artist 👨‍✈️ man_pilot 👩‍✈️ woman_pilot 👨‍🚀 man_astronaut 👩‍🚀 woman_astronaut 👨‍🚒 man_firefighter 👩‍🚒 woman_firefighter 👮‍♂️ cop policeman 👮‍♀️ policewoman 🕵 detective male_detective 🕵️‍♀️ female_detective 💂‍♂️ guardsman 💂‍♀️ guardswoman 👷‍♂️ construction_worker construction_worker_man 👷‍♀️ construction_worker_woman 🤴 prince 👸 princess 👳‍♂️ man_with_turban 👳‍♀️ woman_with_turban 👲 man_with_gua_pi_mao 🤵‍♂️ man_in_tuxedo 👰 bride_with_veil 🤰 pregnant_woman ","date":"2019-10-01","objectID":"/emoji-support/:2:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"幻想的人物 图标 代码 图标 代码 👼 angel 🎅 santa 🤶 mrs_claus ","date":"2019-10-01","objectID":"/emoji-support/:2:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物活动 图标 代码 图标 代码 💆‍♀️ massage massage_woman 💆‍♂️ massage_man 💇‍♀️ haircut haircut_woman 💇‍♂️ haircut_man 🚶‍♂️ walking walking_man 🚶‍♀️ walking_woman 🏃‍♂️ runner running running_man 🏃‍♀️ running_woman 💃 dancer 🕺 man_dancing 🕴️ business_suit_levitating 👯‍♀️ dancers dancing_women 👯‍♂️ dancing_men ","date":"2019-10-01","objectID":"/emoji-support/:2:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"体育 图标 代码 图标 代码 🤺 person_fencing 🏇 horse_racing ⛷️ skier 🏂 snowboarder 🏌️‍♂️ golfing_man 🏌️‍♀️ golfing_woman 🏄‍♂️ surfer surfing_man 🏄‍♀️ surfing_woman 🚣‍♂️ rowboat rowing_man 🚣‍♀️ rowing_woman 🏊‍♂️ swimmer swimming_man 🏊‍♀️ swimming_woman ⛹️‍♂️ basketball_man ⛹️‍♀️ basketball_woman 🏋️‍♂️ weight_lifting_man 🏋️‍♀️ weight_lifting_woman 🚴‍♂️ bicyclist biking_man 🚴‍♀️ biking_woman 🚵‍♂️ mountain_bicyclist mountain_biking_man 🚵‍♀️ mountain_biking_woman 🤸‍♂️ man_cartwheeling 🤸‍♀️ woman_cartwheeling 🤼‍♂️ men_wrestling 🤼‍♀️ women_wrestling 🤽‍♂️ man_playing_water_polo 🤽‍♀️ woman_playing_water_polo 🤾‍♂️ man_playing_handball 🤾‍♀️ woman_playing_handball 🤹‍♂️ man_juggling 🤹‍♀️ woman_juggling ","date":"2019-10-01","objectID":"/emoji-support/:2:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"休息 图标 代码 图标 代码 🛀 bath 🛌 sleeping_bed ","date":"2019-10-01","objectID":"/emoji-support/:2:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"家庭 图标 代码 图标 代码 👭 two_women_holding_hands 👫 couple 👬 two_men_holding_hands 👩‍❤️‍💋‍👨 couplekiss_man_woman 👨‍❤️‍💋‍👨 couplekiss_man_man 👩‍❤️‍💋‍👩 couplekiss_woman_woman 👩‍❤️‍👨 couple_with_heart couple_with_heart_woman_man 👨‍❤️‍👨 couple_with_heart_man_man 👩‍❤️‍👩 couple_with_heart_woman_woman 👨‍👩‍👦 family family_man_woman_boy 👨‍👩‍👧 family_man_woman_girl 👨‍👩‍👧‍👦 family_man_woman_girl_boy 👨‍👩‍👦‍👦 family_man_woman_boy_boy 👨‍👩‍👧‍👧 family_man_woman_girl_girl 👨‍👨‍👦 family_man_man_boy 👨‍👨‍👧 family_man_man_girl 👨‍👨‍👧‍👦 family_man_man_girl_boy 👨‍👨‍👦‍👦 family_man_man_boy_boy 👨‍👨‍👧‍👧 family_man_man_girl_girl 👩‍👩‍👦 family_woman_woman_boy 👩‍👩‍👧 family_woman_woman_girl 👩‍👩‍👧‍👦 family_woman_woman_girl_boy 👩‍👩‍👦‍👦 family_woman_woman_boy_boy 👩‍👩‍👧‍👧 family_woman_woman_girl_girl 👨‍👦 family_man_boy 👨‍👦‍👦 family_man_boy_boy 👨‍👧 family_man_girl 👨‍👧‍👦 family_man_girl_boy 👨‍👧‍👧 family_man_girl_girl 👩‍👦 family_woman_boy 👩‍👦‍👦 family_woman_boy_boy 👩‍👧 family_woman_girl 👩‍👧‍👦 family_woman_girl_boy 👩‍👧‍👧 family_woman_girl_girl ","date":"2019-10-01","objectID":"/emoji-support/:2:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物符号 图标 代码 图标 代码 🗣 speaking_head 👤 bust_in_silhouette 👥 busts_in_silhouette 👣 footprints ","date":"2019-10-01","objectID":"/emoji-support/:2:16","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"动物与自然 ","date":"2019-10-01","objectID":"/emoji-support/:3:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"哺乳动物 图标 代码 图标 代码 🐵 monkey_face 🐒 monkey 🦍 gorilla 🐶 dog 🐕 dog2 🐩 poodle 🐺 wolf 🦊 fox_face 🐱 cat 🐈 cat2 🦁 lion 🐯 tiger 🐅 tiger2 🐆 leopard 🐴 horse 🐎 racehorse 🦄 unicorn 🦌 deer 🐮 cow 🐂 ox 🐃 water_buffalo 🐄 cow2 🐷 pig 🐖 pig2 🐗 boar 🐽 pig_nose 🐏 ram 🐑 sheep 🐐 goat 🐪 dromedary_camel 🐫 camel 🐘 elephant 🦏 rhinoceros 🐭 mouse 🐁 mouse2 🐀 rat 🐹 hamster 🐰 rabbit 🐇 rabbit2 🐿️ chipmunk 🦇 bat 🐻 bear 🐨 koala 🐼 panda_face 🐾 feet paw_prints ","date":"2019-10-01","objectID":"/emoji-support/:3:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"鸟类 图标 代码 图标 代码 🦃 turkey 🐔 chicken 🐓 rooster 🐣 hatching_chick 🐤 baby_chick 🐥 hatched_chick 🐦 bird 🐧 penguin 🕊 dove 🦅 eagle 🦆 duck 🦉 owl ","date":"2019-10-01","objectID":"/emoji-support/:3:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"两栖动物 icon code icon code 🐸 frog ","date":"2019-10-01","objectID":"/emoji-support/:3:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"爬虫类 图标 代码 图标 代码 🐊 crocodile 🐢 turtle 🦎 lizard 🐍 snake 🐲 dragon_face 🐉 dragon ","date":"2019-10-01","objectID":"/emoji-support/:3:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"海洋动物 图标 代码 图标 代码 🐳 whale 🐋 whale2 🐬 dolphin flipper 🐟 fish 🐠 tropical_fish 🐡 blowfish 🦈 shark 🐙 octopus 🐚 shell ","date":"2019-10-01","objectID":"/emoji-support/:3:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"虫类 图标 代码 图标 代码 🐌 snail 🦋 butterfly 🐛 bug 🐜 ant 🐝 bee honeybee 🪲 beetle 🕷️ spider 🕸️ spider_web 🦂 scorpion ","date":"2019-10-01","objectID":"/emoji-support/:3:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"花类植物 图标 代码 图标 代码 💐 bouquet 🌸 cherry_blossom 💮 white_flower 🏵️ rosette 🌹 rose 🥀 wilted_flower 🌺 hibiscus 🌻 sunflower 🌼 blossom 🌷 tulip ","date":"2019-10-01","objectID":"/emoji-support/:3:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它植物 图标 代码 图标 代码 🌱 seedling 🌲 evergreen_tree 🌳 deciduous_tree 🌴 palm_tree 🌵 cactus 🌾 ear_of_rice 🌿 herb ☘️ shamrock 🍀 four_leaf_clover 🍁 maple_leaf 🍂 fallen_leaf 🍃 leaves ","date":"2019-10-01","objectID":"/emoji-support/:3:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"食物与饮料 ","date":"2019-10-01","objectID":"/emoji-support/:4:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"水果 图标 代码 图标 代码 🍇 grapes 🍈 melon 🍉 watermelon 🍊 mandarin orange tangerine 🍋 lemon 🍌 banana 🍍 pineapple 🍎 apple 🍏 green_apple 🍐 pear 🍑 peach 🍒 cherries 🍓 strawberry 🥝 kiwi_fruit 🍅 tomato ","date":"2019-10-01","objectID":"/emoji-support/:4:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"蔬菜 图标 代码 图标 代码 🥑 avocado 🍆 eggplant 🥔 potato 🥕 carrot 🌽 corn 🌶️ hot_pepper 🥒 cucumber 🍄 mushroom 🥜 peanuts 🌰 chestnut ","date":"2019-10-01","objectID":"/emoji-support/:4:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"快餐 图标 代码 图标 代码 🍞 bread 🥐 croissant 🥖 baguette_bread 🥞 pancakes 🧀 cheese 🍖 meat_on_bone 🍗 poultry_leg 🥓 bacon 🍔 hamburger 🍟 fries 🍕 pizza 🌭 hotdog 🌮 taco 🌯 burrito 🥙 stuffed_flatbread 🥚 egg 🍳 fried_egg 🥘 shallow_pan_of_food 🍲 stew 🥗 green_salad 🍿 popcorn ","date":"2019-10-01","objectID":"/emoji-support/:4:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"亚洲食物 图标 代码 图标 代码 🍱 bento 🍘 rice_cracker 🍙 rice_ball 🍚 rice 🍛 curry 🍜 ramen 🍝 spaghetti 🍠 sweet_potato 🍢 oden 🍣 sushi 🍤 fried_shrimp 🍥 fish_cake 🍡 dango ","date":"2019-10-01","objectID":"/emoji-support/:4:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"海鲜 图标 代码 图标 代码 🦀 crab 🦐 shrimp 🦑 squid ","date":"2019-10-01","objectID":"/emoji-support/:4:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"甜点 图标 代码 图标 代码 🍦 icecream 🍧 shaved_ice 🍨 ice_cream 🍩 doughnut 🍪 cookie 🎂 birthday 🍰 cake 🍫 chocolate_bar 🍬 candy 🍭 lollipop 🍮 custard 🍯 honey_pot ","date":"2019-10-01","objectID":"/emoji-support/:4:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"饮料 图标 代码 图标 代码 🍼 baby_bottle 🥛 milk_glass ☕ coffee 🍵 tea 🍶 sake 🍾 champagne 🍷 wine_glass 🍸 cocktail 🍹 tropical_drink 🍺 beer 🍻 beers 🥂 clinking_glasses 🥃 tumbler_glass ","date":"2019-10-01","objectID":"/emoji-support/:4:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"餐具 图标 代码 图标 代码 🍽️ plate_with_cutlery 🍴 fork_and_knife 🥄 spoon 🔪 hocho knife 🏺 amphora ","date":"2019-10-01","objectID":"/emoji-support/:4:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旅游与地理 ","date":"2019-10-01","objectID":"/emoji-support/:5:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"地图 图标 代码 图标 代码 🌍 earth_africa 🌎 earth_americas 🌏 earth_asia 🌐 globe_with_meridians 🗺️ world_map 🗾 japan ","date":"2019-10-01","objectID":"/emoji-support/:5:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"地理现象 图标 代码 图标 代码 🏔 mountain_snow ⛰️ mountain 🌋 volcano 🗻 mount_fuji 🏕️ camping ⛱ beach_umbrella 🏜️ desert 🏝️ desert_island 🏞️ national_park ","date":"2019-10-01","objectID":"/emoji-support/:5:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"建筑物 图标 代码 图标 代码 🏟️ stadium 🏛️ classical_building 🏗️ building_construction 🏘 houses 🏚 derelict_house 🏠 house 🏡 house_with_garden 🏢 office 🏣 post_office 🏤 european_post_office 🏥 hospital 🏦 bank 🏨 hotel 🏩 love_hotel 🏪 convenience_store 🏫 school 🏬 department_store 🏭 factory 🏯 japanese_castle 🏰 european_castle 💒 wedding 🗼 tokyo_tower 🗽 statue_of_liberty ","date":"2019-10-01","objectID":"/emoji-support/:5:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"宗教建筑 图标 代码 图标 代码 ⛪ church 🕌 mosque 🕍 synagogue ⛩️ shinto_shrine 🕋 kaaba ","date":"2019-10-01","objectID":"/emoji-support/:5:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它地点 图标 代码 图标 代码 ⛲ fountain ⛺ tent 🌁 foggy 🌃 night_with_stars 🏙️ cityscape 🌄 sunrise_over_mountains 🌅 sunrise 🌆 city_sunset 🌇 city_sunrise 🌉 bridge_at_night ♨️ hotsprings 🎠 carousel_horse 🎡 ferris_wheel 🎢 roller_coaster 💈 barber 🎪 circus_tent ","date":"2019-10-01","objectID":"/emoji-support/:5:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"陆路运输 图标 代码 图标 代码 🚂 steam_locomotive 🚃 railway_car 🚄 bullettrain_side 🚅 bullettrain_front 🚆 train2 🚇 metro 🚈 light_rail 🚉 station 🚊 tram 🚝 monorail 🚞 mountain_railway 🚋 train 🚌 bus 🚍 oncoming_bus 🚎 trolleybus 🚐 minibus 🚑 ambulance 🚒 fire_engine 🚓 police_car 🚔 oncoming_police_car 🚕 taxi 🚖 oncoming_taxi 🚗 car red_car 🚘 oncoming_automobile 🚙 blue_car 🚚 truck 🚛 articulated_lorry 🚜 tractor 🏎️ racing_car 🏍 motorcycle 🛵 motor_scooter 🚲 bike 🛴 kick_scooter 🚏 busstop 🛣️ motorway 🛤️ railway_track 🛢️ oil_drum ⛽ fuelpump 🚨 rotating_light 🚥 traffic_light 🚦 vertical_traffic_light 🛑 stop_sign 🚧 construction ","date":"2019-10-01","objectID":"/emoji-support/:5:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"水路运输 图标 代码 图标 代码 ⚓ anchor ⛵ boat sailboat 🛶 canoe 🚤 speedboat 🛳️ passenger_ship ⛴️ ferry 🛥️ motor_boat 🚢 ship ","date":"2019-10-01","objectID":"/emoji-support/:5:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"空中运输 图标 代码 图标 代码 ✈️ airplane 🛩️ small_airplane 🛫 flight_departure 🛬 flight_arrival 💺 seat 🚁 helicopter 🚟 suspension_railway 🚠 mountain_cableway 🚡 aerial_tramway 🛰️ artificial_satellite 🚀 rocket ","date":"2019-10-01","objectID":"/emoji-support/:5:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旅馆 icon code icon code 🛎️ bellhop_bell ","date":"2019-10-01","objectID":"/emoji-support/:5:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"时间 图标 代码 图标 代码 ⌛ hourglass ⏳ hourglass_flowing_sand ⌚ watch ⏰ alarm_clock ⏱️ stopwatch ⏲️ timer_clock 🕰️ mantelpiece_clock 🕛 clock12 🕧 clock1230 🕐 clock1 🕜 clock130 🕑 clock2 🕝 clock230 🕒 clock3 🕞 clock330 🕓 clock4 🕟 clock430 🕔 clock5 🕠 clock530 🕕 clock6 🕡 clock630 🕖 clock7 🕢 clock730 🕗 clock8 🕣 clock830 🕘 clock9 🕤 clock930 🕙 clock10 🕥 clock1030 🕚 clock11 🕦 clock1130 ","date":"2019-10-01","objectID":"/emoji-support/:5:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"天空与天气 图标 代码 图标 代码 🌑 new_moon 🌒 waxing_crescent_moon 🌓 first_quarter_moon 🌔 moon waxing_gibbous_moon 🌕 full_moon 🌖 waning_gibbous_moon 🌗 last_quarter_moon 🌘 waning_crescent_moon 🌙 crescent_moon 🌚 new_moon_with_face 🌛 first_quarter_moon_with_face 🌜 last_quarter_moon_with_face 🌡️ thermometer ☀️ sunny 🌝 full_moon_with_face 🌞 sun_with_face ⭐ star 🌟 star2 🌠 stars 🌌 milky_way ☁️ cloud ⛅ partly_sunny ⛈ cloud_with_lightning_and_rain 🌤 sun_behind_small_cloud 🌥 sun_behind_large_cloud 🌦 sun_behind_rain_cloud 🌧 cloud_with_rain 🌨 cloud_with_snow 🌩 cloud_with_lightning 🌪️ tornado 🌫️ fog 🌬 wind_face 🌀 cyclone 🌈 rainbow 🌂 closed_umbrella ☂️ open_umbrella ☂️ umbrella ⛱️ parasol_on_ground ⚡ zap ❄️ snowflake ☃️ snowman_with_snow ☃️ snowman ☄️ comet 🔥 fire 💧 droplet 🌊 ocean ","date":"2019-10-01","objectID":"/emoji-support/:5:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"活动 ","date":"2019-10-01","objectID":"/emoji-support/:6:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"事件 图标 代码 图标 代码 🎃 jack_o_lantern 🎄 christmas_tree 🎆 fireworks 🎇 sparkler ✨ sparkles 🎈 balloon 🎉 tada 🎊 confetti_ball 🎋 tanabata_tree 🎍 bamboo 🎎 dolls 🎏 flags 🎐 wind_chime 🎑 rice_scene 🎀 ribbon 🎁 gift 🎗️ reminder_ribbon 🎟 tickets 🎫 ticket ","date":"2019-10-01","objectID":"/emoji-support/:6:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"奖杯与奖牌 图标 代码 图标 代码 🎖️ medal_military 🏆 trophy 🏅 medal_sports 🥇 1st_place_medal 🥈 2nd_place_medal 🥉 3rd_place_medal ","date":"2019-10-01","objectID":"/emoji-support/:6:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"体育运动 图标 代码 图标 代码 ⚽ soccer ⚾ baseball 🏀 basketball 🏐 volleyball 🏈 football 🏉 rugby_football 🎾 tennis 🎳 bowling 🦗 cricket 🏑 field_hockey 🏒 ice_hockey 🏓 ping_pong 🏸 badminton 🥊 boxing_glove 🥋 martial_arts_uniform 🥅 goal_net ⛳ golf ⛸️ ice_skate 🎣 fishing_pole_and_fish 🎽 running_shirt_with_sash 🎿 ski ","date":"2019-10-01","objectID":"/emoji-support/:6:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"游戏 图标 代码 图标 代码 🎯 dart 🎱 8ball 🔮 crystal_ball 🎮 video_game 🕹️ joystick 🎰 slot_machine 🎲 game_die ♠️ spades ♥️ hearts ♦️ diamonds ♣️ clubs 🃏 black_joker 🀄 mahjong 🎴 flower_playing_cards ","date":"2019-10-01","objectID":"/emoji-support/:6:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"艺术与工艺 图标 代码 图标 代码 🎭 performing_arts 🖼 framed_picture 🎨 art ","date":"2019-10-01","objectID":"/emoji-support/:6:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"物品 ","date":"2019-10-01","objectID":"/emoji-support/:7:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"服装 图标 代码 图标 代码 👓 eyeglasses 🕶️ dark_sunglasses 👔 necktie 👕 shirt tshirt 👖 jeans 👗 dress 👘 kimono 👙 bikini 👚 womans_clothes 👛 purse 👜 handbag 👝 pouch 🛍️ shopping 🎒 school_satchel 👞 mans_shoe shoe 👟 athletic_shoe 👠 high_heel 👡 sandal 👢 boot 👑 crown 👒 womans_hat 🎩 tophat 🎓 mortar_board ⛑️ rescue_worker_helmet 📿 prayer_beads 💄 lipstick 💍 ring 💎 gem ","date":"2019-10-01","objectID":"/emoji-support/:7:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"声音 图标 代码 图标 代码 🔇 mute 🔈 speaker 🔉 sound 🔊 loud_sound 📢 loudspeaker 📣 mega 📯 postal_horn 🔔 bell 🔕 no_bell ","date":"2019-10-01","objectID":"/emoji-support/:7:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"音乐 图标 代码 图标 代码 🎼 musical_score 🎵 musical_note 🎶 notes 🎙️ studio_microphone 🎚️ level_slider 🎛️ control_knobs 🎤 microphone 🎧 headphones 📻 radio ","date":"2019-10-01","objectID":"/emoji-support/:7:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"乐器 图标 代码 图标 代码 🎷 saxophone 🎸 guitar 🎹 musical_keyboard 🎺 trumpet 🎻 violin 🥁 drum ","date":"2019-10-01","objectID":"/emoji-support/:7:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"电话 图标 代码 图标 代码 📱 iphone 📲 calling ☎️ phone telephone 📞 telephone_receiver 📟 pager 📠 fax ","date":"2019-10-01","objectID":"/emoji-support/:7:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"电脑 图标 代码 图标 代码 🔋 battery 🔌 electric_plug 💻 computer 🖥️ desktop_computer 🖨️ printer ⌨️ keyboard 🖱 computer_mouse 🖲️ trackball 💽 minidisc 💾 floppy_disk 💿 cd 📀 dvd ","date":"2019-10-01","objectID":"/emoji-support/:7:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"灯光与影像 图标 代码 图标 代码 🎥 movie_camera 🎞️ film_strip 📽️ film_projector 🎬 clapper 📺 tv 📷 camera 📸 camera_flash 📹 video_camera 📼 vhs 🔍 mag 🔎 mag_right 🕯️ candle 💡 bulb 🔦 flashlight 🏮 izakaya_lantern lantern ","date":"2019-10-01","objectID":"/emoji-support/:7:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"书与纸张 图标 代码 图标 代码 📔 notebook_with_decorative_cover 📕 closed_book 📖 book open_book 📗 green_book 📘 blue_book 📙 orange_book 📚 books 📓 notebook 📒 ledger 📃 page_with_curl 📜 scroll 📄 page_facing_up 📰 newspaper 🗞️ newspaper_roll 📑 bookmark_tabs 🔖 bookmark 🏷️ label ","date":"2019-10-01","objectID":"/emoji-support/:7:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"钱 图标 代码 图标 代码 💰 moneybag 💴 yen 💵 dollar 💶 euro 💷 pound 💸 money_with_wings 💳 credit_card 💹 chart ","date":"2019-10-01","objectID":"/emoji-support/:7:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"邮件 图标 代码 图标 代码 ✉️ email envelope 📧 📧 📨 incoming_envelope 📩 envelope_with_arrow 📤 outbox_tray 📥 inbox_tray 📦 package 📫 mailbox 📪 mailbox_closed 📬 mailbox_with_mail 📭 mailbox_with_no_mail 📮 postbox 🗳 ballot_box ","date":"2019-10-01","objectID":"/emoji-support/:7:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"书写 图标 代码 图标 代码 ✏️ pencil2 ✒️ black_nib 🖋 fountain_pen 🖊 pen 🖌 paintbrush 🖍 crayon 📝 memo pencil ","date":"2019-10-01","objectID":"/emoji-support/:7:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"办公 图标 代码 图标 代码 💼 briefcase 📁 file_folder 📂 open_file_folder 🗂️ card_index_dividers 📅 date 📆 calendar 🗒 spiral_notepad 🗓 spiral_calendar 📇 card_index 📈 chart_with_upwards_trend 📉 chart_with_downwards_trend 📊 bar_chart 📋 clipboard 📌 pushpin 📍 round_pushpin 📎 paperclip 🖇 paperclips 📏 straight_ruler 📐 triangular_ruler ✂️ scissors 🗃️ card_file_box 🗄️ file_cabinet 🗑️ wastebasket ","date":"2019-10-01","objectID":"/emoji-support/:7:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"锁 图标 代码 图标 代码 🔒 lock 🔓 unlock 🔏 lock_with_ink_pen 🔐 closed_lock_with_key 🔑 key 🗝️ old_key ","date":"2019-10-01","objectID":"/emoji-support/:7:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"工具 图标 代码 图标 代码 🔨 hammer ⛏️ pick ⚒️ hammer_and_pick 🛠️ hammer_and_wrench 🗡 dagger ⚔️ crossed_swords 🔫 gun 🏹 bow_and_arrow 🛡️ shield 🔧 wrench 🔩 nut_and_bolt ⚙️ gear 🗜 clamp ⚖ balance_scale 🔗 link ⛓️ chains ","date":"2019-10-01","objectID":"/emoji-support/:7:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"科学 图标 代码 图标 代码 ⚗️ alembic 🔬 microscope 🔭 telescope 🛰️ satellite ","date":"2019-10-01","objectID":"/emoji-support/:7:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"医疗 图标 代码 图标 代码 💉 syringe 💊 pill ","date":"2019-10-01","objectID":"/emoji-support/:7:16","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"生活用品 图标 代码 图标 代码 🚪 door 🛏️ bed 🛋️ couch_and_lamp 🚽 toilet 🚿 shower 🛁 bathtub 🛒 shopping_cart ","date":"2019-10-01","objectID":"/emoji-support/:7:17","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它物品 图标 代码 图标 代码 🚬 smoking ⚰️ coffin ⚱️ funeral_urn 🗿 moyai ","date":"2019-10-01","objectID":"/emoji-support/:7:18","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"符号 ","date":"2019-10-01","objectID":"/emoji-support/:8:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"交通标识 图标 代码 图标 代码 🏧 atm 🚮 put_litter_in_its_place 🚰 potable_water ♿ wheelchair 🚹 mens 🚺 womens 🚻 restroom 🚼 baby_symbol 🚾 wc 🛂 passport_control 🛃 customs 🛄 baggage_claim 🛅 left_luggage ","date":"2019-10-01","objectID":"/emoji-support/:8:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"警告 图标 代码 图标 代码 ⚠️ warning 🚸 children_crossing ⛔ no_entry 🚫 no_entry_sign 🚳 no_bicycles 🚭 no_smoking 🚯 do_not_litter 🚱 🚱 🚷 no_pedestrians 📵 no_mobile_phones 🔞 underage ☢ radioactive ☣ biohazard ","date":"2019-10-01","objectID":"/emoji-support/:8:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"箭头 图标 代码 图标 代码 ⬆️ arrow_up ↗️ arrow_upper_right ➡️ arrow_right ↘️ arrow_lower_right ⬇️ arrow_down ↙️ arrow_lower_left ⬅️ arrow_left ↖️ arrow_upper_left ↕️ arrow_up_down ↔️ left_right_arrow ↩️ leftwards_arrow_with_hook ↪️ arrow_right_hook ⤴️ arrow_heading_up ⤵️ arrow_heading_down 🔃 arrows_clockwise 🔄 arrows_counterclockwise 🔙 back 🔚 end 🔛 on 🔜 soon 🔝 top ","date":"2019-10-01","objectID":"/emoji-support/:8:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"宗教 图标 代码 图标 代码 🛐 place_of_worship ⚛️ atom_symbol 🕉 om ✡️ star_of_david ☸️ wheel_of_dharma ☯️ yin_yang ✝️ latin_cross ☦️ orthodox_cross ☪️ star_and_crescent ☮️ peace_symbol 🕎 menorah 🔯 six_pointed_star ","date":"2019-10-01","objectID":"/emoji-support/:8:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"生肖 图标 代码 图标 代码 ♈ aries ♉ taurus ♊ gemini ♋ cancer ♌ leo ♍ virgo ♎ libra ♏ scorpius ♐ sagittarius ♑ capricorn ♒ aquarius ♓ pisces ⛎ ophiuchus ","date":"2019-10-01","objectID":"/emoji-support/:8:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"影像符号 图标 代码 图标 代码 🔀 twisted_rightwards_arrows 🔁 repeat 🔂 repeat_one ▶️ arrow_forward ⏩ fast_forward ⏭ next_track_button ⏯ play_or_pause_button ◀️ arrow_backward ⏪ rewind ⏮️ previous_track_button 🔼 arrow_up_small ⏫ arrow_double_up 🔽 arrow_down_small ⏬ arrow_double_down ⏸ pause_button ⏹ stop_button ⏺ record_button 🎦 cinema 🔅 low_brightness 🔆 high_brightness 📶 signal_strength 📳 vibration_mode 📴 mobile_phone_off ","date":"2019-10-01","objectID":"/emoji-support/:8:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"数学 图标 代码 图标 代码 ✖️ heavy_multiplication_x ➕ heavy_plus_sign ➖ heavy_minus_sign ➗ heavy_division_sign ","date":"2019-10-01","objectID":"/emoji-support/:8:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"标点符号 图标 代码 图标 代码 ‼️ bangbang ⁉️ interrobang ❓ question ❔ grey_question ❕ grey_exclamation ❗ exclamation heavy_exclamation_mark 〰️ wavy_dash ","date":"2019-10-01","objectID":"/emoji-support/:8:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"货币 图标 代码 图标 代码 💱 currency_exchange 💲 heavy_dollar_sign ","date":"2019-10-01","objectID":"/emoji-support/:8:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"按键符号 图标 代码 图标 代码 #️⃣ hash *️⃣ asterisk 0️⃣ zero 1️⃣ one 2️⃣ two 3️⃣ three 4️⃣ four 5️⃣ five 6️⃣ six 7️⃣ seven 8️⃣ eight 9️⃣ nine 🔟 keycap_ten ","date":"2019-10-01","objectID":"/emoji-support/:8:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"字母符号 图标 代码 图标 代码 🔠 capital_abcd 🔡 abcd 🔢 1234 🔣 symbols 🔤 abc 🅰️ a 🆎 ab 🅱️ b 🆑 cl 🆒 cool 🆓 free ℹ️ information_source 🆔 id ⓜ️ m 🆕 new 🆖 ng 🅾️ o2 🆗 ok 🅿️ parking 🆘 sos 🆙 up 🆚 vs 🈁 koko 🈂️ sa 🈷️ u6708 🈶 u6709 🈯 u6307 🉐 ideograph_advantage 🈹 u5272 🈚 u7121 🈲 u7981 🉑 accept 🈸 u7533 🈴 u5408 🈳 u7a7a ㊗️ congratulations ㊙️ secret 🈺 u55b6 🈵 u6e80 ","date":"2019-10-01","objectID":"/emoji-support/:8:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"几何符号 图标 代码 图标 代码 🔴 red_circle 🔵 large_blue_circle ⚫ black_circle ⚪ white_circle ⬛ black_large_square ⬜ white_large_square ◼️ black_medium_square ◻️ white_medium_square ◾ black_medium_small_square ◽ white_medium_small_square ▪️ black_small_square ▫️ white_small_square 🔶 large_orange_diamond 🔷 large_blue_diamond 🔸 small_orange_diamond 🔹 small_blue_diamond 🔺 small_red_triangle 🔻 small_red_triangle_down 💠 diamond_shape_with_a_dot_inside 🔘 radio_button 🔳 white_square_button 🔲 black_square_button ","date":"2019-10-01","objectID":"/emoji-support/:8:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它符合 图标 代码 图标 代码 ♻️ recycle ⚜️ fleur_de_lis 🔱 trident 📛 name_badge 🔰 beginner ⭕ o ✅ white_check_mark ☑️ ballot_box_with_check ✔️ heavy_check_mark ❌ x ❎ negative_squared_cross_mark ➰ curly_loop ➿ loop 〽️ part_alternation_mark ✳️ eight_spoked_asterisk ✴️ eight_pointed_black_star ❇️ sparkle ©️ copyright ®️ registered ™️ tm ","date":"2019-10-01","objectID":"/emoji-support/:8:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旗帜 ","date":"2019-10-01","objectID":"/emoji-support/:9:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"常用旗帜 图标 代码 图标 代码 🏁 checkered_flag 🚩 triangular_flag_on_post 🎌 crossed_flags 🏴 black_flag 🏳 white_flag 🏳️‍🌈 rainbow_flag ","date":"2019-10-01","objectID":"/emoji-support/:9:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"国家和地区旗帜 图标 代码 图标 代码 🇦🇩 andorra 🇦🇪 united_arab_emirates 🇦🇫 afghanistan 🇦🇬 antigua_barbuda 🇦🇮 anguilla 🇦🇱 albania 🇦🇲 armenia 🇦🇴 angola 🇦🇶 antarctica 🇦🇷 argentina 🇦🇸 american_samoa 🇦🇹 austria 🇦🇺 australia 🇦🇼 aruba 🇦🇽 aland_islands 🇦🇿 azerbaijan 🇧🇦 bosnia_herzegovina 🇧🇧 barbados 🇧🇩 bangladesh 🇧🇪 belgium 🇧🇫 burkina_faso 🇧🇬 bulgaria 🇧🇭 bahrain 🇧🇮 burundi 🇧🇯 benin 🇧🇱 st_barthelemy 🇧🇲 bermuda 🇧🇳 brunei 🇧🇴 bolivia 🇧🇶 caribbean_netherlands 🇧🇷 brazil 🇧🇸 bahamas 🇧🇹 bhutan 🇧🇼 botswana 🇧🇾 belarus 🇧🇿 belize 🇨🇦 canada 🇨🇨 cocos_islands 🇨🇩 congo_kinshasa 🇨🇫 central_african_republic 🇨🇬 congo_brazzaville 🇨🇭 switzerland 🇨🇮 cote_divoire 🇨🇰 cook_islands 🇨🇱 chile 🇨🇲 cameroon 🇨🇳 cn 🇨🇴 colombia 🇨🇷 costa_rica 🇨🇺 cuba 🇨🇻 cape_verde 🇨🇼 curacao 🇨🇽 christmas_island 🇨🇾 cyprus 🇨🇿 czech_republic 🇩🇪 de 🇩🇯 djibouti 🇩🇰 denmark 🇩🇲 dominica 🇩🇴 dominican_republic 🇩🇿 algeria 🇪🇨 ecuador 🇪🇪 estonia 🇪🇬 egypt 🇪🇭 western_sahara 🇪🇷 eritrea 🇪🇸 es 🇪🇹 ethiopia 🇪🇺 eu european_union 🇫🇮 finland 🇫🇯 fiji 🇫🇰 falkland_islands 🇫🇲 micronesia 🇫🇴 faroe_islands 🇫🇷 fr 🇬🇦 gabon 🇬🇧 gb uk 🇬🇩 grenada 🇬🇪 georgia 🇬🇫 french_guiana 🇬🇬 guernsey 🇬🇭 ghana 🇬🇮 gibraltar 🇬🇱 greenland 🇬🇲 gambia 🇬🇳 guinea 🇬🇵 guadeloupe 🇬🇶 equatorial_guinea 🇬🇷 greece 🇬🇸 south_georgia_south_sandwich_islands 🇬🇹 guatemala 🇬🇺 guam 🇬🇼 guinea_bissau 🇬🇾 guyana 🇭🇰 hong_kong 🇭🇳 honduras 🇭🇷 croatia 🇭🇹 haiti 🇭🇺 hungary 🇮🇨 canary_islands 🇮🇩 indonesia 🇮🇪 ireland 🇮🇱 israel 🇮🇲 isle_of_man 🇮🇳 india 🇮🇴 british_indian_ocean_territory 🇮🇶 iraq 🇮🇷 iran 🇮🇸 iceland 🇮🇹 it 🇯🇪 jersey 🇯🇲 jamaica 🇯🇴 jordan 🇯🇵 jp 🇰🇪 kenya 🇰🇬 kyrgyzstan 🇰🇭 cambodia 🇰🇮 kiribati 🇰🇲 comoros 🇰🇳 st_kitts_nevis 🇰🇵 north_korea 🇰🇷 kr 🇰🇼 kuwait 🇰🇾 cayman_islands 🇰🇿 kazakhstan 🇱🇦 laos 🇱🇧 lebanon 🇱🇨 st_lucia 🇱🇮 liechtenstein 🇱🇰 sri_lanka 🇱🇷 liberia 🇱🇸 lesotho 🇱🇹 lithuania 🇱🇺 luxembourg 🇱🇻 latvia 🇱🇾 libya 🇲🇦 morocco 🇲🇨 monaco 🇲🇩 moldova 🇲🇪 montenegro 🇲🇬 madagascar 🇲🇭 marshall_islands 🇲🇰 macedonia 🇲🇱 mali 🇲🇲 myanmar 🇲🇳 mongolia 🇲🇴 macau 🇲🇵 northern_mariana_islands 🇲🇶 martinique 🇲🇷 mauritania 🇲🇸 montserrat 🇲🇹 malta 🇲🇺 mauritius 🇲🇻 maldives 🇲🇼 malawi 🇲🇽 mexico 🇲🇾 malaysia 🇲🇿 mozambique 🇳🇦 namibia 🇳🇨 new_caledonia 🇳🇪 niger 🇳🇫 norfolk_island 🇳🇬 nigeria 🇳🇮 nicaragua 🇳🇱 netherlands 🇳🇴 norway 🇳🇵 nepal 🇳🇷 nauru 🇳🇺 niue 🇳🇿 new_zealand 🇴🇲 oman 🇵🇦 panama 🇵🇪 peru 🇵🇫 french_polynesia 🇵🇬 papua_new_guinea 🇵🇭 philippines 🇵🇰 pakistan 🇵🇱 poland 🇵🇲 st_pierre_miquelon 🇵🇳 pitcairn_islands 🇵🇷 puerto_rico 🇵🇸 palestinian_territories 🇵🇹 portugal 🇵🇼 palau 🇵🇾 paraguay 🇶🇦 qatar 🇷🇪 reunion 🇷🇴 romania 🇷🇸 serbia 🇷🇺 ru 🇷🇼 rwanda 🇸🇦 saudi_arabia 🇸🇧 solomon_islands 🇸🇨 seychelles 🇸🇩 sudan 🇸🇪 sweden 🇸🇬 singapore 🇸🇭 st_helena 🇸🇮 slovenia 🇸🇰 slovakia 🇸🇱 sierra_leone 🇸🇲 san_marino 🇸🇳 senegal 🇸🇴 somalia 🇸🇷 suriname 🇸🇸 south_sudan 🇸🇹 sao_tome_principe 🇸🇻 el_salvador 🇸🇽 sint_maarten 🇸🇾 syria 🇸🇿 swaziland 🇹🇨 turks_caicos_islands 🇹🇩 chad 🇹🇫 french_southern_territories 🇹🇬 togo 🇹🇭 thailand 🇹🇯 tajikistan 🇹🇰 tokelau 🇹🇱 timor_leste 🇹🇲 turkmenistan 🇹🇳 tunisia 🇹🇴 tonga 🇹🇷 tr 🇹🇹 trinidad_tobago 🇹🇻 tuvalu 🇹🇼 taiwan 🇹🇿 tanzania 🇺🇦 ukraine 🇺🇬 uganda 🇺🇸 us 🇺🇾 uruguay 🇺🇿 uzbekistan 🇻🇦 vatican_city 🇻🇨 st_vincent_grenadines 🇻🇪 venezuela 🇻🇬 british_virgin_islands 🇻🇮 us_virgin_islands 🇻🇳 vietnam 🇻🇺 vanuatu 🇼🇫 wallis_futuna 🇼🇸 samoa 🇽🇰 kosovo 🇾🇪 yemen 🇾🇹 mayotte 🇿🇦 south_africa 🇿🇲 zambia 🇿🇼 zimbabwe ","date":"2019-10-01","objectID":"/emoji-support/:9:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/","tags":["K8S","转载"],"title":"06_K8S_核心插件-ingress(服务暴露)控制器traefik","uri":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/"},{"categories":["转载","K8S"],"content":"06_K8S_核心插件-ingress(服务暴露)控制器traefik ","date":"2020-10-01","objectID":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/:0:0","tags":["K8S","转载"],"title":"06_K8S_核心插件-ingress(服务暴露)控制器traefik","uri":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/"},{"categories":["转载","K8S"],"content":"1 K8S两种服务暴露方法 前面通过coredns在k8s集群内部做了serviceNAME和serviceIP之间的自动映射,使得不需要记录service的IP地址,只需要通过serviceNAME就能访问POD 但是在K8S集群外部,显然是不能通过serviceNAME或serviceIP来解析服务的 要在K8S集群外部来访问集群内部的资源,需要用到服务暴露功能 ","date":"2020-10-01","objectID":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/:1:0","tags":["K8S","转载"],"title":"06_K8S_核心插件-ingress(服务暴露)控制器traefik","uri":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/"},{"categories":["转载","K8S"],"content":"1.1 K8S常用的两种服务暴露方法 使用NodePort型的Service nodeport型的service原理相当于端口映射，将容器内的端口映射到宿主机上的某个端口。 K8S集群不能使用ipvs的方式调度,必须使用iptables,且只支持rr模式 使用Ingress资源 Ingress是K8S API标准资源之一,也是核心资源 是一组基于域名和URL路径的规则,把用户的请求转发至指定的service资源 可以将集群外部的请求流量,转发至集群内部,从而实现'服务暴露' ","date":"2020-10-01","objectID":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/:1:1","tags":["K8S","转载"],"title":"06_K8S_核心插件-ingress(服务暴露)控制器traefik","uri":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/"},{"categories":["转载","K8S"],"content":"1.2 Ingress控制器是什么 可以理解为一个简化版本的nginx Ingress控制器是能够为Ingress资源健康某套接字,然后根据ingress规则匹配机制路由调度流量的一个组件 只能工作在七层网络下，建议暴露http, https可以使用前端nginx来做证书方面的卸载 我们使用的ingress控制器为**Traefik** traefik：GITHUB官方地址 ","date":"2020-10-01","objectID":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/:1:2","tags":["K8S","转载"],"title":"06_K8S_核心插件-ingress(服务暴露)控制器traefik","uri":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/"},{"categories":["转载","K8S"],"content":"2 部署traefik 同样的,现在7.200完成docker镜像拉取和配置清单创建,然后再到任意master节点执行配置清单 ","date":"2020-10-01","objectID":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/:2:0","tags":["K8S","转载"],"title":"06_K8S_核心插件-ingress(服务暴露)控制器traefik","uri":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/"},{"categories":["转载","K8S"],"content":"2.1 准备docker镜像 docker pull traefik:v1.7.2-alpine docker tag traefik:v1.7.2-alpine harbor.zq.com/public/traefik:v1.7.2 docker push harbor.zq.com/public/traefik:v1.7.2 ","date":"2020-10-01","objectID":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/:2:1","tags":["K8S","转载"],"title":"06_K8S_核心插件-ingress(服务暴露)控制器traefik","uri":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/"},{"categories":["转载","K8S"],"content":"2.2 创建资源清单 mkdir -p /data/k8s-yaml/traefik 2.2.1 rbac授权清单 cat \u003e/data/k8s-yaml/traefik/rbac.yaml \u003c\u003cEOF apiVersion: v1 kind: ServiceAccount metadata: name: traefik-ingress-controller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: traefik-ingress-controller rules: - apiGroups: - \"\" resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: kube-system EOF 2.2.2 delepoly资源清单 cat \u003e/data/k8s-yaml/traefik/ds.yaml \u003c\u003cEOF apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: traefik-ingress namespace: kube-system labels: k8s-app: traefik-ingress spec: template: metadata: labels: k8s-app: traefik-ingress name: traefik-ingress spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 containers: - image: harbor.zq.com/public/traefik:v1.7.2 name: traefik-ingress ports: - name: controller containerPort: 80 hostPort: 81 - name: admin-web containerPort: 8080 securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE args: - --api - --kubernetes - --logLevel=INFO - --insecureskipverify=true - --kubernetes.endpoint=https://10.4.7.10:7443 - --accesslog - --accesslog.filepath=/var/log/traefik_access.log - --traefiklog - --traefiklog.filepath=/var/log/traefik.log - --metrics.prometheus EOF 2.2.3 service清单 cat \u003e/data/k8s-yaml/traefik/svc.yaml \u003c\u003cEOF kind: Service apiVersion: v1 metadata: name: traefik-ingress-service namespace: kube-system spec: selector: k8s-app: traefik-ingress ports: - protocol: TCP port: 80 name: controller - protocol: TCP port: 8080 name: admin-web EOF 2.2.4 ingress清单 cat \u003e/data/k8s-yaml/traefik/ingress.yaml \u003c\u003cEOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-web-ui namespace: kube-system annotations: kubernetes.io/ingress.class: traefik spec: rules: - host: traefik.zq.com http: paths: - path: / backend: serviceName: traefik-ingress-service servicePort: 8080 EOF ","date":"2020-10-01","objectID":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/:2:2","tags":["K8S","转载"],"title":"06_K8S_核心插件-ingress(服务暴露)控制器traefik","uri":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/"},{"categories":["转载","K8S"],"content":"2.3 创建资源 2.3.1 任意节点上创建资源 kubectl create -f http://k8s-yaml.zq.com/traefik/rbac.yaml kubectl create -f http://k8s-yaml.zq.com/traefik/ds.yaml kubectl create -f http://k8s-yaml.zq.com/traefik/svc.yaml kubectl create -f http://k8s-yaml.zq.com/traefik/ingress.yaml 2.3.2 在前端nginx上做反向代理 在7.11和7.12上,都做反向代理,将泛域名的解析都转发到traefik上去 cat \u003e/etc/nginx/conf.d/zq.com.conf \u003c\u003c'EOF' upstream default_backend_traefik { server 10.4.7.21:81 max_fails=3 fail_timeout=10s; server 10.4.7.22:81 max_fails=3 fail_timeout=10s; } server { server_name *.zq.com; location / { proxy_pass http://default_backend_traefik; proxy_set_header Host $http_host; proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for; } } EOF # 重启nginx服务 nginx -t nginx -s reload 2.3.3 在bind9中添加域名解析 需要将traefik 服务的解析记录添加的DNS解析中,注意是绑定到VIP上 vi /var/named/zq.com.zone ........ traefik A 10.4.7.10 注意前滚serial编号 重启named服务 systemctl restart named #dig验证解析结果 [root@hdss7-11 ~]# dig -t A traefik.zq.com +short 10.4.7.10 2.3.4 在集群外访问验证 在集群外,访问[http://traefik.zq.com](http://traefik.zq.com),如果能正常显示web页面.说明我们已经暴露服务成功 ","date":"2020-10-01","objectID":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/:2:3","tags":["K8S","转载"],"title":"06_K8S_核心插件-ingress(服务暴露)控制器traefik","uri":"/06_k8s_%E6%A0%B8%E5%BF%83%E6%8F%92%E4%BB%B6-ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E6%8E%A7%E5%88%B6%E5%99%A8traefik/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/","tags":["K8S","转载"],"title":"07_K8S_web管理方式dashboard","uri":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/"},{"categories":["转载","K8S"],"content":"07_K8S_web管理方式dashboard dashboard是k8s的可视化管理平台，是三种管理k8s集群方法之一 ","date":"2020-10-01","objectID":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/:0:0","tags":["K8S","转载"],"title":"07_K8S_web管理方式dashboard","uri":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/"},{"categories":["转载","K8S"],"content":"1 部署dashboard ","date":"2020-10-01","objectID":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/:1:0","tags":["K8S","转载"],"title":"07_K8S_web管理方式dashboard","uri":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/"},{"categories":["转载","K8S"],"content":"1.1 获取dashboard镜像 获取镜像和创建资源配置清单的操作,还是老规矩:7.200上操作 1.1.1 获取1.8.3版本的dsashboard docker pull k8scn/kubernetes-dashboard-amd64:v1.8.3 docker tag k8scn/kubernetes-dashboard-amd64:v1.8.3 harbor.zq.com/public/dashboard:v1.8.3 docker push harbor.zq.com/public/dashboard:v1.8.3 1.1.2 获取1.10.1版本的dashboard docker pull loveone/kubernetes-dashboard-amd64:v1.10.1 docker tag loveone/kubernetes-dashboard-amd64:v1.10.1 harbor.zq.com/public/dashboard:v1.10.1 docker push harbor.zq.com/public/dashboard:v1.10.1 1.1.3 为何要两个版本的dashbosrd 1.8.3版本授权不严格,方便学习使用 1.10.1版本授权严格,学习使用麻烦,但生产需要 ","date":"2020-10-01","objectID":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/:1:1","tags":["K8S","转载"],"title":"07_K8S_web管理方式dashboard","uri":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/"},{"categories":["转载","K8S"],"content":"1.2 创建dashboard资源配置清单 mkdir -p /data/k8s-yaml/dashboard 1.2.1 创建rbca授权清单 cat \u003e/data/k8s-yaml/dashboard/rbac.yaml \u003c\u003cEOF apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: Reconcile name: kubernetes-dashboard-admin namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard-admin namespace: kube-system labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: Reconcile roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard-admin namespace: kube-system EOF 1.2.2 创建depoloy清单 cat \u003e/data/k8s-yaml/dashboard/dp.yaml \u003c\u003cEOF apiVersion: apps/v1 kind: Deployment metadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: priorityClassName: system-cluster-critical containers: - name: kubernetes-dashboard image: harbor.zq.com/public/dashboard:v1.8.3 resources: limits: cpu: 100m memory: 300Mi requests: cpu: 50m memory: 100Mi ports: - containerPort: 8443 protocol: TCP args: # PLATFORM-SPECIFIC ARGS HERE - --auto-generate-certificates volumeMounts: - name: tmp-volume mountPath: /tmp livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard-admin tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" EOF 1.2.3 创建service清单 cat \u003e/data/k8s-yaml/dashboard/svc.yaml \u003c\u003cEOF apiVersion: v1 kind: Service metadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile spec: selector: k8s-app: kubernetes-dashboard ports: - port: 443 targetPort: 8443 EOF 1.2.4 创建ingress清单暴露服务 cat \u003e/data/k8s-yaml/dashboard/ingress.yaml \u003c\u003cEOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kubernetes-dashboard namespace: kube-system annotations: kubernetes.io/ingress.class: traefik spec: rules: - host: dashboard.zq.com http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 443 EOF ","date":"2020-10-01","objectID":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/:1:2","tags":["K8S","转载"],"title":"07_K8S_web管理方式dashboard","uri":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/"},{"categories":["转载","K8S"],"content":"1.3 创建相关资源 1.3.1 在任意node上创建 kubectl create -f http://k8s-yaml.zq.com/dashboard/rbac.yaml kubectl create -f http://k8s-yaml.zq.com/dashboard/dp.yaml kubectl create -f http://k8s-yaml.zq.com/dashboard/svc.yaml kubectl create -f http://k8s-yaml.zq.com/dashboard/ingress.yaml 1.3.2 添加域名解析 vi /var/named/zq.com.zone dashboard A 10.4.7.10 # 注意前滚serial编号 systemctl restart named 1.3.3 通过浏览器验证 在本机浏览器上访问[http://dashboard.zq.com](http://dashboard.zq.com),如果出来web界面,表示部署成功 可以看到安装1.8版本的dashboard，默认是可以跳过验证的： ","date":"2020-10-01","objectID":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/:1:3","tags":["K8S","转载"],"title":"07_K8S_web管理方式dashboard","uri":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/"},{"categories":["转载","K8S"],"content":"2 升级dashboard版本 跳过登录是不科学的，因为我们在配置dashboard的rbac权限时，绑定的角色是system:admin，这个是集群管理员的角色，权限很大，如果任何人都可跳过登录直接使用,那你就等着背锅吧 ","date":"2020-10-01","objectID":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/:2:0","tags":["K8S","转载"],"title":"07_K8S_web管理方式dashboard","uri":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/"},{"categories":["转载","K8S"],"content":"2.1 把版本换成1.10以上版本 在前面我们已经同时下载了1.10.1版本的docker镜像 2.1.1 在线修改直接使用 kubectl edit deploy kubernetes-dashboard -n kube-system 2.2.2 等待滚动发布 [root@hdss7-21 ~]# kubectl -n kube-system get pod|grep dashboard kubernetes-dashboard-5bccc5946b-vgk5n 1/1 Running 0 20s kubernetes-dashboard-b75bfb487-h7zft 0/1 Terminating 0 2m27s [root@hdss7-21 ~]# kubectl -n kube-system get pod|grep dashboard kubernetes-dashboard-5bccc5946b-vgk5n 1/1 Running 0 52s 2.2.3 刷新dashboard页面： 可以看到这里原来的skip跳过已经没有了，我们如果想登陆，必须输入token，那我们如何获取token呢： ","date":"2020-10-01","objectID":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/:2:1","tags":["K8S","转载"],"title":"07_K8S_web管理方式dashboard","uri":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/"},{"categories":["转载","K8S"],"content":"2.2 使用token登录 2.2.1 首先获取secret资源列表 kubectl get secret -n kube-system 2.2.2 获取角色的详情 列表中有很多角色,不同到角色有不同的权限,找到想要的角色dashboard-admin后,再用describe命令获取详情 kubectl -n kube-system describe secrets kubernetes-dashboard-admin-token-85gmd 找到详情中的token字段,就是我们需要用来登录的东西 拿到token去尝试登录,发现仍然登录不了,因为必须使用https登录,所以需要申请证书 2.2.3 申请证书 申请证书在7.200主机上 创建json文件: cd /opt/certs/ cat \u003e/opt/certs/dashboard-csr.json \u003c\u003cEOF { \"CN\": \"*.zq.com\", \"hosts\": [ ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"beijing\", \"L\": \"beijing\", \"O\": \"zq\", \"OU\": \"ops\" } ] } EOF 申请证书 cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=server \\ dashboard-csr.json |cfssl-json -bare dashboard 查看申请的证书 [root@hdss7-200 certs]# ll |grep dash -rw-r--r-- 1 root root 993 May 4 12:08 dashboard.csr -rw-r--r-- 1 root root 280 May 4 12:08 dashboard-csr.json -rw------- 1 root root 1675 May 4 12:08 dashboard-key.pem -rw-r--r-- 1 root root 1359 May 4 12:08 dashboard.pem 2.2.4 前端nginx服务部署证书 在7.11,7.12两个前端代理上,都做相同操作 拷贝证书: mkdir /etc/nginx/certs scp 10.4.7.200:/opt/certs/dashboard.pem /etc/nginx/certs scp 10.4.7.200:/opt/certs/dashboard-key.pem /etc/nginx/certs 创建nginx配置 cat \u003e/etc/nginx/conf.d/dashboard.zq.com.conf \u003c\u003c'EOF' server { listen 80; server_name dashboard.zq.com; rewrite ^(.*)$ https://${server_name}$1 permanent; } server { listen 443 ssl; server_name dashboard.zq.com; ssl_certificate \"certs/dashboard.pem\"; ssl_certificate_key \"certs/dashboard-key.pem\"; ssl_session_cache shared:SSL:1m; ssl_session_timeout 10m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / { proxy_pass http://default_backend_traefik; proxy_set_header Host $http_host; proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for; } } EOF 重启nginx服务 nginx -t nginx -s reload 2.2.5 再次登录dashboard 刷新页面后,再次使用前面的token登录,可以成功登录进去了 ","date":"2020-10-01","objectID":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/:2:2","tags":["K8S","转载"],"title":"07_K8S_web管理方式dashboard","uri":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/"},{"categories":["转载","K8S"],"content":"2.3 授权细则思考 登录是登录了，但是我们要思考一个问题，我们使用rbac授权来访问dashboard,如何做到权限精细化呢？比如开发，只能看，不能摸，不同的项目组，看到的资源应该是不一样的，测试看到的应该是测试相关的资源 ","date":"2020-10-01","objectID":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/:2:3","tags":["K8S","转载"],"title":"07_K8S_web管理方式dashboard","uri":"/07_k8s_web%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8Fdashboard/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"08_K8S_交付实战-架构说明并准备zk集群 ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:0:0","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"1 交付的服务架构图： ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:1:0","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"1.1 架构图解 最上面一排为K8S集群外服务 1.1 代码仓库使用基于git的gitee 1.2 注册中心使用3台zk组成集群 1.3 用户通过ingress暴露出去的服务进行访问 中间层是K8S集群内服务 2.1 jenkins以容器方式运行,数据目录通过共享磁盘做持久化 2.2 整套dubbo微服务都以POD方式交付,通过zk集群通信 2.3 需要提供的外部访问的服务通过ingress方式暴露 最下层是运维主机层 3.1 harbor是docker私有仓库,存放docker镜像 3.2 POD相关yaml文件创建在运维主机特定目录 3.3 在K8S集群内通过nginx提供的下载连接应用yaml配置 ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:1:1","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"1.2 交付说明: docker虽然可以部署有状态服务,但如果不是有特别需要,还是建议不要部署有状态服务 K8S同理,也不建议部署有状态服务，如mysql，zk等。 因此手动将zookeeper创建集群提供给dubbo使用 ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:1:2","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"2 部署ZK集群 集群分布：7-11，7-12，7-21 zk是java服务，需要依赖jdk ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:2:0","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"2.1 二进制安装JDK jdk请自行下载,只要是1.8版本的就可以,rpm安装或二进制安装均可： 2.1.1 解压jdk mkdir /opt/src mkdir /usr/java cd /opt/src tar -xf jdk-8u221-linux-x64.tar.gz -C /usr/java/ ln -s /usr/java/jdk1.8.0_221/ /usr/java/jdk 2.1.2 写入环境变量 cat \u003e\u003e/etc/profile \u003c\u003c'EOF' #JAVA HOME export JAVA_HOME=/usr/java/jdk export PATH=$JAVA_HOME/bin:$JAVA_HOME/bin:$PATH export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar EOF # 使环境变量生效 source /etc/profile 验证结果 [root@hdss7-11 ~]# java -version java version \"1.8.0_221\" Java(TM) SE Runtime Environment (build 1.8.0_221-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.221-b11, mixed mode) ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:2:1","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"2.2 二进制安装zk 2.2.1 下载zookeeper 下载地址 wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz tar -zxf zookeeper-3.4.14.tar.gz -C /opt/ ln -s /opt/zookeeper-3.4.14/ /opt/zookeeper 2.2.2 创建zk配置文件： cat \u003e/opt/zookeeper/conf/zoo.cfg \u003c\u003c'EOF' tickTime=2000 initLimit=10 syncLimit=5 dataDir=/data/zookeeper/data dataLogDir=/data/zookeeper/logs clientPort=2181 server.1=zk1.zq.com:2888:3888 server.2=zk2.zq.com:2888:3888 server.3=zk3.zq.com:2888:3888 EOF 创建相关目录 mkdir -p /data/zookeeper/data mkdir -p /data/zookeeper/logs 2.2.3 创建集群配置 给每个zk不同的myid,以便区分主从 #7-11上 echo 1 \u003e /data/zookeeper/data/myid #7-12上 echo 2 \u003e /data/zookeeper/data/myid #7-21上 echo 3 \u003e /data/zookeeper/data/myid 2.2.4 修改dns解析 到7.11上增加dns解析记录 vi /var/named/zq.com.zone ... zk1 A 10.4.7.11 zk2 A 10.4.7.12 zk3 A 10.4.7.21 #验证结果 ~]# dig -t A zk1.zq.com +short 10.4.7.11 ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:3:0","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"2.3 启动zk集群 ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:3:1","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"2.3.1 启动zookeeper 在每台zk机器上都执行此操作 /opt/zookeeper/bin/zkServer.sh start 2.3.2 检查zk启动情况 ~]# ss -ln|grep 2181 tcp LISTEN 0 50 :::2181 :::* 2.3.3 检查zk集群情况 [root@hdss7-11 ~]# /opt/zookeeper/bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /opt/zookeeper/bin/../conf/zoo.cfg Mode: follower [root@hdss7-12 ~]# /opt/zookeeper/bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /opt/zookeeper/bin/../conf/zoo.cfg Mode: leader [root@hdss7-21 ~]# /opt/zookeeper/bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /opt/zookeeper/bin/../conf/zoo.cfg Mode: follower 到此，zookeeper集群就搭建好了。 ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:3:2","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"3 准备java运行底包 运维主机上操作 ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:4:0","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"3.1 拉取原始底包 docker pull stanleyws/jre8:8u112 docker tag fa3a085d6ef1 harbor.zq.com/public/jre:8u112 docker push harbor.zq.com/public/jre:8u112 ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:4:1","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"3.2 制作新底包 mkdir -p /data/dockerfile/jre8/ cd /data/dockerfile/jre8/ 3.2.1 制作dockerfile cat \u003eDockerfile \u003c\u003c'EOF' FROM harbor.zq.com/public/jre:8u112 RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026\u0026\\ echo 'Asia/Shanghai' \u003e/etc/timezone ADD config.yml /opt/prom/config.yml ADD jmx_javaagent-0.3.1.jar /opt/prom/ WORKDIR /opt/project_dir ADD entrypoint.sh /entrypoint.sh CMD [\"sh\",\"/entrypoint.sh\"] EOF 3.2.2准备dockerfile需要的文件 添加config.yml 此文件是为后面用普罗米修斯监控做准备的 cat \u003econfig.yml \u003c\u003c'EOF' --- rules: - pattern: '.*' EOF 下载jmx_javaagent,监控jvm信息： wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.3.1/jmx_prometheus_javaagent-0.3.1.jar -O jmx_javaagent-0.3.1.jar 创建entrypoint.sh启动脚本： 使用exec 来运行java的jar包，能够使脚本将自己的pid 为‘1’ 传递给java进程，避免docker容器因没有前台进程而退出。并且不要加\u0026符。 cat \u003eentrypoint.sh \u003c\u003c'EOF' #!/bin/sh M_OPTS=\"-Duser.timezone=Asia/Shanghai -javaagent:/opt/prom/jmx_javaagent-0.3.1.jar=$(hostname -i):${M_PORT:-\"12346\"}:/opt/prom/config.yml\" C_OPTS=${C_OPTS} JAR_BALL=${JAR_BALL} exec java -jar ${M_OPTS} ${C_OPTS} ${JAR_BALL} EOF 3.2.3 构建底包并上传 在harbor中创建名为base的公开仓库,用来存放自己自定义的底包 docker build . -t harbor.zq.com/base/jre8:8u112 docker push harbor.zq.com/base/jre8:8u112 ","date":"2020-10-01","objectID":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/:4:2","tags":["K8S","转载"],"title":"08_K8S_交付实战-架构说明并准备zk集群","uri":"/08_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%E5%B9%B6%E5%87%86%E5%A4%87zk%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"09_K8S_交付实战-交付jenkins到k8s集群 ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:0:0","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"1 准备jenkins镜像 准备镜像的操作在7.200运维机上完成 ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:1:0","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"1.1 下载官方镜像 docker pull jenkins/jenkins:2.190.3 docker tag jenkins/jenkins:2.190.3 harbor.zq.com/public/jenkins:v2.190.3 docker push harbor.zq.com/public/jenkins:v2.190.3 ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:1:1","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"1.2 修改官方镜像 基于官方jenkins镜像,编写dockerfile做个性化配置 1.2.1 创建目录 mkdir -p /data/dockerfile/jenkins/ cd /data/dockerfile/jenkins/ 1.2.2 创建dockerfile cat \u003e/data/dockerfile/jenkins/Dockerfile \u003c\u003c'EOF' FROM harbor.zq.com/public/jenkins:v2.190.3 #定义启动jenkins的用户 USER root #修改时区为东八区 RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026\u0026\\ echo 'Asia/Shanghai' \u003e/etc/timezone #加载用户密钥，使用ssh拉取dubbo代码需要 ADD id_rsa /root/.ssh/id_rsa #加载运维主机的docker配置文件，里面包含登录harbor仓库的认证信息。 ADD config.json /root/.docker/config.json #在jenkins容器内安装docker客户端，docker引擎用的是宿主机的docker引擎 ADD get-docker.sh /get-docker.sh # 跳过ssh时候输入yes的交互步骤，并执行安装docker RUN echo \" StrictHostKeyChecking no\" \u003e/etc/ssh/ssh_config \u0026\u0026\\ /get-docker.sh EOF 1.2.3 准备dockerfile所需文件 创建秘钥对: ssh-keygen -t rsa -b 2048 -C \"lg@126.com\" -N \"\" -f /root/.ssh/id_rsa cp /root/.ssh/id_rsa /data/dockerfile/jenkins/ 邮箱请根据自己的邮箱自行修改 创建完成后记得把公钥放到gitee的信任中 获取docker.sh脚本: curl -fsSL get.docker.com -o /data/dockerfile/jenkins/get-docker.sh chmod u+x /data/dockerfile/jenkins/get-docker.sh 拷贝config.json文件: cp /root/.docker/config.json /data/dockerfile/jenkins/ 1.2.4 harbor中创建私有仓库infra 1.2.5 构建自定义的jenkins镜像 cd /data/dockerfile/jenkins/ docker build . -t harbor.zq.com/infra/jenkins:v2.190.3 docker push harbor.zq.com/infra/jenkins:v2.190.3 ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:1:2","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"2 准备jenkins运行环境 ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:2:0","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"2.1 专有名称空间和secret资源 2.1.1 创建专有namespace 创建专有名词空间infra的目录是将jenkins等运维相关软件放到同一个namespace下,便于统一管理以及和其他资源分开 kubectl create ns infra 2.1.2 创建访问harbor的secret规则 Secret用来保存敏感信息，例如密码、OAuth 令牌和 ssh key等,有三种类型: Opaque： base64 编码格式的 Secret，用来存储密码、密钥等,可以反解,加密能力弱 kubernetes.io/dockerconfigjson： 用来存储私有docker registry的认证信息。 kubernetes.io/service-account-token： 用于被serviceaccount引用，serviceaccout 创建时Kubernetes会默认创建对应的secret 前面dashborad部分以及用过了 访问docker的私有仓库,必须要创建专有的secret类型,创建方法如下: kubectl create secret docker-registry harbor \\ --docker-server=harbor.zq.com \\ --docker-username=admin \\ --docker-password=Harbor12345 \\ -n infra # 查看结果 ~]# kubectl -n infra get secrets NAME TYPE DATA AGE default-token-rkg7q kubernetes.io/service-account-token 3 19s harbor kubernetes.io/dockerconfigjson 1 12s 解释命令： 创建一条secret，资源类型是docker-registry，名字是 harbor 并指定docker仓库地址、访问用户、密码、仓库名 ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:2:1","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"2.2 创建NFS共享存储 jenkins中一些数据需要持久化的，可以使用共享存储进行挂载： 这里使用最简单的NFS共享存储，因为k8s默认支持nfs模块 如果使用其他类型的共享存储 2.2.1 运维机部署NFS yum install nfs-utils -y echo '/data/nfs-volume 10.4.7.0/24(rw,no_root_squash)' \u003e\u003e/etc/exports mkdir -p /data/nfs-volume/jenkins_home systemctl start nfs systemctl enable nfs # 查看结果 ~]# showmount -e Export list for hdss7-200: /data/nfs-volume 10.4.7.0/24 2.2.2 node节点安装nfs yum install nfs-utils -y ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:2:2","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"2.3 运维机创建jenkins资源清单 mkdir /data/k8s-yaml/jenkins 2.3.1 创建depeloy清单 有两个需要注意的地方: 挂载了宿主机的docker.sock 使容器内的docker客户端可以直接与宿主机的docker引擎进行通信 在使用私有仓库的时候，资源清单中，一定要声明： imagePullSecrets: - name: harbor cat \u003e/data/k8s-yaml/jenkins/dp.yaml \u003c\u003cEOF kind: Deployment apiVersion: extensions/v1beta1 metadata: name: jenkins namespace: infra labels: name: jenkins spec: replicas: 1 selector: matchLabels: name: jenkins template: metadata: labels: app: jenkins name: jenkins spec: volumes: - name: data nfs: server: hdss7-200 path: /data/nfs-volume/jenkins_home - name: docker hostPath: path: /run/docker.sock type: '' containers: - name: jenkins image: harbor.zq.com/infra/jenkins:v2.190.3 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 protocol: TCP env: - name: JAVA_OPTS value: -Xmx512m -Xms512m volumeMounts: - name: data mountPath: /var/jenkins_home - name: docker mountPath: /run/docker.sock imagePullSecrets: - name: harbor securityContext: runAsUser: 0 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 revisionHistoryLimit: 7 progressDeadlineSeconds: 600 EOF ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:2:3","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"2.3.2 创建service清单 cat \u003e/data/k8s-yaml/jenkins/svc.yaml \u003c\u003cEOF kind: Service apiVersion: v1 metadata: name: jenkins namespace: infra spec: ports: - protocol: TCP port: 80 targetPort: 8080 selector: app: jenkins EOF 2.3.3 创建ingress清单 cat \u003e/data/k8s-yaml/jenkins/ingress.yaml \u003c\u003cEOF kind: Ingress apiVersion: extensions/v1beta1 metadata: name: jenkins namespace: infra spec: rules: - host: jenkins.zq.com http: paths: - path: / backend: serviceName: jenkins servicePort: 80 EOF ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:2:4","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"3 交付jenkins ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:3:0","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"3.1 应用jenkins资源清单 3.1.2 部署jenkins 任意node节点 kubectl create -f http://k8s-yaml.zq.com/jenkins/dp.yaml kubectl create -f http://k8s-yaml.zq.com/jenkins/svc.yaml kubectl create -f http://k8s-yaml.zq.com/jenkins/ingress.yaml 启动时间很长,等待结果 kubectl get pod -n infra 3.1.2 验证jenkins容器状态 docker exec -it 8ff92f08e3aa /bin/bash # 查看用户 whoami # 查看时区 date # 查看是否能用宿主机的docker引擎 docker ps # 看是否能免密访问gitee ssh -i /root/.ssh/id_rsa -T git@gitee.com # 是否能访问是否harbor仓库 docker login harbor.zq.com 3.1.3 查看持久化结果和密码 到运维机上查看持久化数据是否成功存放到共享存储 ~]# ll /data/nfs-volume/jenkins_home total 36 -rw-r--r-- 1 root root 1643 May 5 13:18 config.xml -rw-r--r-- 1 root root 50 May 5 13:13 copy_reference_file.log -rw-r--r-- 1 root root 156 May 5 13:14 hudson.model.UpdateCenter.xml -rw------- 1 root root 1712 May 5 13:14 identity.key.enc -rw-r--r-- 1 root root 7 May 5 13:14 jenkins.install.UpgradeWizard.state -rw-r--r-- 1 root root 171 May 5 13:14 jenkins.telemetry.Correlator.xml drwxr-xr-x 2 root root 6 May 5 13:13 jobs drwxr-xr-x 3 root root 19 May 5 13:14 logs -rw-r--r-- 1 root root 907 May 5 13:14 nodeMonitors.xml drwxr-xr-x 2 root root 6 May 5 13:14 nodes drwxr-xr-x 2 root root 6 May 5 13:13 plugins -rw-r--r-- 1 root root 64 May 5 13:13 secret.key -rw-r--r-- 1 root root 0 May 5 13:13 secret.key.not-so-secret drwx------ 4 root root 265 May 5 13:14 secrets drwxr-xr-x 2 root root 67 May 5 13:19 updates drwxr-xr-x 2 root root 24 May 5 13:14 userContent drwxr-xr-x 3 root root 56 May 5 13:14 users drwxr-xr-x 11 root root 4096 May 5 13:13 war 找到jenkins初始化的密码 ~]# cat /data/nfs-volume/jenkins_home/secrets/initialAdminPassword 02f69d78026d489e87b01332f1caa85a 3.1.4 替换jenkins插件源 cd /data/nfs-volume/jenkins_home/updates sed -i 's#http:\\/\\/updates.jenkins-ci.org\\/download#https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins#g' default.json sed -i 's#http:\\/\\/www.google.com#https:\\/\\/www.baidu.com#g' default.json ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:3:1","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"3.2 解析jenkins jenkins部署成功后后,需要给他添加外网的域名解析 vi /var/named/zq.com.zone jenkins A 10.4.7.10 # 重启服务 systemctl restart named ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:3:2","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"3.3 初始化jenkins 浏览器访问[http://jenkins.zq.com](http://jenkins.zq.com),使用前面的密码进入jenkins 进入后操作: 跳过安装自动安装插件的步骤 在manage jenkins-\u003eConfigure Global Security菜单中设置 2.1 允许匿名读：勾选allow anonymous read access 2.2 允许跨域：勾掉prevent cross site request forgery exploits 搜索并安装蓝海插件blue ocean 设置用户名密码为admin:admin123 ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:3:3","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"3.4 给jenkins配置maven环境 因为jenkins的数据目录已经挂载到了NFS中做持久化,因此可以直接将maven放到NFS目录中,同时也就部署进了jenkins 3.4.1 下载并解压 wget https://archive.apache.org/dist/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz tar -zxf apache-maven-3.6.1-bin.tar.gz -C /data/nfs-volume/jenkins_home/ mv /data/nfs-volume/jenkins_home/{apache-,}maven-3.6.1 cd /data/nfs-volume/jenkins_home/maven-3.6.1 3.4.2 初始化maven配置： 修改下载仓库地址,除了\u003cmirror\u003e中是新增的阿里云仓库地址外,其他内容都是settings.xml中原有的配置(只是清除了注释内容) cat \u003econf/settings.xml \u003c\u003c'EOF' \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003csettings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"\u003e \u003cpluginGroups\u003e \u003c/pluginGroups\u003e \u003cproxies\u003e \u003c/proxies\u003e \u003cservers\u003e \u003c/servers\u003e \u003cmirrors\u003e \u003cmirror\u003e \u003cid\u003enexus-aliyun\u003c/id\u003e \u003cmirrorOf\u003e*\u003c/mirrorOf\u003e \u003cname\u003eNexus aliyun\u003c/name\u003e \u003curl\u003ehttp://maven.aliyun.com/nexus/content/groups/public\u003c/url\u003e \u003c/mirror\u003e \u003c/mirrors\u003e \u003cprofiles\u003e \u003c/profiles\u003e \u003c/settings\u003e EOF ","date":"2020-10-01","objectID":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/:3:4","tags":["K8S","转载"],"title":"09_K8S_交付实战-交付jenkins到k8s集群","uri":"/09_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98jenkins%E5%88%B0k8s%E9%9B%86%E7%BE%A4/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"10_K8S_交付实战-流水线构建dubbo服务 ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:0:0","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"1 jenkins流水线准备工作 ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:1:0","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"1.1 参数构建要点 jenkins流水线配置的java项目的十个常用参数: 参数名 作用 举例或说明 app_name 项目名 dubbo_demo_service image_name docker镜像名 app/dubbo-demo-service git_repo 项目的git地址 https://x.com/x/x.git git_ver 项目的git分支或版本号 master add_tag 镜像标签,常用时间戳 191203_1830 mvn_dir 执行mvn编译的目录 ./ target_dir 编译产生包的目录 ./target mvn_cmd 编译maven项目的命令 mvc clean package -Dmaven. base_image 项目的docker底包 不同的项目底包不一样,下拉选择 maven maven软件版本 不同的项目可能maven环境不一样 除了base_image和maven是choice parameter，其他都是string parameter ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:1:1","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"1.2 创建流水线 1.2.1 创建流水线 创建名为dubbo-demo的流水线(pipeline),并设置Discard old builds 为如下 Discard old builds选项 值 Days to keep builds 3 Max # of builds to keep 30 1.2.2 添加10个构建参数 This project is parameterized点击Add Parameter,分别添加如下10个参数 #第1个参数 参数类型 : String Parameter Name : app_name Description : 项目名 eg:dubbo-demo-service #第2个参数 参数类型 : String Parameter Name : image_name Description : docker镜像名 eg: app/dubbo-demo-service #第3个参数 参数类型 : String Parameter Name : git_repo Description : 仓库地址 eg: https://gitee.com/xxx/xxx.git #第4个参数 参数类型 : String Parameter Name : git_ver Description : 项目的git分支或版本号 #第5个参数 参数类型 : String Parameter Name : add_tag Description : 给docker镜像添加标签组合的一部分,如 $git_ver_$add_tag=master_191203_1830 #第6个参数 参数类型 : String Parameter Name : mvn_dir Default Value : ./ Description : 执行mvn编译的目录,默认是项目根目录, eg: ./ #第7个参数 参数类型 : String Parameter Name : target_dir Default Value : ./target Description : 编译产生的war/jar包目录 eg: ./dubbo-server/target #第8个参数 参数类型 : String Parameter Name : mvn_cmd Default Value : mvn clean package -Dmaven.test.skip=true Description : 编译命令,常加上-e -q参数只输出错误 #第9个参数 参数类型 : Choice Parameter Name : base_image Choices : base/jre7:7u80 base/jre8:8u112 Description : 项目的docker底包 #第10个参数 参数类型 : Choice Parameter Name : maven Choices : 3.6.1 3.2.5 2.2.1 Description : 执行编译使用maven软件版本 1.2.3 添加完成效果如下: 1.2.4 添加pipiline代码 流水线构建所用的pipiline代码语法比较有专门的生成工具 以下语句的作用大致是分为四步:拉代码-\u003e构建包-\u003e移动包-打docker镜像并推送 pipeline { agent any stages { stage('pull') { //get project code from repo steps { sh \"git clone ${params.git_repo} ${params.app_name}/${env.BUILD_NUMBER} \u0026\u0026 cd ${params.app_name}/${env.BUILD_NUMBER} \u0026\u0026 git checkout ${params.git_ver}\" } } stage('build') { //exec mvn cmd steps { sh \"cd ${params.app_name}/${env.BUILD_NUMBER} \u0026\u0026 /var/jenkins_home/maven-${params.maven}/bin/${params.mvn_cmd}\" } } stage('package') { //move jar file into project_dir steps { sh \"cd ${params.app_name}/${env.BUILD_NUMBER} \u0026\u0026 cd ${params.target_dir} \u0026\u0026 mkdir project_dir \u0026\u0026 mv *.jar ./project_dir\" } } stage('image') { //build image and push to registry steps { writeFile file: \"${params.app_name}/${env.BUILD_NUMBER}/Dockerfile\", text: \"\"\"FROM harbor.zq.com/${params.base_image} ADD ${params.target_dir}/project_dir /opt/project_dir\"\"\" sh \"cd ${params.app_name}/${env.BUILD_NUMBER} \u0026\u0026 docker build -t harbor.zq.com/${params.image_name}:${params.git_ver}_${params.add_tag} . \u0026\u0026 docker push harbor.zq.com/${params.image_name}:${params.git_ver}_${params.add_tag}\" } } } } ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:1:2","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"1.3 用流水线完成dubbo-service的构建 记得先在harbor中创建私有仓库app 1.3.1 选择参数化构建 进入dubbo-demo后,选择的参数化构建build with parameters ,填写10个构建的参数 参数名 参数值 app_name dubbo-demo-service image_name app/dubbo-demo-service git_repo https://gitee.com/noah-luo/dubbo-demo-service.git git_ver master add_tag 200509_0800 mvn_dir ./ target_dir ./dubbo-server/target mvn_cmd mvn clean package -Dmaven.test.skip=true base_image base/jre8:8u112 maven 3.6.1 1.3.2 填写完成效果如下 1.3.3 执行构建并检查 填写完以后执行bulid 第一次构建需要下载很多依赖包，时间很长，抽根烟，喝杯茶 经过漫长的等待后，已经构建完成了 点击打开 Blue Ocean查看构建历史及过程： 检查harbor是否已经有这版镜像： ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:1:3","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"2 交付dubbo-service到k8s ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:2:0","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"2.1 准备资源清单 创建清单操作都在7.200上操作 mkdir /data/k8s-yaml/dubbo-server/ cd /data/k8s-yaml/dubbo-server 2.1.1 创建depeloy清单 cat \u003edp.yaml \u003c\u003cEOFkind:DeploymentapiVersion:extensions/v1beta1metadata:name:dubbo-demo-servicenamespace:applabels:name:dubbo-demo-servicespec:replicas:1selector:matchLabels:name:dubbo-demo-servicetemplate:metadata:labels:app:dubbo-demo-servicename:dubbo-demo-servicespec:containers:- name:dubbo-demo-serviceimage:harbor.zq.com/app/dubbo-demo-service:master_200509_0800ports:- containerPort:20880protocol:TCPenv:- name:JAR_BALLvalue:dubbo-server.jarimagePullPolicy:IfNotPresentimagePullSecrets:- name:harborrestartPolicy:AlwaysterminationGracePeriodSeconds:30securityContext:runAsUser:0schedulerName:default-schedulerstrategy:type:RollingUpdaterollingUpdate:maxUnavailable:1maxSurge:1revisionHistoryLimit:7progressDeadlineSeconds:600EOF 需要根据自己构建镜像的tag来修改image dubbo的server服务,只向zk注册并通过zk与dobbo的web交互,不需要对外提供服务 因此不需要service资源和ingress资源 ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:2:1","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"2.2 创建k8s资源 创建K8S资源的操作,在任意node节点上操作即可 2.2.1 创建app名称空间 业务资源和运维资源等应该通过名称空间来隔离,因此创建专有名称空间app kubectl create namespace app 2.2.2 创建secret资源 我们的业务镜像是harbor中的私有项目，所以需要创建docker-registry的secret资源： kubectl -n app \\ create secret docker-registry harbor \\ --docker-server=harbor.zq.com \\ --docker-username=admin \\ --docker-password=Harbor12345 2.2.3 应用资源清单 kubectl apply -f http://k8s-yaml.zq.com/dubbo-server/dp.yaml 3分钟后检查启动情况 # 检查pod是否创建： ~]# kubectl -n app get pod NAME READY STATUS RESTARTS AGE dubbo-demo-service-79574b6879-cxkls 1/1 Running 0 24s # 检查是否启动成功： ~]# kubectl -n app logs dubbo-demo-service-79574b6879-cxkls --tail=2 Dubbo server started Dubbo 服务端已经启动 到zk服务器检查是否有服务注册 sh /opt/zookeeper/bin/zkCli.sh [zk: localhost:2181(CONNECTED) 0] ls / [dubbo, zookeeper] [zk: localhost:2181(CONNECTED) 1] ls /dubbo [com.od.dubbotest.api.HelloService] ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:2:2","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"3 交付dubbo-monitor监控服务到k8s dobbo-monitor源码地址: https://github.com/Jeromefromcn/dubbo-monitor.git dubbo-monitor是监控zookeeper状态的一个服务，另外还有dubbo-admin，效果一样 ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:3:0","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"3.1 制作dobbo-monitor镜像 制作镜像在管理机7.200上操作 3.1.1 下载源码 cd /opt/src wget https://github.com/Jeromefromcn/dubbo-monitor/archive/master.zip yum -y install unzip unzip master.zip mv dubbo-monitor-mster /data/dockerfile/dubbo-monitor cd /data/dockerfile/dubbo-monitor 3.1.2 修改配置文件： 直接覆盖它原始的配置 其实它原本就没什么内容,只是修改了addr,端口,目录等 cat \u003edubbo-monitor-simple/conf/dubbo_origin.properties \u003c\u003c'EOF' dubbo.container=log4j,spring,registry,jetty dubbo.application.name=simple-monitor dubbo.application.owner= dubbo.registry.address=zookeeper://zk1.zq.com:2181?backup=zk2.zq.com:2181,zk3.zq.com:2181 dubbo.protocol.port=20880 dubbo.jetty.port=8080 dubbo.jetty.directory=/dubbo-monitor-simple/monitor dubbo.statistics.directory=/dubbo-monitor-simple/statistics dubbo.charts.directory=/dubbo-monitor-simple/charts dubbo.log4j.file=logs/dubbo-monitor.log dubbo.log4j.level=WARN EOF 3.1.3 优化Dockerfile启动脚本 # 修改jvm资源限制(非必须) sed -i '/Xmx2g/ s#128m#16m#g' ./dubbo-monitor-simple/bin/start.sh sed -i '/Xmx2g/ s#256m#32m#g' ./dubbo-monitor-simple/bin/start.sh sed -i '/Xmx2g/ s#2g#128m#g' ./dubbo-monitor-simple/bin/start.sh # 修改nohup为exec不能改去掉改行最后的\u0026符号 sed -ri 's#^nohup(.*) \u0026#exec\\1#g' ./dubbo-monitor-simple/bin/start.sh # 删除exec命令行后面所有行 sed -i '66,$d' ./dubbo-monitor-simple/bin/start.sh 3.1.4 构建并上传 docker build . -t harbor.zq.com/infra/dubbo-monitor:latest docker push harbor.zq.com/infra/dubbo-monitor:latest ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:3:1","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"3.2 创建资源配置清单 3.2.1 准备目录 mkdir /data/k8s-yaml/dubbo-monitor cd /data/k8s-yaml/dubbo-monitor 3.2.2 创建deploy资源文件 cat \u003edp.yaml \u003c\u003cEOF kind: Deployment apiVersion: extensions/v1beta1 metadata: name: dubbo-monitor namespace: infra labels: name: dubbo-monitor spec: replicas: 1 selector: matchLabels: name: dubbo-monitor template: metadata: labels: app: dubbo-monitor name: dubbo-monitor spec: containers: - name: dubbo-monitor image: harbor.zq.com/infra/dubbo-monitor:latest ports: - containerPort: 8080 protocol: TCP - containerPort: 20880 protocol: TCP imagePullPolicy: IfNotPresent imagePullSecrets: - name: harbor restartPolicy: Always terminationGracePeriodSeconds: 30 securityContext: runAsUser: 0 schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 revisionHistoryLimit: 7 progressDeadlineSeconds: 600 EOF 3.2.3 创建service资源文件 cat \u003esvc.yaml \u003c\u003cEOF kind: Service apiVersion: v1 metadata: name: dubbo-monitor namespace: infra spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: dubbo-monitor EOF 3.2.4 创建ingress资源文件 cat \u003eingress.yaml \u003c\u003cEOF kind: Ingress apiVersion: extensions/v1beta1 metadata: name: dubbo-monitor namespace: infra spec: rules: - host: dubbo-monitor.zq.com http: paths: - path: / backend: serviceName: dubbo-monitor servicePort: 8080 EOF ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:3:2","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"3.3 创建dobbo-miniotr服务 3.3.1 应用资源配置清单 在任意node节点 kubectl apply -f http://k8s-yaml.zq.com/dubbo-monitor/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/dubbo-monitor/svc.yaml kubectl apply -f http://k8s-yaml.zq.com/dubbo-monitor/ingress.yaml 验证： ~]# kubectl -n infra get pod NAME READY STATUS RESTARTS AGE dubbo-monitor-d9675688c-sctsx 1/1 Running 0 29s jenkins-7cd8b95d79-6vrbn 1/1 Running 0 3d2h 3.3.2 添加dns解析 这个服务是有web页面的，创建了ingress和service资源的,所以需要添加dns解析 vi /var/named/zq.com.zone dobbo-monitor A 10.4.7.10 重启并验证 systemctl restart named dig -t A dubbo-monitor.zq.com @10.4.7.11 +short 3.3.3 访问monitor的web页面 访问dubbo-monitor.zq.com 这里已经可以看到我们之前部署的dubbo-demo-service服务了，启动了两个进程来提供服务。 至此，dubbo-monitor监控服务已经部署完成。 ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:3:3","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"4 构建dubbo-consumer服务 ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:4:0","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"4.1 构建docker镜像 4.1.1 获取私有仓库代码 之前创建的dubbo-service是微服务的提供者,现在创建一个微服务的消费者 使用git@gitee.com:noah-luo/dubbo-demo-web.git这个私有仓库中的代码构建消费者 先从[https://gitee.com/sunx66/dubbo-demo-service](https://gitee.com/sunx66/dubbo-demo-service)这里fork到自己仓库,在设为私有 并修改zk的配置 4.1.2 配置流水线 之前已经在jenkins配置好了流水线，只需要填写参数就行了。 参数名 参数值 app_name dubbo-demo-consumer image_name app/dubbo-demo-consumer git_repo git@gitee.com:noah-luo/dubbo-demo-web.git git_ver master add_tag 200506_1430 mvn_dir ./ target_dir ./dubbo-client/target mvn_cmd mvn clean package -Dmaven.test.skip=true base_image base/jre8:8u112 maven 3.6.1 4.1.3 查看构建结果 如果构建不报错,则应该已经推送到harbor仓库中了,这时我们直接再给镜像一个新tag,以便后续模拟更新 docker tag \\ harbor.zq.com/app/dubbo-demo-consumer:master_200506_1430 \\ harbor.zq.com/app/dubbo-demo-consumer:master_200510_1430 docker push harbor.zq.com/app/dubbo-demo-consumer:master_200510_1430 查看harbor仓库 ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:4:1","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"4.2 准备资源配置清单： 先准备目录 mkdir /data/k8s-yaml/dubbo-consumer cd /data/k8s-yaml/dubbo-consumer 4.2.1 创建deploy资源清单 cat \u003edp.yaml \u003c\u003cEOF kind: Deployment apiVersion: extensions/v1beta1 metadata: name: dubbo-demo-consumer namespace: app labels: name: dubbo-demo-consumer spec: replicas: 1 selector: matchLabels: name: dubbo-demo-consumer template: metadata: labels: app: dubbo-demo-consumer name: dubbo-demo-consumer spec: containers: - name: dubbo-demo-consumer image: harbor.zq.com/app/dubbo-demo-consumer:master_200506_1430 ports: - containerPort: 8080 protocol: TCP - containerPort: 20880 protocol: TCP env: - name: JAR_BALL value: dubbo-client.jar imagePullPolicy: IfNotPresent imagePullSecrets: - name: harbor restartPolicy: Always terminationGracePeriodSeconds: 30 securityContext: runAsUser: 0 schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 revisionHistoryLimit: 7 progressDeadlineSeconds: 600 EOF 注意修改镜像的tag 4.2.2 创建service资源清单 cat \u003esvc.yaml \u003c\u003cEOF kind: Service apiVersion: v1 metadata: name: dubbo-demo-consumer namespace: app spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: dubbo-demo-consumer EOF 4.2.3 创建ingress资源清单 cat \u003eingress.yaml \u003c\u003cEOF kind: Ingress apiVersion: extensions/v1beta1 metadata: name: dubbo-demo-consumer namespace: app spec: rules: - host: dubbo-demo.zq.com http: paths: - path: / backend: serviceName: dubbo-demo-consumer servicePort: 8080 EOF ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:4:2","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"4.3 创建K8S资源 4.3.1 应用资源配置清单： kubectl apply -f http://k8s-yaml.zq.com/dubbo-consumer/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/dubbo-consumer/svc.yaml kubectl apply -f http://k8s-yaml.zq.com/dubbo-consumer/ingress.yaml # 查看容器启动成功没 ~]# kubectl get pod -n app NAME READY STATUS RESTARTS AGE dubbo-demo-consumer-b8d86bd5b-wbqhs 1/1 Running 0 6s dubbo-demo-service-79574b6879-cxkls 1/1 Running 0 4h39m 4.3.2 验证启动结果 查看log，是否启动成功： ~]# kubectl -n app logs --tail=2 dubbo-demo-consumer-b8d86bd5b-wbqhs Dubbo client started Dubbo 消费者端启动 检查dubbo-monitor是否已经注册成功： 4.3.3 添加dns解析 vi /var/named/zq.com.zone dubbo-demo A 10.4.7.10 # 重启服务 systemctl restart named # 验证 ~]# dig -t A dubbo-demo.zq.com @10.4.7.11 +short 10.4.7.10 浏览器访问[http://dubbo-demo.zq.com/hello?name=lg](http://dubbo-demo.zq.com/hello?name=lg) ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:4:3","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"4.4 模拟版本升级 接下来我们模拟升级发版，之前已经用同一个镜像打了不同的tag并推送到从库 当然正常发版的顺序是: 提交修改过的代码的代码块 使用jenkins构建新镜像 上传到私有harbor仓库中 更新de文件并apply 4.4.1 修改dp.yaml资源配置清单 修改harbor镜像仓库中对应的tag版本： sed -i 's#master_200506_1430#master_200510_1430#g' dp.yaml 4.4.2 应用修改后的资源配置清单 当然也可以在dashboard中进行在线修改： kubectl apply -f http://k8s-yaml.zq.com/dubbo-consumer/dp.yaml ~]# kubectl -n app get pod NAME READY STATUS RESTARTS AGE dubbo-demo-consumer-84f75b679c-kdwd7 1/1 Running 0 54s dubbo-demo-service-79574b6879-cxkls 1/1 Running 0 4h58m 4.4.3 使用浏览器验证 使用浏览器验证：http://dubbo-demo.zq.com/hello?name=lg 在短暂的超时后,即可正常访问 至此，我们一套完成的dubbo服务就已经交付到k8s集群当中了，并且也演示了如何发版。 ","date":"2020-10-01","objectID":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/:4:4","tags":["K8S","转载"],"title":"10_K8S_交付实战-流水线构建dubbo服务","uri":"/10_k8s_%E4%BA%A4%E4%BB%98%E5%AE%9E%E6%88%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BAdubbo%E6%9C%8D%E5%8A%A1/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"11_K8S配置中心实战-configmap资源 ","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/:0:0","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"1 configmap使用准备 使用configmap前,需要先做如下准备工作 ","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/:1:0","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"1.1 清理资源 先将前面部署的3个dubbo服务的POD个数全部调整(scale)为0个,避免在应用configmap过程中可能的报错,也为了节省资源 直接在dashboard上操作即可, ","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/:1:1","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"1.2 拆分zk集群 将3个zk组成的集群,拆分成独立的zk单机服务,分别表示测试环境和开发环境(节约资源) IP地址 ZK地址 角色 10.4.7.11 zk1.zq.com test测试环境 10.4.7.12 zk2.zq.com pro生产环境 停止3个zk服务 sh /opt/zookeeper/bin/zkServer.sh stop rm -rf /data/zookeeper/data/* rm -rf /data/zookeeper/logs/* 注释掉集群配置 sed -i 's@^server@#server@g' /opt/zookeeper/conf/zoo.cfg 启动zk单机 sh /opt/zookeeper/bin/zkServer.sh start sh /opt/zookeeper/bin/zkServer.sh status ","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/:1:2","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"1.3 创建dubbo-monitor资源清单 老规矩,资源清单在7.200运维机上统一操作 cd /data/k8s-yaml/dubbo-monitor 1.3.1 创建comfigmap清单 cat \u003ecm.yaml \u003c\u003c'EOF' apiVersion: v1 kind: ConfigMap metadata: name: dubbo-monitor-cm namespace: infra data: dubbo.properties: | dubbo.container=log4j,spring,registry,jetty dubbo.application.name=simple-monitor dubbo.application.owner=zqkj dubbo.registry.address=zookeeper://zk1.zq.com:2181 dubbo.protocol.port=20880 dubbo.jetty.port=8080 dubbo.jetty.directory=/dubbo-monitor-simple/monitor dubbo.charts.directory=/dubbo-monitor-simple/charts dubbo.statistics.directory=/dubbo-monitor-simple/statistics dubbo.log4j.file=/dubbo-monitor-simple/logs/dubbo-monitor.log dubbo.log4j.level=WARN EOF 其实就是把dubbo-monitor配置文件中的内容用configmap语法展示出来了 当然最前面加上了相应的元数据信息 如果转换不来格式,也可以使用命令行工具直接将配置文件转换为configmap kubectl create configmap \u003cmap-name\u003e \u003cdata-source\u003e # \u003cmap-name\u003e 是希望创建的ConfigMap的名称，\u003cdata-source\u003e是一个目录、文件和具体值。 案例如下: # 1.通过单个文件创建ConfigMap kubectl create configmap game-config-1 --from-file=/xxx/xxx.properties # 2.通过多个文件创建ConfigMap kubectl create configmap game-config-2 \\ --from-file=/xxx/xxx.properties \\ --from-file=/xxx/www.properties # 3.通过在一个目录下的多个文件创建ConfigMap kubectl create configmap game-config-3 --from-file=/xxx/www/ 1.3.2 修改deploy清单内容 为了和原来的dp.yaml对比,我们新建一个dp-cm.yaml cat \u003edp-cm.yaml \u003c\u003c'EOF' kind: Deployment apiVersion: extensions/v1beta1 metadata: name: dubbo-monitor namespace: infra labels: name: dubbo-monitor spec: replicas: 1 selector: matchLabels: name: dubbo-monitor template: metadata: labels: app: dubbo-monitor name: dubbo-monitor spec: containers: - name: dubbo-monitor image: harbor.zq.com/infra/dubbo-monitor:latest ports: - containerPort: 8080 protocol: TCP - containerPort: 20880 protocol: TCP imagePullPolicy: IfNotPresent #----------------start--------------------------- volumeMounts: - name: configmap-volume mountPath: /dubbo-monitor-simple/conf volumes: - name: configmap-volume configMap: name: dubbo-monitor-cm #----------------end----------------------------- imagePullSecrets: - name: harbor restartPolicy: Always terminationGracePeriodSeconds: 30 securityContext: runAsUser: 0 schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 revisionHistoryLimit: 7 progressDeadlineSeconds: 600 EOF 注释中的内容就是新增在原dp.yaml中增加的内容,解释如下: 申明一个卷,卷名为configmap-volume 指定这个卷使用名为dubbo-monitor-cm的configMap 在containers中挂载卷,卷名与申明的卷相同 用mountPath的方式挂载到指定目录 ","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/:1:3","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"1.4 创建资源并检查 1.4.1 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/dubbo-monitor/cm.yaml kubectl apply -f http://k8s-yaml.zq.com/dubbo-monitor/dp-cm.yaml 1.4.2 dashboard检查创建结果 在dashboard中查看infra名称空间中的configmap资源 然后检查容器中的配置 kubectl -n infra exec -it dubbo-monitor-5b7cdddbc5-xpft6 bash # 容器内 bash-4.3# cat /dubbo-monitor-simple/conf/dubbo.properties dubbo.container=log4j,spring,registry,jetty dubbo.application.name=simple-monitor dubbo.application.owner=zqkj dubbo.registry.address=zookeeper://zk1.zq.com:2181 .... 1.4.3 检查dubbo-monitor页面的注册信息 ","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/:1:4","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"2 更新configmap资源 ","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/:2:0","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"2.1 多配置更新法 2.1.1 准备新configmap 再准备一个configmap叫cm-pro.yaml cp cm.yaml cm-pro.yaml # 把资源名字改成dubbo-monitor-cm-pro sed -i 's#dubbo-monitor-cm#dubbo-monitor-cm-pro#g' cm-pro.yaml # 把服务注册到zk2.zq.com上 sed -i 's#zk1#zk2#g' cm-pro.yaml 2.1.2 修改deploy配置 sed -i 's#dubbo-monitor-cm#dubbo-monitor-cm-pro#g' dp-cm.yaml 2.1.3 更新资源 # 应用新configmap kubectl apply -f http://k8s-yaml.zq.com/dubbo-monitor/cm-pro.yaml # 更新deploy kubectl apply -f http://k8s-yaml.zq.com/dubbo-monitor/dp-cm.yaml 2.1.4 检查配置是否更新 新的pod已经起来了 ~]# kubectl -n infra get pod NAME READY STATUS RESTARTS AGE dubbo-monitor-c7fbf68b9-7nffj 1/1 Running 0 52s 进去看看是不是应用的新的configmap配置： kubectl -n infra exec -it dubbo-monitor-5cb756cc6c-xtnrt bash # 容器内 bash-4.3# cat /dubbo-monitor-simple/conf/dubbo.properties |grep zook dubbo.registry.address=zookeeper://zk2.zq.com:2181 看下dubbo-monitor的页面：已经是zk2了。 ","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/:2:1","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"3 挂载方式探讨 ","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/:3:0","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"3.1 monutPath挂载的问题 我们使用的是mountPath，这个是挂载整个目录，会使容器内的被挂载目录中原有的文件不可见，可以看见我们。 查看我们pod容器启动的命令可以看见原来脚本中的命令已经无法对挂载的目录操作了 如何单独挂载一个配置文件: 只挂载单独一个文件而不是整个目录，需要添加subPath方法 ","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/:3:1","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"3.2 单独挂载文件演示 3.2.1 更新配置 在dp-cm.yaml的配置中,将原来的volume配置做一下更改 #----------------start--------------------------- volumeMounts: - name: configmap-volume mountPath: /dubbo-monitor-simple/conf volumes: - name: configmap-volume configMap: name: dubbo-monitor-cm #----------------end----------------------------- # 调整为 #----------------start--------------------------- volumeMounts: - name: configmap-volume mountPath: /dubbo-monitor-simple/conf - name: configmap-volume mountPath: /var/dubbo.properties subPath: dubbo.properties volumes: - name: configmap-volume configMap: name: dubbo-monitor-cm #----------------end----------------------------- 3.2.2 应用apply配置并验证 kubectl apply -f http://k8s-yaml.zq.com/dubbo-monitor/dp-cm.yaml kubectl -n infra exec -it dubbo-monitor-5cb756cc6c-xtnrt bash # 容器内操作 bash-4.3# ls -l /var/ total 4 drwxr-xr-x 1 root root 29 Apr 13 2016 cache -rw-r--r-- 1 root root 459 May 10 10:02 dubbo.properties drwxr-xr-x 2 root root 6 Apr 1 2016 empty ..... ","date":"2020-10-01","objectID":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/:3:2","tags":["K8S","转载"],"title":"11_K8S_配置中心实战-configmap资源","uri":"/11_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-configmap%E8%B5%84%E6%BA%90/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"12_K8S_配置中心实战-交付apollo三组件 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:0:0","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"1 apollo简单说明 官方地址 概念请参考 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:1:0","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"1.1 apollo最简架构图： ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:1:1","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"1.2 apollo组件部署关系 configservice自带eureka注册中心、配置写入configDB数据库、优先部署、为client提供服务 adminservice向eureka注册服务、与configservice共用数据库、为portal提供服务 configservice和adminservice组成一套环境、多个环境就得部署多套config和admin portal是web端、各环境共用、只需部署一套、有自己单独的数据库 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:1:2","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"2 为appllo准备数据库 apollo需要使用数据库，如果是mysql，需要版本在5.6以上： 本次环境mysql部署在10.4.7.11上，使用mysql5.7，为测试简单起见，各环境数据库使用同一个，不做隔离 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:2:0","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"2.1 下载安装mysql 2.1.1 yum安装mysql rpm -Uvh https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm yum -y install yum-utils yum-config-manager --disable mysql80-community yum-config-manager --enable mysql57-community yum install mysql-server -y 2.1.2 创建简单配置文件 cat \u003e/etc/my.cnf \u003c\u003c'EOF' [mysqld] character_set_server = utf8mb4 collation_server = utf8mb4_general_ci init_connect = \"SET NAMES 'utf8mb4'\" [mysql] default-character-set = utf8mb4 EOF 2.1.2 启动mysql并初始设置 systemctl start mysqld systemctl enable mysqld mysql -u root -p`grep password /var/log/messages|awk '{print $NF}'` # 修改密码 \u003e set global validate_password_policy=0; \u003e set global validate_password_length=1; \u003e set password=password('123456'); \u003e flush privileges; # 检查字符集：需要四个都是utf8mb4 \u003e \\s ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:2:1","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"3 初始化appllo数据库 configdb初始化脚本 portal初始化脚本 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:3:0","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"3.1 configdb数据库 3.1.1下载脚本并执行： wget -O apolloconfig.sql https://raw.githubusercontent.com/ctripcorp/apollo/1.5.1/scripts/db/migration/configdb/V1.0.0__initialization.sql # 导入sql文件 mysql -uroot -p123456 \u003c apolloconfig.sql # 检查是否导入成功 mysql -uroot -p123456 -e \"show databases;\"|grep ApolloConfigDB 3.1.2 授权并修改初始数据： mysql -uroot -p123456 \u003e grant INSERT,DELETE,UPDATE,SELECT on ApolloConfigDB.* to 'apollo'@'10.4.7.%' identified by \"123456\"; # 修改数据 \u003e use ApolloConfigDB \u003e update ServerConfig set Value='http://apollo-config.zq.com/eureka' where Id=1; 3.1.3 添加config域名解析： vi /var/named/zq.com.zone mysql A 10.4.7.11 apollo-config A 10.4.7.10 apollo-admin A 10.4.7.10 apollo-portal A 10.4.7.10 # 重启并验证 systemctl restart named dig -t A apollo-config.zq.com @10.4.7.11 +short ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:3:1","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"3.2 portal数据库 由于portal使用的是另一个portaldb，我们需要在数据库中新建portdb，并初始化 3.2.1 下载并执行 wget -O apollo-portal.sql https://raw.githubusercontent.com/ctripcorp/apollo/1.5.1/scripts/db/migration/portaldb/V1.0.0__initialization.sql # 导入sql文件 mysql -uroot -p123456 \u003c apollo-portal.sql # 检查是否导入成功 mysql -uroot -p123456 -e \"show databases;\"|grep ApolloPortalDB 3.2.2 授权用户并更新初始数据 都使用apollo用户来管理数据库是为了方便,如果有相关的安全考虑可以给config和portal分别使用不同的数据库账号 mysql -uroot -p123456 \u003e grant INSERT,DELETE,UPDATE,SELECT on ApolloPortalDB.* to \"apollo\"@\"10.4.7.%\" identified by \"123456\"; # 更新部门名 \u003e update ApolloPortalDB.ServerConfig set Value='[{\"orgId\":\"zq01\",\"orgName\":\"研发部\"},{\"orgId\":\"zq02\",\"orgName\":\"运维部\"}]' where Id=2; ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:3:2","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"4 部署configservice ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:4:0","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"4.1 制作docker镜像 操作在7.200上完成 4.1.1 下载程序包 wget https://github.com/ctripcorp/apollo/releases/download/v1.5.1/apollo-configservice-1.5.1-github.zip mkdir /data/dockerfile/apollo-configservice unzip -o apollo-configservice-1.5.1-github.zip -d /data/dockerfile/apollo-configservice/ 4.1.2 修改连接数据库配置： cd /data/dockerfile/apollo-configservice/config # 修改数据库连接地址 sed -i 's#fill-in-the-correct-server#mysql.zq.com#g' application-github.properties # 修改数据库连接用户和密码 sed -i 's#FillInCorrectUser#apollo#g' application-github.properties sed -i 's#FillInCorrectPassword#123456#g' application-github.properties # 查看结果 config]# egrep -v \"^#|$^\" application-github.properties spring.datasource.url = jdbc:mysql://mysql.zq.com:3306/ApolloConfigDB?characterEncoding=utf8 spring.datasource.username = apollo spring.datasource.password = 123456 4.1.3 创建启动脚本： 程序中自带的start.sh启动脚本时不适用与K8S运行,因此需要专门下载他们提供的K8S内使用的脚本 # 1.从官网下载启动脚本 cd /data/dockerfile/apollo-configservice/scripts/ wget https://raw.githubusercontent.com/ctripcorp/apollo/1.5.1/scripts/apollo-on-kubernetes/apollo-config-server/scripts/startup-kubernetes.sh # 2. 添加一行使用主机名的变量 sed -i '5i APOLLO_CONFIG_SERVICE_NAME=$(hostname -i)' startup-kubernetes.sh # 3.根据需要修改下jvm限制 4.1.4 编写dockerfile dockerfile官方地址 cd .. cat \u003eDockerfile \u003c\u003c'EOF' FROM harbor.zq.com/base/jre8:8u112 ENV VERSION 1.5.1 RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026\u0026\\ echo \"Asia/Shanghai\" \u003e /etc/timezone ADD apollo-configservice-${VERSION}.jar /apollo-configservice/apollo-configservice.jar ADD config/ /apollo-configservice/config ADD scripts/ /apollo-configservice/scripts CMD [\"sh\",\"/apollo-configservice/scripts/startup-kubernetes.sh\"] EOF 4.1.5 构建docker镜像 docker build . -t harbor.zq.com/infra/apollo-configservice:v1.5.1 docker push harbor.zq.com/infra/apollo-configservice:v1.5.1 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:4:1","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"4.2 编写资源配置清单： mkdir /data/k8s-yaml/apollo-configservice cd /data/k8s-yaml/apollo-configservice 4.2.1 创建config的configmap资源清单 给configservice创建cm资源的清单的目的是方便修改 其实里面的内容就是前面修改的application-github.properties文件 如果确定不会修改,可以不创建此cm,直接写死配置到docker镜像中 cat \u003ecm.yaml \u003c\u003c'EOF' apiVersion: v1 kind: ConfigMap metadata: name: apollo-configservice-cm namespace: infra data: application-github.properties: | # DataSource spring.datasource.url = jdbc:mysql://mysql.zq.com:3306/ApolloConfigDB?characterEncoding=utf8 spring.datasource.username = apollo spring.datasource.password = 123456 eureka.service.url = http://apollo-config.zq.com/eureka app.properties: | appId=100003171 EOF 在同一个configmap资源中,可以添加多个配置文件,上述配置就有两个,分别是: application-github.properties和app.properties 4.2.2 创建Deployment资源清单 cat \u003edp.yaml \u003c\u003c'EOF' kind: Deployment apiVersion: extensions/v1beta1 metadata: name: apollo-configservice namespace: infra labels: name: apollo-configservice spec: replicas: 1 selector: matchLabels: name: apollo-configservice template: metadata: labels: app: apollo-configservice name: apollo-configservice spec: volumes: - name: configmap-volume configMap: name: apollo-configservice-cm containers: - name: apollo-configservice image: harbor.zq.com/infra/apollo-configservice:v1.5.1 ports: - containerPort: 8080 protocol: TCP volumeMounts: - name: configmap-volume mountPath: /apollo-configservice/config terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: IfNotPresent imagePullSecrets: - name: harbor restartPolicy: Always terminationGracePeriodSeconds: 30 securityContext: runAsUser: 0 schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 revisionHistoryLimit: 7 progressDeadlineSeconds: 600 EOF 4.2.3 创建service资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' kind: Service apiVersion: v1 metadata: name: apollo-configservice namespace: infra spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: apollo-configservice EOF 4.2.4 创建ingress资源清单 cat \u003eingress.yaml \u003c\u003c'EOF' kind: Ingress apiVersion: extensions/v1beta1 metadata: name: apollo-configservice namespace: infra spec: rules: - host: apollo-config.zq.com http: paths: - path: / backend: serviceName: apollo-configservice servicePort: 8080 EOF service中不一定必须暴露8080,分配的clusterIP中所有的端口都可以 但ingress中的servicePort一定要与service中暴露的端口匹配 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:4:2","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"4.3 应用资源配置清单： 4.3.1 任意node执行 kubectl create -f http://k8s-yaml.zq.com/apollo-configservice/cm.yaml kubectl create -f http://k8s-yaml.zq.com/apollo-configservice/dp.yaml kubectl create -f http://k8s-yaml.zq.com/apollo-configservice/svc.yaml kubectl create -f http://k8s-yaml.zq.com/apollo-configservice/ingress.yaml 4.3.2 检查启动情况： kubectl -n infra get pod|grep apollo-config # 检查命令 kubectl -n infra logs apollo-configservice-64fc749978-9nz5h --tail=4 需要等到eureka启动以后才可以，接下来使用浏览器访问apollo-config.zq.com ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:4:3","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"5 部署adminservice 官方地址 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:5:0","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"5.1 制作docker镜像 操作在7.200上完成 5.1.1 下载程序包 wget https://github.com/ctripcorp/apollo/releases/download/v1.5.1/apollo-adminservice-1.5.1-github.zip mkdir /data/dockerfile/apollo-adminservice unzip -o apollo-adminservice-1.5.1-github.zip -d /data/dockerfile/apollo-adminservice/ 5.1.2 修改连接数据库配置： 由于使用了configmap资源将配置文件挂载出来了，所以不在修改配置文件，如需修改配置文件，请参考部署apollo-configservice时候的修改方法： 5.1.3 创建启动脚本： 程序中自带的start.sh启动脚本时不适用与K8S运行,因此需要专门下载他们提供的K8S内使用的脚本 # 1.从官网下载启动脚本 cd /data/dockerfile/apollo-adminservice/scripts/ wget https://raw.githubusercontent.com/ctripcorp/apollo/1.5.1/scripts/apollo-on-kubernetes/apollo-admin-server/scripts/startup-kubernetes.sh # 2. 添加一行使用主机名的变量 sed -i '5i APOLLO_CONFIG_SERVICE_NAME=$(hostname -i)' startup-kubernetes.sh # 3.修改端口为8080 sed -i 's#8090#8080#g' startup-kubernetes.sh 官方配置文件端口改为8090的目的是虚拟机部署的时候端口不冲突 但我们用K8S部署,会给他单独的clusterIP,所以不用担心端口重复 5.1.4 编写dockerfile cd .. cat \u003eDockerfile \u003c\u003c'EOF' FROM stanleyws/jre8:8u112 ENV VERSION 1.5.1 RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026\u0026\\ echo \"Asia/Shanghai\" \u003e /etc/timezone ADD apollo-adminservice-${VERSION}.jar /apollo-adminservice/apollo-adminservice.jar ADD config/ /apollo-adminservice/config ADD scripts/ /apollo-adminservice/scripts CMD [\"/bin/bash\",\"/apollo-adminservice/scripts/startup-kubernetes.sh\"] EOF 由于要使用cm配置资源,因此就不改config中的配置了 5.1.5 构建docker镜像 docker build . -t harbor.zq.com/infra/apollo-adminservice:v1.5.1 docker push harbor.zq.com/infra/apollo-adminservice:v1.5.1 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:5:1","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"5.2 制作资源配置清单： adminservice向注册中心注册服务,不直接对外提供服务,因此不需要暴露端口,只需要cm资源和dp资源 mkdir /data/k8s-yaml/apollo-adminservice cd /data/k8s-yaml/apollo-adminservice 5.2.1 创建configmap资源清单 cat \u003ecm.yaml \u003c\u003c'EOF' apiVersion: v1 kind: ConfigMap metadata: name: apollo-adminservice-cm namespace: infra data: application-github.properties: | # DataSource spring.datasource.url = jdbc:mysql://mysql.zq.com:3306/ApolloConfigDB?characterEncoding=utf8 spring.datasource.username = apollo spring.datasource.password = 123456 eureka.service.url = http://apollo-config.zq.com/eureka app.properties: | appId=100003172 EOF 注意每个服务的appId都不会一样哦 5.2.2 创建Deployment资源清单 cat \u003edp.yaml \u003c\u003c'EOF' kind: Deployment apiVersion: extensions/v1beta1 metadata: name: apollo-adminservice namespace: infra labels: name: apollo-adminservice spec: replicas: 1 selector: matchLabels: name: apollo-adminservice template: metadata: labels: app: apollo-adminservice name: apollo-adminservice spec: volumes: - name: configmap-volume configMap: name: apollo-adminservice-cm containers: - name: apollo-adminservice image: harbor.zq.com/infra/apollo-adminservice:v1.5.1 ports: - containerPort: 8080 protocol: TCP volumeMounts: - name: configmap-volume mountPath: /apollo-adminservice/config terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: IfNotPresent imagePullSecrets: - name: harbor restartPolicy: Always terminationGracePeriodSeconds: 30 securityContext: runAsUser: 0 schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 revisionHistoryLimit: 7 progressDeadlineSeconds: 600 EOF ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:5:2","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"5.3 应用资源配置清单 5.3.1 任意node执行 kubectl create -f http://k8s-yaml.zq.com/apollo-adminservice/cm.yaml kubectl create -f http://k8s-yaml.zq.com/apollo-adminservice/dp.yaml 5.3.2 检查启动情况 ~]# kubectl -n infra get pod|grep apollo-admin apollo-adminservice-6cd4fcfdc8-2drnq 1/1 Running 0 9s # 检查命令 kubectl -n infra logs apollo-configservice-6cd4fcfdc8-2drnq --tail=4 通过 apollo-config.zq.com 检查是否注册到了eureka： 已经顺利的注册到了注册中心中。 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:5:3","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"6 部署portal ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:6:0","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"6.1 制作docker镜像 portal官方地址 6.1.1 下载程序包 wget https://github.com/ctripcorp/apollo/releases/download/v1.5.1/apollo-portal-1.5.1-github.zip mkdir /data/dockerfile/apollo-portal unzip -o apollo-portal-1.5.1-github.zip -d /data/dockerfile/apollo-portal/ 6.1.2 修改配置文件 由于使用concigmap资源，故不在这里修改 注意如果要修改的话,要分别修改两个文件 apollo-env.properties修改数据库配置 apollo-env.properties修改支持的环境列表 6.1.3 创建启动脚本 脚本官方地址 # 1.从官网下载启动脚本 cd /data/dockerfile/apollo-portal/scripts/ wget https://raw.githubusercontent.com/ctripcorp/apollo/1.5.1/scripts/apollo-on-kubernetes/apollo-portal-server/scripts/startup-kubernetes.sh # 2. 添加一行使用主机名的变量 sed -i '5i APOLLO_CONFIG_SERVICE_NAME=$(hostname -i)' startup-kubernetes.sh # 3.修改端口为8080 sed -i 's#8070#8080#g' startup-kubernetes.sh 6.1.4 制作dockerfile： cd /data/dockerfile/apollo-portal/ cat \u003eDockerfile \u003c\u003c'EOF' FROM stanleyws/jre8:8u112 ENV VERSION 1.5.1 RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026\u0026\\ echo \"Asia/Shanghai\" \u003e /etc/timezone ADD apollo-portal-${VERSION}.jar /apollo-portal/apollo-portal.jar ADD config/ /apollo-portal/config ADD scripts/ /apollo-portal/scripts CMD [\"/bin/bash\",\"/apollo-portal/scripts/startup-kubernetes.sh\"] EOF 6.1.5 构建docker镜像 docker build . -t harbor.zq.com/infra/apollo-portal:v1.5.1 docker push harbor.zq.com/infra/apollo-portal:v1.5.1 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:6:1","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"6.2 编写资源配置清单： mkdir /data/k8s-yaml/apollo-portal cd /data/k8s-yaml/apollo-portal 6.2.1 创建configmap资源清单 cat \u003ecm.yaml \u003c\u003c'EOF' apiVersion: v1 kind: ConfigMap metadata: name: apollo-portal-cm namespace: infra data: application-github.properties: | # DataSource spring.datasource.url = jdbc:mysql://mysql.zq.com:3306/ApolloPortalDB?characterEncoding=utf8 spring.datasource.username = apollo spring.datasource.password = 123456 app.properties: | appId=100003173 apollo-env.properties: | dev.meta=http://apollo-config.zq.com EOF 这里暂时只管理一个环境,等跑通了以后,再演示多环境问题 6.2.2 创建Deployment资源清单 cat \u003edp.yaml \u003c\u003c'EOF' kind: Deployment apiVersion: extensions/v1beta1 metadata: name: apollo-portal namespace: infra labels: name: apollo-portal spec: replicas: 1 selector: matchLabels: name: apollo-portal template: metadata: labels: app: apollo-portal name: apollo-portal spec: volumes: - name: configmap-volume configMap: name: apollo-portal-cm containers: - name: apollo-portal image: harbor.zq.com/infra/apollo-portal:v1.5.1 ports: - containerPort: 8080 protocol: TCP volumeMounts: - name: configmap-volume mountPath: /apollo-portal/config terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: IfNotPresent imagePullSecrets: - name: harbor restartPolicy: Always terminationGracePeriodSeconds: 30 securityContext: runAsUser: 0 schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 revisionHistoryLimit: 7 progressDeadlineSeconds: 600 EOF 6.2.3 创建service资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' kind: Service apiVersion: v1 metadata: name: apollo-portal namespace: infra spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: apollo-portal EOF 6.2.4 创建ingress资源清单 cat \u003eingress.yaml \u003c\u003c'EOF' kind: Ingress apiVersion: extensions/v1beta1 metadata: name: apollo-portal namespace: infra spec: rules: - host: apollo-portal.zq.com http: paths: - path: / backend: serviceName: apollo-portal servicePort: 8080 EOF ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:6:2","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"6.3 应用资源配置清单 6.3.1 在任意node执行 kubectl create -f http://k8s-yaml.zq.com/apollo-portal/cm.yaml kubectl create -f http://k8s-yaml.zq.com/apollo-portal/dp.yaml kubectl create -f http://k8s-yaml.zq.com/apollo-portal/svc.yaml kubectl create -f http://k8s-yaml.zq.com/apollo-portal/ingress.yaml 6.3.2 检查启动情况 6.3.3 网页验证 由于前面已经一起添加了域名解析,因此portal创建好后不需要在添加域名解析,直接浏览器登录验证 网页：apollo-portal.zq.com 默认用户名：apollo 默认密码：admin 登录成功后,立马修改密码为apollo123 到此，apollo的三个组件都已经交付到k8s里了。 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:6:3","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"7 配置服务使用apollo配置中心 使用配置中心，需要开发对代码进行调整，将一些配置，通过变量的形式配置到apollo中，服务通过配置中心来获取具体的配置 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:7:0","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"7.1 新建dubbo-service项目配置 7.1.1 在配置中心修改新增项目: 项目属性： AppId：dubbo-demo-service 应用名称：dubbo服务提供者 部门：研发部 为新项目添加配置如下： key value 备注 dubbo.registry zookeeper://zk1.zq.com:2181 注册中心地址 dubbo.port 20880 dubbo服务监听端口 发布后效果图如下： 7.1.2 重新打包service镜像 还是使用之前的流水线，但是使用分支为apollo的代码进行打包，参数如下： 参数名 参数值 app_name dubbo-demo-service image_name app/dubbo-demo-service git_repo https://gitee.com/noah-luo/dubbo-demo-service.git git_ver apollo add_tag 200512_0746 mvn_dir ./ target_dir ./dubbo-server/target mvn_cmd mvn clean package -Dmaven.test.skip=true base_image base/jre8:8u112 maven 3.6.1 7.1.3 重新应用资源配置清单 修改dp.yaml资源配置清单 将镜像改为刚刚打包的镜像名： 添加环境变量C_OPTS,以便指定配置中心 vim /data/k8s-yaml/dubbo-server/dp.yaml #----------原内容---------- spec: containers: - name: dubbo-demo-service image: harbor.zq.com/app/dubbo-demo-service:master_200509_0800 ports: - containerPort: 20880 protocol: TCP env: - name: JAR_BALL value: dubbo-server.jar #----------新内容---------- spec: containers: - name: dubbo-demo-service image: harbor.zq.com/app/dubbo-demo-service:apollo_200512_0746 ports: - containerPort: 20880 protocol: TCP env: - name: JAR_BALL value: dubbo-server.jar - name: C_OPTS value: -Denv=dev -Dapollo.meta=http://apollo-config.zq.com 应用资源配置清单： kubectl apply -f http://k8s-yaml.zq.com/dubbo-server/dp.yaml ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:7:1","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"7.2 新建dubbo-web项目配置 7.2.1 在配置中心修改新增项目: 项目属性： AppId：dubbo-demo-web 应用名称：dubbo服务消费者 部门：运维部 为新项目添加配置如下： key value 备注 dubbo.registry zookeeper://zk1.zq.com:2181 注册中心地址 发布后效果图如下： 略 7.1.2 重新打包service镜像 还是使用之前的流水线，但是使用分支为apollo的代码进行打包，参数如下： 参数名 参数值 app_name dubbo-demo-consumer image_name app/dubbo-demo-consumer git_repo git@gitee.com:noah-luo/dubbo-demo-web.git git_ver apollo add_tag 200512_0801 mvn_dir ./ target_dir ./dubbo-client/target mvn_cmd mvn clean package -Dmaven.test.skip=true base_image base/jre8:8u112 maven 3.6.1 构建完成后，修改资源配置清单并应用： 7.1.3 重新应用资源配置清单 修改dp.yaml资源配置清单 将镜像改为刚刚打包的镜像名： 添加环境变量C_OPTS,以便指定配置中心 vim /data/k8s-yaml/dubbo-consumer/dp.yaml #----------原内容---------- spec: containers: - name: dubbo-demo-consumer image: harbor.zq.com/app/dubbo-demo-consumer:master_200506_1430 ports: - containerPort: 8080 protocol: TCP - containerPort: 20880 protocol: TCP env: - name: JAR_BALL value: dubbo-client.jar #----------新内容---------- spec: containers: - name: dubbo-demo-consumer image: harbor.zq.com/app/dubbo-demo-consumer:apollo_200512_0801 ports: - containerPort: 8080 protocol: TCP - containerPort: 20880 protocol: TCP env: - name: JAR_BALL value: dubbo-client.jar - name: C_OPTS value: -Denv=dev -Dapollo.meta=http://apollo-config.zq.com 应用资源配置清单： kubectl apply -f http://k8s-yaml.zq.com/dubbo-consumer/dp.yaml ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:7:2","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"7.3 验证结果 7.3.1 修改dubbo-monitor资源 管理机上,修改dubbo-monitor的dp资源的使用的cm资源 set -i 's#dubbo-monitor-cm-pro#dubbo-monitor-cm#g' /data/k8s-yaml/dubbo-monitor/dp-cm.yaml 任意node节点应用资源 kubectl apply -f http://k8s-yaml.zq.com/dubbo-monitor/dp.yaml 登录dubbo-monitor查看 访问http://dubbo-monitor.zq.com/ 浏览器查看 访问http://dubbo-demo.zq.com/hello?name=lg apollo中看实例列表 ","date":"2020-10-01","objectID":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:7:3","tags":["K8S","转载"],"title":"12_K8S_配置中心实战-交付apollo三组件","uri":"/12_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"13_K8S_配置中心实战-多环境交付apollo三组件 ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:0:0","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"1.环境准备工作 先删除infra名称空间中部署的apollo服务 kubectl delete -f http://k8s-yaml.zq.com/apollo-configservice/dp.yaml kubectl delete -f http://k8s-yaml.zq.com/apollo-adminservice/dp.yaml kubectl delete -f http://k8s-yaml.zq.com/apollo-portal/dp.yaml 要进行分环境，需要将现有实验环境进行拆分 portal服务，可以各个环境共用，只部署一套 adminservice和configservice必须要分开每个环境一套 zk和namespace也要区分环境 ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:1:0","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"1.1 zk环境拆分 zk拆分最简单,只需要在dns那里修改解析规则即可： 同时添加好apollo、dubbo两个环境的域名解析 vi /var/named/zq.com.zone zk-test A 10.4.7.11 zk-prod A 10.4.7.12 apollo-testconfig A 10.4.7.10 apollo-prodconfig A 10.4.7.10 dubbo-testdemo A 10.4.7.10 dubbo-proddemo A 10.4.7.10 # 重启服务 systemctl restart named.service ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:1:1","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"1.2 namespace分环境 分别创建test 和prod两个名称空间 kubectl create ns test kubectl create ns prod 给新名称空间创建secret授权 kubectl create secret docker-registry harbor \\ --docker-server=harbor.zq.com \\ --docker-username=harbor \\ --docker-password=Harbor12345 \\ -n test kubectl create secret docker-registry harbor \\ --docker-server=harbor.zq.com \\ --docker-username=harbor \\ --docker-password=Harbor12345 \\ -n prod ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:1:2","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"1.3 数据库拆分 因实验资源有限，故使用分库的形式模拟分环境 1.3.1 修改初始化脚本并导入 数据库再7.11上哦 分别创建ApolloConfigTestDB和ApolloConfigProdDB # 复制为新sql cp apolloconfig.sql apolloconfigTest.sql cp apolloconfig.sql apolloconfigProd.sql # 替换关键字 sed -i 's#ApolloConfigDB#ApolloConfigTestDB#g' apolloconfigTest.sql sed -i 's#ApolloConfigDB#ApolloConfigProdDB#g' apolloconfigProd.sql # 导入数据库 mysql -uroot -p123456 \u003c apolloconfigTest.sql mysql -uroot -p123456 \u003c apolloconfigProd.sql 1.3.2 修改数据库中eureka地址 这里用到了两个新的域名，域名解析已经在添加zk域名那里一起加了 mysql -uroot -p123456 # 1.修改eureka注册中心配置 \u003e update ApolloConfigProdDB.ServerConfig set ServerConfig.Value=\"http://apollo-prodconfig.zq.com/eureka\" where ServerConfig.Key=\"eureka.service.url\"; \u003e update ApolloConfigTestDB.ServerConfig set ServerConfig.Value=\"http://apollo-testconfig.zq.com/eureka\" where ServerConfig.Key=\"eureka.service.url\"; # 2.在portl库中增加支持fat环境和pro环境 \u003e update ApolloPortalDB.ServerConfig set Value='fat,pro' where Id=1; # 3.授权数据库访问用户 \u003e grant INSERT,DELETE,UPDATE,SELECT on ApolloConfigProdDB.* to \"apollo\"@\"10.4.7.%\"; \u003e grant INSERT,DELETE,UPDATE,SELECT on ApolloConfigTestDB.* to \"apollo\"@\"10.4.7.%\"; ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:1:3","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"1.4 变动原有资源配置启动 1.4.1 修改portal的cm资源配置清单 7.200运维机操作,增加两个新环境的支持 cd /data/k8s-yaml/apollo-portal/ sed -i '$a\\ fat.meta=http://apollo-testconfig.zq.com' cm.yaml sed -i '$a\\ pro.meta=http://apollo-prodconfig.zq.com' cm.yaml # 查看结果 apollo-portal]# tail -4 cm.yaml apollo-env.properties: | dev.meta=http://apollo-config.zq.com fat.meta=http://apollo-testconfig.zq.com pro.meta=http://apollo-prodconfig.zq.com 1.4.2 任意节点应用修改 kubectl apply -f http://k8s-yaml.zq.com/apollo-portal/cm.yaml ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:1:4","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"2 部署新环境的APOLLO服务 7.200运维机操作, ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:2:0","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"2.1 先创建出所需目录和文件 cd /data/k8s-yaml mkdir -p test/{apollo-adminservice,apollo-configservice,dubbo-demo-server,dubbo-demo-consumer} mkdir -p prod/{apollo-adminservice,apollo-configservice,dubbo-demo-server,dubbo-demo-consumer} # 查看结果 k8s-yaml]# ll {test,prod} prod: total 0 drwxr-xr-x 2 root root 6 May 13 22:50 apollo-adminservice drwxr-xr-x 2 root root 6 May 13 22:50 apollo-configservice drwxr-xr-x 2 root root 6 May 13 22:50 dubbo-demo-consumer drwxr-xr-x 2 root root 6 May 13 22:50 dubbo-demo-server test: total 0 drwxr-xr-x 2 root root 6 May 13 22:50 apollo-adminservice drwxr-xr-x 2 root root 6 May 13 22:50 apollo-configservice drwxr-xr-x 2 root root 6 May 13 22:50 dubbo-demo-consumer drwxr-xr-x 2 root root 6 May 13 22:50 dubbo-demo-server 将之前的资源配置清单cp到对应环境的目录中，便于修改： cp /data/k8s-yaml/apollo-configservice/* ./test/apollo-configservice/ cp /data/k8s-yaml/apollo-configservice/* ./prod/apollo-configservice/ cp /data/k8s-yaml/apollo-adminservice/* ./test/apollo-adminservice/ cp /data/k8s-yaml/apollo-adminservice/* ./prod/apollo-adminservice/ ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:2:1","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"2.2 部署test环境的configservice 2.2.1 修改configmap资源清单 要修改ns，数据库库名，eureka地址 cd /data/k8s-yaml/test/apollo-configservice/ sed -ri 's#(namespace:) infra#\\1 test#g' cm.yaml sed -i 's#ApolloConfigDB#ApolloConfigTestDB#g' cm.yaml sed -i 's#apollo-config.zq.com#apollo-testconfig.zq.com#g' cm.yaml 2.2.2 修改dp,ns,inress资源清单 # 1.dp只需要修改namesapce空间 sed -ri 's#(namespace:) infra#\\1 test#g' dp.yaml # 2.svc同样只需要修改namespace sed -ri 's#(namespace:) infra#\\1 test#g' svc.yaml # 3.ingress需要修改namespace和域名 sed -ri 's#(namespace:) infra#\\1 test#g' ingress.yaml sed -i 's#apollo-config.zq.com#apollo-testconfig.zq.com#g' ingress.yaml 2.2.3 应用资源配置 任意节点应用配置 kubectl apply -f http://k8s-yaml.zq.com/test/apollo-configservice/cm.yaml kubectl apply -f http://k8s-yaml.zq.com/test/apollo-configservice/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/test/apollo-configservice/svc.yaml kubectl apply -f http://k8s-yaml.zq.com/test/apollo-configservice/ingress.yaml 浏览器检查 浏览器输入http://apollo-testconfig.zq.com,查看测试环境的apollo注册中心是否已有服务注册服务已经注册进来了 ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:2:2","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"2.3 部署test环境的adminservice 2.3.1 修改configmap资源清单 要修改ns，数据库库名，eureka地址 cd /data/k8s-yaml/test/apollo-adminservice/ sed -ri 's#(namespace:) infra#\\1 test#g' cm.yaml sed -i 's#ApolloConfigDB#ApolloConfigTestDB#g' cm.yaml sed -i 's#apollo-config.zq.com#apollo-testconfig.zq.com#g' cm.yaml 2.3.2 修改dp资源清单 # 1.dp只需要修改namesapce空间 sed -ri 's#(namespace:) infra#\\1 test#g' dp.yaml 2.3.3 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/test/apollo-adminservice/cm.yaml kubectl apply -f http://k8s-yaml.zq.com/test/apollo-adminservice/dp.yaml 浏览器检查: 浏览器输入http://apollo-testconfig.zq.com,查看测试环境的apollo注册中心是否已有adminservice服务注册 ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:2:3","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"2.4 部署prod环境的configservice 套路基本上都是一样使用的 2.4.1 修改configmap资源清单 要修改ns，数据库库名，eureka地址 cd /data/k8s-yaml/prod/apollo-configservice/ sed -ri 's#(namespace:) infra#\\1 prod#g' cm.yaml sed -i 's#ApolloConfigDB#ApolloConfigProdDB#g' cm.yaml sed -i 's#apollo-config.zq.com#apollo-prodconfig.zq.com#g' cm.yaml 2.4.2 修改dp,ns,inress资源清单 # 1.dp只需要修改namesapce空间 sed -ri 's#(namespace:) infra#\\1 prod#g' dp.yaml # 2.svc同样只需要修改namespace sed -ri 's#(namespace:) infra#\\1 prod#g' svc.yaml # 3.ingress需要修改namespace和域名 sed -ri 's#(namespace:) infra#\\1 prod#g' ingress.yaml sed -i 's#apollo-config.zq.com#apollo-prodconfig.zq.com#g' ingress.yaml 2.4.3 应用资源配置 任意节点应用配置 kubectl apply -f http://k8s-yaml.zq.com/prod/apollo-configservice/cm.yaml kubectl apply -f http://k8s-yaml.zq.com/prod/apollo-configservice/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/prod/apollo-configservice/svc.yaml kubectl apply -f http://k8s-yaml.zq.com/prod/apollo-configservice/ingress.yaml 浏览器检查 浏览器输入http://apollo-prodconfig.zq.com,查看测试环境的apollo注册中心是否已有服务注册服务已经注册进来了 ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:2:4","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"2.5 部署prod环境的adminservice 2.5.1 修改configmap资源清单 要修改ns，数据库库名，eureka地址 cd /data/k8s-yaml/prod/apollo-adminservice/ sed -ri 's#(namespace:) infra#\\1 prod#g' cm.yaml sed -i 's#ApolloConfigDB#ApolloConfigProdDB#g' cm.yaml sed -i 's#apollo-config.zq.com#apollo-prodconfig.zq.com#g' cm.yaml 2.5.2 修改dp资源清单 # 1.dp只需要修改namesapce空间 sed -ri 's#(namespace:) infra#\\1 prod#g' dp.yaml 2.5.3 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/prod/apollo-adminservice/cm.yaml kubectl apply -f http://k8s-yaml.zq.com/prod/apollo-adminservice/dp.yaml 浏览器检查: 浏览器输入http://apollo-prodconfig.zq.com,查看测试环境的apollo注册中心是否已有adminservice服务注册 ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:2:5","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"2.6 启动portal服务 两个服务都已经注册进来了后，删除portal数据库中存储的关于之前项目的配置，再来启动portal项目 2.6.1 清理数据库 mysql -uroot -p123456 \u003e use ApolloPortalDB; \u003e truncate table App; \u003e truncate table AppNamespace; 2.6.2 启动portal kubectl apply -f http://k8s-yaml.zq.com/apollo-portal/cm.yaml kubectl apply -f http://k8s-yaml.zq.com/apollo-portal/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/apollo-portal/svc.yaml kubectl apply -f http://k8s-yaml.zq.com/apollo-portal/ingress.yaml 2.6.3 验证部署结果 打开http://apollo-portal.zq.com,创建两个项目如下: AppId 应用名称 部门 dubbo-demo-service dubbo服务提供者 研发部 dubbo-demo-web dubbo服务消费者 运维部 项目创建成功后,能看到左侧环境列表中有FAT和PRO,表示正确 2.6.4 添加test环境的配置 dubbo-demo-service key value 备注 dubbo.registry zookeeper://zk-test.zq.com:2181 注册中心地址 dubbo.port 20880 dubbo服务监听端口 dubbo-demo-web key value 备注 dubbo.registry zookeeper://zk-test.zq.com:2181 注册中心地址 2.6.5 添加prod环境的配置 dubbo-demo-service key value 备注 dubbo.registry zookeeper://zk-prod.zq.com:2181 注册中心地址 dubbo.port 20880 dubbo服务监听端口 dubbo-demo-web key value 备注 dubbo.registry zookeeper://zk-prod.zq.com:2181 注册中心地址 ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:2:6","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"3 分环境交付dubbo服务 ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:3:0","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"3.1 交付test环境dubbo服务 cd /data/k8s-yaml/ cp ./dubbo-server/* ./test/dubbo-demo-server/ cp ./dubbo-consumer/* ./test/dubbo-demo-consumer/ 3.1.1 修改server资源配置清单 只修改dp的ns配置和apollo配置 cd /data/k8s-yaml/test/dubbo-demo-server sed -ri 's#(namespace:) app#\\1 test#g' dp.yaml sed -i 's#Denv=dev#Denv=fat#g' dp.yaml sed -i 's#apollo-config.zq.com#apollo-testconfig.zq.com#g' dp.yaml 任意node应用资源清单： kubectl apply -f http://k8s-yaml.zq.com/test/dubbo-demo-server/dp.yaml 3.1.2 修改consumer资源配置清单 修改资源清单 cd /data/k8s-yaml/test/dubbo-demo-consumer # 1.修改dp中的ns配置和apollo配置 sed -ri 's#(namespace:) app#\\1 test#g' dp.yaml sed -i 's#Denv=dev#Denv=fat#g' dp.yaml sed -i 's#apollo-config.zq.com#apollo-testconfig.zq.com#g' dp.yaml # 2.修改svc中的ns配置 sed -ri 's#(namespace:) app#\\1 test#g' svc.yaml # 3.修改ingress中的ns配置和域名 sed -ri 's#(namespace:) app#\\1 test#g' ingress.yaml sed -i 's#dubbo-demo.zq.com#dubbo-testdemo.zq.com#g' ingress.yaml 添加域名解析 由于最开始已经统一做了域名解析,这里就不单独添加了 如果没有添加域名解析的话,需要去添加dubbo-testdemo.zq.com的解析 任意node应用资源清单： kubectl apply -f http://k8s-yaml.zq.com/test/dubbo-demo-consumer/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/test/dubbo-demo-consumer/svc.yaml kubectl apply -f http://k8s-yaml.zq.com/test/dubbo-demo-consumer/ingress.yaml ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:3:1","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"3.2 交付prod环境dubbo服务 cd /data/k8s-yaml/ cp ./dubbo-server/* ./prod/dubbo-demo-server/ cp ./dubbo-consumer/* ./prod/dubbo-demo-consumer/ 3.2.1 修改server资源配置清单 只修改dp的ns配置和apollo配置 cd /data/k8s-yaml/prod/dubbo-demo-server sed -ri 's#(namespace:) app#\\1 prod#g' dp.yaml sed -i 's#Denv=dev#Denv=pro#g' dp.yaml sed -i 's#apollo-config.zq.com#apollo-prodconfig.zq.com#g' dp.yaml 任意node应用资源清单： kubectl apply -f http://k8s-yaml.zq.com/prod/dubbo-demo-server/dp.yaml 3.2.2 修改consumer资源配置清单 修改资源清单 cd /data/k8s-yaml/prod/dubbo-demo-consumer # 1.修改dp中的ns配置和apollo配置 sed -ri 's#(namespace:) app#\\1 prod#g' dp.yaml sed -i 's#Denv=dev#Denv=pro#g' dp.yaml sed -i 's#apollo-config.zq.com#apollo-prodconfig.zq.com#g' dp.yaml # 2.修改svc中的ns配置 sed -ri 's#(namespace:) app#\\1 prod#g' svc.yaml # 3.修改ingress中的ns配置和域名 sed -ri 's#(namespace:) app#\\1 prod#g' ingress.yaml sed -i 's#dubbo-demo.zq.com#dubbo-proddemo.zq.com#g' ingress.yaml 添加域名解析 由于最开始已经统一做了域名解析,这里就不单独添加了 如果没有添加域名解析的话,需要去添加dubbo-proddemo.zq.com的解析 任意node应用资源清单： kubectl apply -f http://k8s-yaml.zq.com/prod/dubbo-demo-consumer/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/prod/dubbo-demo-consumer/svc.yaml kubectl apply -f http://k8s-yaml.zq.com/prod/dubbo-demo-consumer/ingress.yaml ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:3:2","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"3.3 关于dp中的apollo域名 dp.yaml中配置的-Dapollo.meta=http://apollo-testconfig.zq.com 其实可以直接使用-Dapollo.meta=http://apollo-configservice:8080 也直接使用svc资源名称调用,这样还可以少走一次外网解析,相当于走内网 因为不同环境的apollo名称空间都不一样,而svc只在当前namespace中生效 ** ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:3:3","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"4 验证并模拟发布 ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:4:0","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"4.1 验证访问两个环境 分别访问以下域名,看是否可以出来网页内容 test：http://dubbo-testdemo.zq.com/hello?name=noah prod：http://dubbo-proddemo.zq.com/hello?name=noah ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:4:1","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"4.2 模拟发版： 任意修改码云上的dubbo-demo-web项目的say方法返回内容 路径dubbo-client/src/main/java/com/od/dubbotest/action/HelloAction.java 4.2.1 用jenkins构建新镜像 参数如下： 参数名 参数值 app_name dubbo-demo-consumer image_name app/dubbo-demo-consumer git_repo git@gitee.com:noah-luo/dubbo-demo-web.git git_ver apollo add_tag 200513_1808 mvn_dir ./ target_dir ./dubbo-client/target mvn_cmd mvn clean package -Dmaven.test.skip=true base_image base/jre8:8u112 maven 3.6.1 4.2.2 发布test环境 构建成功，然后我们在测试环境发布此版本镜像 修改测试环境的dp.yaml cd /data/k8s-yaml/test/dubbo-demo-consumer sed -ri 's#(dubbo-demo-consumer:apollo).*#\\1_200513_1808#g' dp.yaml 应用修改后的资源配置清单： kubectl apply -f http://k8s-yaml.zq.com/test/dubbo-demo-consumer/dp.yaml 访问http://dubbo-testdemo.zq.com/hello?name=noah看是否有我们更改的内容 4.2.3 发布prod环境 镜像在测试环境测试没有问题后,直接使用该镜像发布生产环境,不在重新打包,避免发生错误 同样修改prod环境的dp.yaml,并且应用该资源配置清单 cd /data/k8s-yaml/prod/dubbo-demo-server sed -ri 's#(dubbo-demo-consumer:apollo).*#\\1_200513_1808#g' dp.yaml 应用修改后的资源配置清单： kubectl apply -f http://k8s-yaml.zq.com/test/dubbo-demo-consumer/dp.yaml 已经上线到生产环境，这样一套完整的分环境使用apollo配置中心发布流程已经可以使用了，并且真正做到了一次构建，多平台使用 ","date":"2020-10-01","objectID":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/:4:2","tags":["K8S","转载"],"title":"13_K8S_配置中心实战-多环境交付apollo三组件","uri":"/13_k8s_%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%9E%E6%88%98-%E5%A4%9A%E7%8E%AF%E5%A2%83%E4%BA%A4%E4%BB%98apollo%E4%B8%89%E7%BB%84%E4%BB%B6/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"14_K8S_监控实战-部署prometheus ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:0:0","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"1 prometheus前言相关 由于docker容器的特殊性，传统的zabbix无法对k8s集群内的docker状态进行监控，所以需要使用prometheus来进行监控 prometheus官网：官网地址 ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:1:0","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"1.1 Prometheus的特点 多维度数据模型,使用时间序列数据库TSDB而不使用mysql。 灵活的查询语言PromQL。 不依赖分布式存储，单个服务器节点是自主的。 主要基于HTTP的pull方式主动采集时序数据 也可通过pushgateway获取主动推送到网关的数据。 通过服务发现或者静态配置来发现目标服务对象。 支持多种多样的图表和界面展示，比如Grafana等。 ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:1:1","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"1.2 基本原理 1.2.1 原理说明 Prometheus的基本原理是通过各种exporter提供的HTTP协议接口 周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控。 不需要任何SDK或者其他的集成过程,非常适合做虚拟化环境监控系统，比如VM、Docker、Kubernetes等。 互联网公司常用的组件大部分都有exporter可以直接使用，如Nginx、MySQL、Linux系统信息等。 1.2.2 架构图: 1.2.3 三大套件 Server 主要负责数据采集和存储，提供PromQL查询语言的支持。 Alertmanager 警告管理器，用来进行报警。 Push Gateway 支持临时性Job主动推送指标的中间网关。 1.2.4 架构服务过程 Prometheus Daemon负责定时去目标上抓取metrics(指标)数据 每个抓取目标需要暴露一个http服务的接口给它定时抓取。 支持通过配置文件、文本文件、Zookeeper、DNS SRV Lookup等方式指定抓取目标。 PushGateway用于Client主动推送metrics到PushGateway 而Prometheus只是定时去Gateway上抓取数据。 适合一次性、短生命周期的服务 Prometheus在TSDB数据库存储抓取的所有数据 通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中。 Prometheus通过PromQL和其他API可视化地展示收集的数据。 支持Grafana、Promdash等方式的图表数据可视化。 Prometheus还提供HTTP API的查询方式，自定义所需要的输出。 Alertmanager是独立于Prometheus的一个报警组件 支持Prometheus的查询语句，提供十分灵活的报警方式。 1.2.5 常用的exporter prometheus不同于zabbix，没有agent，使用的是针对不同服务的exporter 正常情况下，监控k8s集群及node，pod，常用的exporter有四个： kube-state-metrics 收集k8s集群master\u0026etcd等基本状态信息 node-exporter 收集k8s集群node信息 cadvisor 收集k8s集群docker容器内部使用资源信息 blackbox-exporte 收集k8s集群docker容器服务是否存活 ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:1:2","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"2 部署4个exporter 老套路，下载docker镜像，准备资源配置清单，应用资源配置清单： ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:2:0","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"2.1 部署kube-state-metrics 2.1.1 准备docker镜像 docker pull quay.io/coreos/kube-state-metrics:v1.5.0 docker tag 91599517197a harbor.zq.com/public/kube-state-metrics:v1.5.0 docker push harbor.zq.com/public/kube-state-metrics:v1.5.0 准备目录 mkdir /data/k8s-yaml/kube-state-metrics cd /data/k8s-yaml/kube-state-metrics 2.1.2 准备rbac资源清单 cat \u003erbac.yaml \u003c\u003c'EOF' apiVersion: v1 kind: ServiceAccount metadata: labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: kube-state-metrics namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: kube-state-metrics rules: - apiGroups: - \"\" resources: - configmaps - secrets - nodes - pods - services - resourcequotas - replicationcontrollers - limitranges - persistentvolumeclaims - persistentvolumes - namespaces - endpoints verbs: - list - watch - apiGroups: - policy resources: - poddisruptionbudgets verbs: - list - watch - apiGroups: - extensions resources: - daemonsets - deployments - replicasets verbs: - list - watch - apiGroups: - apps resources: - statefulsets verbs: - list - watch - apiGroups: - batch resources: - cronjobs - jobs verbs: - list - watch - apiGroups: - autoscaling resources: - horizontalpodautoscalers verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: kube-state-metrics roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-state-metrics subjects: - kind: ServiceAccount name: kube-state-metrics namespace: kube-system EOF 2.1.3 准备Dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"2\" labels: grafanak8sapp: \"true\" app: kube-state-metrics name: kube-state-metrics namespace: kube-system spec: selector: matchLabels: grafanak8sapp: \"true\" app: kube-state-metrics strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: grafanak8sapp: \"true\" app: kube-state-metrics spec: containers: - name: kube-state-metrics image: harbor.zq.com/public/kube-state-metrics:v1.5.0 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: http-metrics protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 8080 scheme: HTTP initialDelaySeconds: 5 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 serviceAccountName: kube-state-metrics EOF 2.1.4 应用资源配置清单 任意node节点执行 kubectl apply -f http://k8s-yaml.zq.com/kube-state-metrics/rbac.yaml kubectl apply -f http://k8s-yaml.zq.com/kube-state-metrics/dp.yaml 验证测试 kubectl get pod -n kube-system -o wide|grep kube-state-metrices ~]# curl http://172.7.21.4:8080/healthz ok 返回OK表示已经成功运行。 ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:2:1","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"2.2 部署node-exporter 由于node-exporter是监控node的，需要每个节点启动一个，所以使用ds控制器 2.2.1 准备docker镜像 docker pull prom/node-exporter:v0.15.0 docker tag 12d51ffa2b22 harbor.zq.com/public/node-exporter:v0.15.0 docker push harbor.zq.com/public/node-exporter:v0.15.0 准备目录 mkdir /data/k8s-yaml/node-exporter cd /data/k8s-yaml/node-exporter 2.2.2 准备ds资源清单 cat \u003eds.yaml \u003c\u003c'EOF' kind: DaemonSet apiVersion: extensions/v1beta1 metadata: name: node-exporter namespace: kube-system labels: daemon: \"node-exporter\" grafanak8sapp: \"true\" spec: selector: matchLabels: daemon: \"node-exporter\" grafanak8sapp: \"true\" template: metadata: name: node-exporter labels: daemon: \"node-exporter\" grafanak8sapp: \"true\" spec: volumes: - name: proc hostPath: path: /proc type: \"\" - name: sys hostPath: path: /sys type: \"\" containers: - name: node-exporter image: harbor.zq.com/public/node-exporter:v0.15.0 imagePullPolicy: IfNotPresent args: - --path.procfs=/host_proc - --path.sysfs=/host_sys ports: - name: node-exporter hostPort: 9100 containerPort: 9100 protocol: TCP volumeMounts: - name: sys readOnly: true mountPath: /host_sys - name: proc readOnly: true mountPath: /host_proc hostNetwork: true EOF 主要用途就是将宿主机的/proc,sys目录挂载给容器,是容器能获取node节点宿主机信息 2.2.3 应用资源配置清单： 任意node节点 kubectl apply -f http://k8s-yaml.zq.com/node-exporter/ds.yaml kubectl get pod -n kube-system -o wide|grep node-exporter ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:2:2","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"2.3 部署cadvisor 2.3.1 准备docker镜像 docker pull google/cadvisor:v0.28.3 docker tag 75f88e3ec333 harbor.zq.com/public/cadvisor:0.28.3 docker push harbor.zq.com/public/cadvisor:0.28.3 准备目录 mkdir /data/k8s-yaml/cadvisor cd /data/k8s-yaml/cadvisor 2.3.2 准备ds资源清单 cadvisor由于要获取每个node上的pod信息,因此也需要使用daemonset方式运行 cat \u003eds.yaml \u003c\u003c'EOF' apiVersion: apps/v1 kind: DaemonSet metadata: name: cadvisor namespace: kube-system labels: app: cadvisor spec: selector: matchLabels: name: cadvisor template: metadata: labels: name: cadvisor spec: hostNetwork: true #------pod的tolerations与node的Taints配合,做POD指定调度---- tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule #------------------------------------- containers: - name: cadvisor image: harbor.zq.com/public/cadvisor:v0.28.3 imagePullPolicy: IfNotPresent volumeMounts: - name: rootfs mountPath: /rootfs readOnly: true - name: var-run mountPath: /var/run - name: sys mountPath: /sys readOnly: true - name: docker mountPath: /var/lib/docker readOnly: true ports: - name: http containerPort: 4194 protocol: TCP readinessProbe: tcpSocket: port: 4194 initialDelaySeconds: 5 periodSeconds: 10 args: - --housekeeping_interval=10s - --port=4194 terminationGracePeriodSeconds: 30 volumes: - name: rootfs hostPath: path: / - name: var-run hostPath: path: /var/run - name: sys hostPath: path: /sys - name: docker hostPath: path: /data/docker EOF 2.3.3 应用资源配置清单： 应用清单前,先在每个node上做以下软连接,否则服务可能报错 mount -o remount,rw /sys/fs/cgroup/ ln -s /sys/fs/cgroup/cpu,cpuacct /sys/fs/cgroup/cpuacct,cpu 应用清单 kubectl apply -f http://k8s-yaml.zq.com/cadvisor/ds.yaml 检查： kubectl -n kube-system get pod -o wide|grep cadvisor ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:2:3","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"2.4 部署blackbox-exporter 2.4.1 准备docker镜像 docker pull prom/blackbox-exporter:v0.15.1 docker tag 81b70b6158be harbor.zq.com/public/blackbox-exporter:v0.15.1 docker push harbor.zq.com/public/blackbox-exporter:v0.15.1 准备目录 mkdir /data/k8s-yaml/blackbox-exporter cd /data/k8s-yaml/blackbox-exporter 2.4.2 准备cm资源清单 cat \u003ecm.yaml \u003c\u003c'EOF' apiVersion: v1 kind: ConfigMap metadata: labels: app: blackbox-exporter name: blackbox-exporter namespace: kube-system data: blackbox.yml: |- modules: http_2xx: prober: http timeout: 2s http: valid_http_versions: [\"HTTP/1.1\", \"HTTP/2\"] valid_status_codes: [200,301,302] method: GET preferred_ip_protocol: \"ip4\" tcp_connect: prober: tcp timeout: 2s EOF 2.4.3 准备dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' kind: Deployment apiVersion: extensions/v1beta1 metadata: name: blackbox-exporter namespace: kube-system labels: app: blackbox-exporter annotations: deployment.kubernetes.io/revision: 1 spec: replicas: 1 selector: matchLabels: app: blackbox-exporter template: metadata: labels: app: blackbox-exporter spec: volumes: - name: config configMap: name: blackbox-exporter defaultMode: 420 containers: - name: blackbox-exporter image: harbor.zq.com/public/blackbox-exporter:v0.15.1 imagePullPolicy: IfNotPresent args: - --config.file=/etc/blackbox_exporter/blackbox.yml - --log.level=info - --web.listen-address=:9115 ports: - name: blackbox-port containerPort: 9115 protocol: TCP resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 50Mi volumeMounts: - name: config mountPath: /etc/blackbox_exporter readinessProbe: tcpSocket: port: 9115 initialDelaySeconds: 5 timeoutSeconds: 5 periodSeconds: 10 successThreshold: 1 failureThreshold: 3 EOF 2.4.4 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' kind: Service apiVersion: v1 metadata: name: blackbox-exporter namespace: kube-system spec: selector: app: blackbox-exporter ports: - name: blackbox-port protocol: TCP port: 9115 EOF 2.4.5 准备ingress资源清单 cat \u003eingress.yaml \u003c\u003c'EOF' apiVersion: extensions/v1beta1 kind: Ingress metadata: name: blackbox-exporter namespace: kube-system spec: rules: - host: blackbox.zq.com http: paths: - path: / backend: serviceName: blackbox-exporter servicePort: blackbox-port EOF 2.4.6 添加域名解析 这里用到了一个域名，添加解析 vi /var/named/zq.com.zone blackbox A 10.4.7.10 systemctl restart named 2.4.7 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/blackbox-exporter/cm.yaml kubectl apply -f http://k8s-yaml.zq.com/blackbox-exporter/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/blackbox-exporter/svc.yaml kubectl apply -f http://k8s-yaml.zq.com/blackbox-exporter/ingress.yaml 2.4.8 访问域名测试 访问http://blackbox.zq.com，显示如下界面,表示blackbox已经运行成 ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:2:4","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"3 部署prometheus server ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:3:0","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"3.1 准备prometheus server环境 3.1.1 准备docker镜像 docker pull prom/prometheus:v2.14.0 docker tag 7317640d555e harbor.zq.com/infra/prometheus:v2.14.0 docker push harbor.zq.com/infra/prometheus:v2.14.0 准备目录 mkdir /data/k8s-yaml/prometheus-server cd /data/k8s-yaml/prometheus-server 3.1.2 准备rbac资源清单 cat \u003erbac.yaml \u003c\u003c'EOF' apiVersion: v1 kind: ServiceAccount metadata: labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: prometheus namespace: infra --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: prometheus rules: - apiGroups: - \"\" resources: - nodes - nodes/metrics - services - endpoints - pods verbs: - get - list - watch - apiGroups: - \"\" resources: - configmaps verbs: - get - nonResourceURLs: - /metrics verbs: - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: prometheus roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus subjects: - kind: ServiceAccount name: prometheus namespace: infra EOF 3.1.3 准备dp资源清单 加上--web.enable-lifecycle启用远程热加载配置文件,配置文件改变后不用重启prometheus 调用指令是curl -X POST http://localhost:9090/-/reload storage.tsdb.min-block-duration=10m只加载10分钟数据到内 storage.tsdb.retention=72h 保留72小时数据 cat \u003edp.yaml \u003c\u003c'EOF' apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"5\" labels: name: prometheus name: prometheus namespace: infra spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 7 selector: matchLabels: app: prometheus strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: prometheus spec: containers: - name: prometheus image: harbor.zq.com/infra/prometheus:v2.14.0 imagePullPolicy: IfNotPresent command: - /bin/prometheus args: - --config.file=/data/etc/prometheus.yml - --storage.tsdb.path=/data/prom-db - --storage.tsdb.min-block-duration=10m - --storage.tsdb.retention=72h - --web.enable-lifecycle ports: - containerPort: 9090 protocol: TCP volumeMounts: - mountPath: /data name: data resources: requests: cpu: \"1000m\" memory: \"1.5Gi\" limits: cpu: \"2000m\" memory: \"3Gi\" imagePullSecrets: - name: harbor securityContext: runAsUser: 0 serviceAccountName: prometheus volumes: - name: data nfs: server: hdss7-200 path: /data/nfs-volume/prometheus EOF 3.1.4 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: prometheus namespace: infra spec: ports: - port: 9090 protocol: TCP targetPort: 9090 selector: app: prometheus EOF 3.1.5 准备ingress资源清单 cat \u003eingress.yaml \u003c\u003c'EOF' apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: traefik name: prometheus namespace: infra spec: rules: - host: prometheus.zq.com http: paths: - path: / backend: serviceName: prometheus servicePort: 9090 EOF 3.1.6 添加域名解析 这里用到一个域名prometheus.zq.com，添加解析： vi /var/named/od.com.zone prometheus A 10.4.7.10 systemctl restart named ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:3:1","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"3.2 部署prometheus server 3.2.1 准备目录和证书 mkdir -p /data/nfs-volume/prometheus/etc mkdir -p /data/nfs-volume/prometheus/prom-db cd /data/nfs-volume/prometheus/etc/ # 拷贝配置文件中用到的证书： cp /opt/certs/ca.pem ./ cp /opt/certs/client.pem ./ cp /opt/certs/client-key.pem ./ 3.2.2 创建prometheus配置文件 配置文件说明: 此配置为通用配置,除第一个jobetcd是做的静态配置外,其他8个job都是做的自动发现 因此只需要修改etcd的配置后,就可以直接用于生产环境 cat \u003e/data/nfs-volume/prometheus/etc/prometheus.yml \u003c\u003c'EOF' global: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: 'etcd' tls_config: ca_file: /data/etc/ca.pem cert_file: /data/etc/client.pem key_file: /data/etc/client-key.pem scheme: https static_configs: - targets: - '10.4.7.12:2379' - '10.4.7.21:2379' - '10.4.7.22:2379' - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - job_name: 'kubernetes-kubelet' kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __address__ replacement: ${1}:10255 - job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __address__ replacement: ${1}:4194 - job_name: 'kubernetes-kube-state' kubernetes_sd_configs: - role: pod relabel_configs: - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - source_labels: [__meta_kubernetes_pod_label_grafanak8sapp] regex: .*true.* action: keep - source_labels: ['__meta_kubernetes_pod_label_daemon', '__meta_kubernetes_pod_node_name'] regex: 'node-exporter;(.*)' action: replace target_label: nodename - job_name: 'blackbox_http_pod_probe' metrics_path: /probe kubernetes_sd_configs: - role: pod params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_blackbox_scheme] action: keep regex: http - source_labels: [__address__, __meta_kubernetes_pod_annotation_blackbox_port, __meta_kubernetes_pod_annotation_blackbox_path] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+);(.+) replacement: $1:$2$3 target_label: __param_target - action: replace target_label: __address__ replacement: blackbox-exporter.kube-system:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - job_name: 'blackbox_tcp_pod_probe' metrics_path: /probe kubernetes_sd_configs: - role: pod params: module: [tcp_connect] relabel_configs: - source_labels: [__meta_kubern","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:3:2","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"4 使服务能被prometheus自动监控 点击status-\u003etargets，展示的就是我们在prometheus.yml中配置的job-name，这些targets基本可以满足我们收集数据的需求。 5个编号的job-name已经被发现并获取数据 接下来就需要将剩下的4个ob-name对应的服务纳入监控 纳入监控的方式是给需要收集数据的服务添加annotations ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:4:0","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"4.1 让traefik能被自动监控 4.1.1 修改traefik的yaml 修改fraefik的yaml文件,跟labels同级,添加annotations配置 vim /data/k8s-yaml/traefik/ds.yaml ........ spec: template: metadata: labels: k8s-app: traefik-ingress name: traefik-ingress #--------增加内容-------- annotations: prometheus_io_scheme: \"traefik\" prometheus_io_path: \"/metrics\" prometheus_io_port: \"8080\" #--------增加结束-------- spec: serviceAccountName: traefik-ingress-controller ........ 任意节点重新应用配置 kubectl delete -f http://k8s-yaml.zq.com/traefik/ds.yaml kubectl apply -f http://k8s-yaml.zq.com/traefik/ds.yaml 4.1.2 应用配置查看 等待pod重启以后，再在prometheus上查看traefik是否能正常获取数据了 ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:4:1","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"4.2 用blackbox检测TCP/HTTP服务状态 blackbox是检测容器内服务存活性的，也就是端口健康状态检查，分为tcp和http两种方法 能用http的情况尽量用http,没有提供http接口的服务才用tcp 4.2.1 被检测服务准备 使用测试环境的dubbo服务来做演示,其他环境类似 dashboard中开启apollo-portal和test空间中的apollo dubbo-demo-service使用tcp的annotation dubbo-demo-consumer使用HTTP的annotation 4.2.2 添加tcp的annotation 等两个服务起来以后，首先在dubbo-demo-service资源中添加一个TCP的annotation vim /data/k8s-yaml/test/dubbo-demo-server/dp.yaml ...... spec: ...... template: metadata: labels: app: dubbo-demo-service name: dubbo-demo-service #--------增加内容-------- annotations: blackbox_port: \"20880\" blackbox_scheme: \"tcp\" #--------增加结束-------- spec: containers: image: harbor.zq.com/app/dubbo-demo-service:apollo_200512_0746 任意节点重新应用配置 kubectl delete -f http://k8s-yaml.zq.com/test/dubbo-demo-server/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/test/dubbo-demo-server/dp.yaml 浏览器中查看http://blackbox.zq.com/和http://prometheus.zq.com/targets 我们运行的dubbo-demo-server服务,tcp端口20880已经被发现并在监控中 4.2.3 添加http的annotation 接下来在dubbo-demo-consumer资源中添加一个HTTP的annotation： vim /data/k8s-yaml/test/dubbo-demo-consumer/dp.yaml spec: ...... template: metadata: labels: app: dubbo-demo-consumer name: dubbo-demo-consumer #--------增加内容-------- annotations: blackbox_path: \"/hello?name=health\" blackbox_port: \"8080\" blackbox_scheme: \"http\" #--------增加结束-------- spec: containers: - name: dubbo-demo-consumer ...... 任意节点重新应用配置 kubectl delete -f http://k8s-yaml.zq.com/test/dubbo-demo-consumer/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/test/dubbo-demo-consumer/dp.yaml ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:4:2","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"4.3 添加监控jvm信息 dubbo-demo-service和dubbo-demo-consumer都添加下列annotation注解,以便监控pod中的jvm信息 vim /data/k8s-yaml/test/dubbo-demo-server/dp.yaml vim /data/k8s-yaml/test/dubbo-demo-consumer/dp.yaml annotations: #....已有略.... prometheus_io_scrape: \"true\" prometheus_io_port: \"12346\" prometheus_io_path: \"/\" 12346是dubbo的POD启动命令中使用jmx_javaagent用到的端口,因此可以用来收集jvm信息 任意节点重新应用配置 kubectl apply -f http://k8s-yaml.zq.com/test/dubbo-demo-server/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/test/dubbo-demo-consumer/dp.yaml 至此,所有9个服务,都获取了数据 ","date":"2020-10-01","objectID":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/:4:3","tags":["K8S","转载"],"title":"14_K8S_监控实战-部署prometheus","uri":"/14_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-%E9%83%A8%E7%BD%B2prometheus/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/","tags":["K8S","转载"],"title":"15_K8S_监控实战-grafana出图_alert告警","uri":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/"},{"categories":["转载","K8S"],"content":"15_K8S_监控实战-grafana出图_alert告警 ","date":"2020-10-01","objectID":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/:0:0","tags":["K8S","转载"],"title":"15_K8S_监控实战-grafana出图_alert告警","uri":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/"},{"categories":["转载","K8S"],"content":"1 使用炫酷的grafana出图 prometheus的dashboard虽然号称拥有多种多样的图表,但是实在太简陋了,一般都用专业的grafana工具来出图 grafana官方dockerhub地址 grafana官方github地址 grafana官网 ","date":"2020-10-01","objectID":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/:1:0","tags":["K8S","转载"],"title":"15_K8S_监控实战-grafana出图_alert告警","uri":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/"},{"categories":["转载","K8S"],"content":"1.1 部署grafana 1.1.1 准备镜像 docker pull grafana/grafana:5.4.2 docker tag 6f18ddf9e552 harbor.zq.com/infra/grafana:v5.4.2 docker push harbor.zq.com/infra/grafana:v5.4.2 准备目录 mkdir /data/k8s-yaml/grafana cd /data/k8s-yaml/grafana 1.1.2 准备rbac资源清单 cat \u003erbac.yaml \u003c\u003c'EOF' apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: grafana rules: - apiGroups: - \"*\" resources: - namespaces - deployments - pods verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: grafana roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: grafana subjects: - kind: User name: k8s-node EOF 1.1.3 准备dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: grafana name: grafana name: grafana namespace: infra spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 7 selector: matchLabels: name: grafana strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: grafana name: grafana spec: containers: - name: grafana image: harbor.zq.com/infra/grafana:v5.4.2 imagePullPolicy: IfNotPresent ports: - containerPort: 3000 protocol: TCP volumeMounts: - mountPath: /var/lib/grafana name: data imagePullSecrets: - name: harbor securityContext: runAsUser: 0 volumes: - nfs: server: hdss7-200 path: /data/nfs-volume/grafana name: data EOF 创建frafana数据目录 mkdir /data/nfs-volume/grafana 1.1.4 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: grafana namespace: infra spec: ports: - port: 3000 protocol: TCP targetPort: 3000 selector: app: grafana EOF 1.1.5 准备ingress资源清单 cat \u003eingress.yaml \u003c\u003c'EOF' apiVersion: extensions/v1beta1 kind: Ingress metadata: name: grafana namespace: infra spec: rules: - host: grafana.zq.com http: paths: - path: / backend: serviceName: grafana servicePort: 3000 EOF 1.1.6 域名解析 vi /var/named/zq.com.zone grafana A 10.4.7.10 systemctl restart named 1.1.7 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/grafana/rbac.yaml kubectl apply -f http://k8s-yaml.zq.com/grafana/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/grafana/svc.yaml kubectl apply -f http://k8s-yaml.zq.com/grafana/ingress.yaml ","date":"2020-10-01","objectID":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/:1:1","tags":["K8S","转载"],"title":"15_K8S_监控实战-grafana出图_alert告警","uri":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/"},{"categories":["转载","K8S"],"content":"1.2 使用grafana出图 1.2.1 浏览器访问验证 访问http://grafana.zq.com,默认用户名密码admin/admin 能成功访问表示安装成功 进入后立即修改管理员密码为admin123 1.2.2 进入容器安装插件 grafana确认启动好以后,需要进入grafana容器内部,安装以下插件 kubectl -n infra exec -it grafana-d6588db94-xr4s6 /bin/bash # 以下命令在容器内执行 grafana-cli plugins install grafana-kubernetes-app grafana-cli plugins install grafana-clock-panel grafana-cli plugins install grafana-piechart-panel grafana-cli plugins install briangann-gauge-panel grafana-cli plugins install natel-discrete-panel 1.2.3 配置数据源 添加数据源,依次点击：左侧锯齿图标–\u003eadd data source–\u003ePrometheus 添加完成后重启grafana kubectl -n infra delete pod grafana-7dd95b4c8d-nj5cx 1.2.4 添加K8S集群信息 启用K8S插件,依次点击：左侧锯齿图标–\u003ePlugins–\u003ekubernetes–\u003eEnable 新建cluster,依次点击：左侧K8S图标–\u003eNew Cluster 1.2.5 查看k8s集群数据和图表 添加完需要稍等几分钟，在没有取到数据之前，会报http forbidden，没关系，等一会就好。大概2-5分钟。 点击Cluster Dashboard ","date":"2020-10-01","objectID":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/:1:2","tags":["K8S","转载"],"title":"15_K8S_监控实战-grafana出图_alert告警","uri":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/"},{"categories":["转载","K8S"],"content":"2 配置alert告警插件 ","date":"2020-10-01","objectID":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/:2:0","tags":["K8S","转载"],"title":"15_K8S_监控实战-grafana出图_alert告警","uri":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/"},{"categories":["转载","K8S"],"content":"2.1 部署alert插件 2.1.1 准备docker镜像 docker pull docker.io/prom/alertmanager:v0.14.0 docker tag 23744b2d645c harbor.zq.com/infra/alertmanager:v0.14.0 docker push harbor.zq.com/infra/alertmanager:v0.14.0 准备目录 mkdir /data/k8s-yaml/alertmanager cd /data/k8s-yaml/alertmanager 2.1.2 准备cm资源清单 cat \u003ecm.yaml \u003c\u003c'EOF' apiVersion: v1 kind: ConfigMap metadata: name: alertmanager-config namespace: infra data: config.yml: |- global: # 在没有报警的情况下声明为已解决的时间 resolve_timeout: 5m # 配置邮件发送信息 smtp_smarthost: 'smtp.163.com:25' smtp_from: 'xxx@163.com' smtp_auth_username: 'xxx@163.com' smtp_auth_password: 'xxxxxx' smtp_require_tls: false templates: - '/etc/alertmanager/*.tmpl' # 所有报警信息进入后的根路由，用来设置报警的分发策略 route: # 这里的标签列表是接收到报警信息后的重新分组标签，例如，接收到的报警信息里面有许多具有 cluster=A 和 alertname=LatncyHigh 这样的标签的报警信息将会批量被聚合到一个分组里面 group_by: ['alertname', 'cluster'] # 当一个新的报警分组被创建后，需要等待至少group_wait时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。 group_wait: 30s # 当第一个报警发送后，等待'group_interval'时间来发送新的一组报警信息。 group_interval: 5m # 如果一个报警信息已经发送成功了，等待'repeat_interval'时间来重新发送他们 repeat_interval: 5m # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器 receiver: default receivers: - name: 'default' email_configs: - to: 'xxxx@qq.com' send_resolved: true html: '{{ template \"email.to.html\" . }}' headers: { Subject: \" {{ .CommonLabels.instance }} {{ .CommonAnnotations.summary }}\" } email.tmpl: | {{ define \"email.to.html\" }} {{- if gt (len .Alerts.Firing) 0 -}} {{ range .Alerts }} 告警程序: prometheus_alert \u003cbr\u003e 告警级别: {{ .Labels.severity }} \u003cbr\u003e 告警类型: {{ .Labels.alertname }} \u003cbr\u003e 故障主机: {{ .Labels.instance }} \u003cbr\u003e 告警主题: {{ .Annotations.summary }} \u003cbr\u003e 触发时间: {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} \u003cbr\u003e {{ end }}{{ end -}} {{- if gt (len .Alerts.Resolved) 0 -}} {{ range .Alerts }} 告警程序: prometheus_alert \u003cbr\u003e 告警级别: {{ .Labels.severity }} \u003cbr\u003e 告警类型: {{ .Labels.alertname }} \u003cbr\u003e 故障主机: {{ .Labels.instance }} \u003cbr\u003e 告警主题: {{ .Annotations.summary }} \u003cbr\u003e 触发时间: {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} \u003cbr\u003e 恢复时间: {{ .EndsAt.Format \"2006-01-02 15:04:05\" }} \u003cbr\u003e {{ end }}{{ end -}} {{- end }} EOF 2.1.3 准备dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' apiVersion: extensions/v1beta1 kind: Deployment metadata: name: alertmanager namespace: infra spec: replicas: 1 selector: matchLabels: app: alertmanager template: metadata: labels: app: alertmanager spec: containers: - name: alertmanager image: harbor.zq.com/infra/alertmanager:v0.14.0 args: - \"--config.file=/etc/alertmanager/config.yml\" - \"--storage.path=/alertmanager\" ports: - name: alertmanager containerPort: 9093 volumeMounts: - name: alertmanager-cm mountPath: /etc/alertmanager volumes: - name: alertmanager-cm configMap: name: alertmanager-config imagePullSecrets: - name: harbor EOF 2.1.4 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: alertmanager namespace: infra spec: selector: app: alertmanager ports: - port: 80 targetPort: 9093 EOF 2.1.5 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/alertmanager/cm.yaml kubectl apply -f http://k8s-yaml.zq.com/alertmanager/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/alertmanager/svc.yaml ","date":"2020-10-01","objectID":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/:2:1","tags":["K8S","转载"],"title":"15_K8S_监控实战-grafana出图_alert告警","uri":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/"},{"categories":["转载","K8S"],"content":"2.2 K8S使用alert报警 2.2.1 k8s创建基础报警规则文件 cat \u003e/data/nfs-volume/prometheus/etc/rules.yml \u003c\u003c'EOF' groups: - name: hostStatsAlert rules: - alert: hostCpuUsageAlert expr: sum(avg without (cpu)(irate(node_cpu{mode!='idle'}[5m]))) by (instance) \u003e 0.85 for: 5m labels: severity: warning annotations: summary: \"{{ $labels.instance }} CPU usage above 85% (current value: {{ $value }}%)\" - alert: hostMemUsageAlert expr: (node_memory_MemTotal - node_memory_MemAvailable)/node_memory_MemTotal \u003e 0.85 for: 5m labels: severity: warning annotations: summary: \"{{ $labels.instance }} MEM usage above 85% (current value: {{ $value }}%)\" - alert: OutOfInodes expr: node_filesystem_free{fstype=\"overlay\",mountpoint =\"/\"} / node_filesystem_size{fstype=\"overlay\",mountpoint =\"/\"} * 100 \u003c 10 for: 5m labels: severity: warning annotations: summary: \"Out of inodes (instance {{ $labels.instance }})\" description: \"Disk is almost running out of available inodes (\u003c 10% left) (current value: {{ $value }})\" - alert: OutOfDiskSpace expr: node_filesystem_free{fstype=\"overlay\",mountpoint =\"/rootfs\"} / node_filesystem_size{fstype=\"overlay\",mountpoint =\"/rootfs\"} * 100 \u003c 10 for: 5m labels: severity: warning annotations: summary: \"Out of disk space (instance {{ $labels.instance }})\" description: \"Disk is almost full (\u003c 10% left) (current value: {{ $value }})\" - alert: UnusualNetworkThroughputIn expr: sum by (instance) (irate(node_network_receive_bytes[2m])) / 1024 / 1024 \u003e 100 for: 5m labels: severity: warning annotations: summary: \"Unusual network throughput in (instance {{ $labels.instance }})\" description: \"Host network interfaces are probably receiving too much data (\u003e 100 MB/s) (current value: {{ $value }})\" - alert: UnusualNetworkThroughputOut expr: sum by (instance) (irate(node_network_transmit_bytes[2m])) / 1024 / 1024 \u003e 100 for: 5m labels: severity: warning annotations: summary: \"Unusual network throughput out (instance {{ $labels.instance }})\" description: \"Host network interfaces are probably sending too much data (\u003e 100 MB/s) (current value: {{ $value }})\" - alert: UnusualDiskReadRate expr: sum by (instance) (irate(node_disk_bytes_read[2m])) / 1024 / 1024 \u003e 50 for: 5m labels: severity: warning annotations: summary: \"Unusual disk read rate (instance {{ $labels.instance }})\" description: \"Disk is probably reading too much data (\u003e 50 MB/s) (current value: {{ $value }})\" - alert: UnusualDiskWriteRate expr: sum by (instance) (irate(node_disk_bytes_written[2m])) / 1024 / 1024 \u003e 50 for: 5m labels: severity: warning annotations: summary: \"Unusual disk write rate (instance {{ $labels.instance }})\" description: \"Disk is probably writing too much data (\u003e 50 MB/s) (current value: {{ $value }})\" - alert: UnusualDiskReadLatency expr: rate(node_disk_read_time_ms[1m]) / rate(node_disk_reads_completed[1m]) \u003e 100 for: 5m labels: severity: warning annotations: summary: \"Unusual disk read latency (instance {{ $labels.instance }})\" description: \"Disk latency is growing (read operations \u003e 100ms) (current value: {{ $value }})\" - alert: UnusualDiskWriteLatency expr: rate(node_disk_write_time_ms[1m]) / rate(node_disk_writes_completedl[1m]) \u003e 100 for: 5m labels: severity: warning annotations: summary: \"Unusual disk write latency (instance {{ $labels.instance }})\" description: \"Disk latency is growing (write operations \u003e 100ms) (current value: {{ $value }})\" - name: http_status rules: - alert: ProbeFailed expr: probe_success == 0 for: 1m labels: severity: error annotations: summary: \"Probe failed (instance {{ $labels.instance }})\" description: \"Probe failed (current value: {{ $value }})\" - alert: StatusCode expr: probe_http_status_code \u003c= 199 OR probe_http_status_code \u003e= 400 for: 1m labels: severity: error annotations: summary: \"Status Code (instance {{ $labels.instance }})\" description: \"HTTP status code is not 200-399 (current value: {{ $value }})\" - alert: SslCertificateWillExpireSoon expr: probe_ssl_earliest_cert_expiry - time() \u003c 86400 * 30 for: 5m labels: severity: warning annotations:","date":"2020-10-01","objectID":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/:2:2","tags":["K8S","转载"],"title":"15_K8S_监控实战-grafana出图_alert告警","uri":"/15_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-grafana%E5%87%BA%E5%9B%BE_alert%E5%91%8A%E8%AD%A6/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"16_K8S_监控实战-ELK收集K8S内应用日志 ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:0:0","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"1 收集K8S日志方案 K8s系统里的业务应用是高度“动态化”的，随着容器编排的进行，业务容器在不断的被创建、被摧毁、被漂移、被扩缩容… 我们需要这样一套日志收集、分析的系统： 收集 – 能够采集多种来源的日志数据（流式日志收集器） 传输 – 能够稳定的把日志数据传输到中央系统（消息队列） 存储 – 可以将日志以结构化数据的形式存储起来（搜索引擎） 分析 – 支持方便的分析、检索方法，最好有GUI管理系统（web） 警告 – 能够提供错误报告，监控机制（监控系统） ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:1:0","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"1.1 传统ELk模型缺点： Logstash使用Jruby语言开发，吃资源，大量部署消耗极高 业务程序与logstash耦合过松，不利于业务迁移 日志收集与ES耦合又过紧，（Logstash）易打爆（ES）、丢数据 在容器云环境下，传统ELk模型难以完成工作 ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:1:1","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"1.2 K8s容器日志收集模型 ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:1:2","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"2 制作tomcat底包 ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:2:0","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"2.1 准备tomcat底包 2.1.1 下载tomcat8 cd /opt/src/ wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-8/v8.5.50/bin/apache-tomcat-8.5.50.tar.gz mkdir /data/dockerfile/tomcat tar xf apache-tomcat-8.5.50.tar.gz -C /data/dockerfile/tomcat cd /data/dockerfile/tomcat 2.1.2 简单配置tomcat 删除自带网页 rm -rf apache-tomcat-8.5.50/webapps/* 关闭AJP端口 tomcat]# vim apache-tomcat-8.5.50/conf/server.xml \u003c!-- \u003cConnector port=\"8009\" protocol=\"AJP/1.3\" redirectPort=\"8443\" /\u003e --\u003e 修改日志类型 删除3manager，4host-manager的handlers tomcat]# vim apache-tomcat-8.5.50/conf/logging.properties handlers = [1catalina.org.apache.juli.AsyncFileHandler](http://1catalina.org.apache.juli.asyncfilehandler/), [2localhost.org.apache.juli.AsyncFileHandler](http://2localhost.org.apache.juli.asyncfilehandler/), java.util.logging.ConsoleHandler 日志级别改为INFO 1catalina.org.apache.juli.AsyncFileHandler.level = INFO 2localhost.org.apache.juli.AsyncFileHandler.level = INFO java.util.logging.ConsoleHandler.level = INFO 注释所有关于3manager，4host-manager日志的配置 #3manager.org.apache.juli.AsyncFileHandler.level = FINE #3manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs #3manager.org.apache.juli.AsyncFileHandler.prefix = manager. #3manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 #4host-manager.org.apache.juli.AsyncFileHandler.level = FINE #4host-manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs #4host-manager.org.apache.juli.AsyncFileHandler.prefix = host-manager. #4host-manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:2:1","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"2.2 准备docker镜像 2.2.1 创建dockerfile cat \u003eDockerfile \u003c\u003c'EOF' From harbor.od.com/public/jre:8u112 RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026\u0026\\ echo 'Asia/Shanghai' \u003e/etc/timezone ENV CATALINA_HOME /opt/tomcat ENV LANG zh_CN.UTF-8 ADD apache-tomcat-8.5.50/ /opt/tomcat ADD config.yml /opt/prom/config.yml ADD jmx_javaagent-0.3.1.jar /opt/prom/jmx_javaagent-0.3.1.jar WORKDIR /opt/tomcat ADD entrypoint.sh /entrypoint.sh CMD [\"/bin/bash\",\"/entrypoint.sh\"] EOF 2.2.2 准备dockerfile所需文件 JVM监控所需jar包 wget -O jmx_javaagent-0.3.1.jar https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.3.1/jmx_prometheus_javaagent-0.3.1.jar jmx_agent读取的配置文件 cat \u003econfig.yml \u003c\u003c'EOF' --- rules: - pattern: '.*' EOF 容器启动脚本 cat \u003eentrypoint.sh \u003c\u003c'EOF' #!/bin/bash M_OPTS=\"-Duser.timezone=Asia/Shanghai -javaagent:/opt/prom/jmx_javaagent-0.3.1.jar=$(hostname -i):${M_PORT:-\"12346\"}:/opt/prom/config.yml\" # Pod ip:port 监控规则传给jvm监控客户端 C_OPTS=${C_OPTS} # 启动追加参数 MIN_HEAP=${MIN_HEAP:-\"128m\"} # java虚拟机初始化时的最小内存 MAX_HEAP=${MAX_HEAP:-\"128m\"} # java虚拟机初始化时的最大内存 JAVA_OPTS=${JAVA_OPTS:-\"-Xmn384m -Xss256k -Duser.timezone=GMT+08 -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSClassUnloadingEnabled -XX:LargePageSizeInBytes=128m -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram -Dfile.encoding=UTF8 -Dsun.jnu.encoding=UTF8\"} # 年轻代，gc回收 CATALINA_OPTS=\"${CATALINA_OPTS}\" JAVA_OPTS=\"${M_OPTS} ${C_OPTS} -Xms${MIN_HEAP} -Xmx${MAX_HEAP} ${JAVA_OPTS}\" sed -i -e \"1a\\JAVA_OPTS=\\\"$JAVA_OPTS\\\"\" -e \"1a\\CATALINA_OPTS=\\\"$CATALINA_OPTS\\\"\" /opt/tomcat/bin/catalina.sh cd /opt/tomcat \u0026\u0026 /opt/tomcat/bin/catalina.sh run 2\u003e\u00261 \u003e\u003e /opt/tomcat/logs/stdout.log # 日志文件 EOF 2.2.3 构建docker docker build . -t harbor.zq.com/base/tomcat:v8.5.50 docker push harbor.zq.com/base/tomcat:v8.5.50 ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:2:2","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"3 部署ElasticSearch 官网 官方github地址 下载地址 部署HDSS7-12.host.com上： ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:3:0","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"3.1 安装ElasticSearch 3.1.1 下载二进制包 cd /opt/src wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.8.6.tar.gz tar xf elasticsearch-6.8.6.tar.gz -C /opt/ ln -s /opt/elasticsearch-6.8.6/ /opt/elasticsearch cd /opt/elasticsearch 3.1.2 配置elasticsearch.yml mkdir -p /data/elasticsearch/{data,logs} cat \u003econfig/elasticsearch.yml \u003c\u003c'EOF' cluster.name: es.zq.com node.name: hdss7-12.host.com path.data: /data/elasticsearch/data path.logs: /data/elasticsearch/logs bootstrap.memory_lock: true network.host: 10.4.7.12 http.port: 9200 EOF ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:3:1","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"3.2 优化其他设置 3.2.1 设置jvm参数 elasticsearch]# vi config/jvm.options # 根据环境设置，-Xms和-Xmx设置为相同的值，推荐设置为机器内存的一半左右 -Xms512m -Xmx512m 3.2.2 创建普通用户 useradd -s /bin/bash -M es chown -R es.es /opt/elasticsearch-6.8.6 chown -R es.es /data/elasticsearch/ 3.2.3 调整文件描述符 vim /etc/security/limits.d/es.conf es hard nofile 65536 es soft fsize unlimited es hard memlock unlimited es soft memlock unlimited 3.2.4 调整内核参数 sysctl -w vm.max_map_count=262144 echo \"vm.max_map_count=262144\" \u003e /etc/sysctl.conf sysctl -p ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:3:2","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"3.3 启动ES 3.3.1 启动es服务 ]# su -c \"/opt/elasticsearch/bin/elasticsearch -d\" es ]# netstat -luntp|grep 9200 tcp6 0 0 10.4.7.12:9200 :::* LISTEN 16784/java 3.3.1 调整ES日志模板 curl -XPUT http://10.4.7.12:9200/_template/k8s -d '{ \"template\" : \"k8s*\", \"index_patterns\": [\"k8s*\"], \"settings\": { \"number_of_shards\": 5, \"number_of_replicas\": 0 # 生产为3份副本集，本es为单节点，不能配置副本集 } }' ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:3:3","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"4 部署kafka和kafka-manager 官网 官方github地址 下载地址 HDSS7-11.host.com上： ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:4:0","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"4.1 但节点安装kafka 4.1.1 下载包 cd /opt/src wget https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz tar xf kafka_2.12-2.2.0.tgz -C /opt/ ln -s /opt/kafka_2.12-2.2.0/ /opt/kafka cd /opt/kafka 4.1.2 修改配置 mkdir /data/kafka/logs -p cat \u003econfig/server.properties \u003c\u003c'EOF' log.dirs=/data/kafka/logs zookeeper.connect=localhost:2181 # zk消息队列地址 log.flush.interval.messages=10000 log.flush.interval.ms=1000 delete.topic.enable=true host.name=hdss7-11.host.com EOF 4.1.3 启动kafka bin/kafka-server-start.sh -daemon config/server.properties ]# netstat -luntp|grep 9092 tcp6 0 0 10.4.7.11:9092 :::* LISTEN 34240/java ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:4:1","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"4.2 获取kafka-manager的docker镜像 官方github地址 源码下载地址 运维主机HDSS7-200.host.com上： kafka-manager是kafka的一个web管理页面,非必须 4.2.1 方法一 通过dockerfile获取 1 准备Dockerfile cat \u003e/data/dockerfile/kafka-manager/Dockerfile \u003c\u003c'EOF' FROM hseeberger/scala-sbt ENV ZK_HOSTS=10.4.7.11:2181 \\ KM_VERSION=2.0.0.2 RUN mkdir -p /tmp \u0026\u0026 \\ cd /tmp \u0026\u0026 \\ wget https://github.com/yahoo/kafka-manager/archive/${KM_VERSION}.tar.gz \u0026\u0026 \\ tar xxf ${KM_VERSION}.tar.gz \u0026\u0026 \\ cd /tmp/kafka-manager-${KM_VERSION} \u0026\u0026 \\ sbt clean dist \u0026\u0026 \\ unzip -d / ./target/universal/kafka-manager-${KM_VERSION}.zip \u0026\u0026 \\ rm -fr /tmp/${KM_VERSION} /tmp/kafka-manager-${KM_VERSION} WORKDIR /kafka-manager-${KM_VERSION} EXPOSE 9000 ENTRYPOINT [\"./bin/kafka-manager\",\"-Dconfig.file=conf/application.conf\"] EOF 2 制作docker镜像 cd /data/dockerfile/kafka-manager docker build . -t harbor.od.com/infra/kafka-manager:v2.0.0.2 (漫长的过程) docker push harbor.zq.com/infra/kafka-manager:latest 构建过程极其漫长,大概率会失败,因此可以通过第二种方式下载构建好的镜像 但构建好的镜像写死了zk地址,要注意传入变量修改zk地址 4.2.2 直接下载docker镜像 镜像下载地址 docker pull sheepkiller/kafka-manager:latest docker images|grep kafka-manager docker tag 4e4a8c5dabab harbor.zq.com/infra/kafka-manager:latest docker push harbor.zq.com/infra/kafka-manager:latest 4.3 部署kafka-manager mkdir /data/k8s-yaml/kafka-manager cd /data/k8s-yaml/kafka-manager ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:4:2","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"4.3.1 准备dp清单 cat \u003edeployment.yaml \u003c\u003c'EOF' kind: Deployment apiVersion: extensions/v1beta1 metadata: name: kafka-manager namespace: infra labels: name: kafka-manager spec: replicas: 1 selector: matchLabels: name: kafka-manager template: metadata: labels: app: kafka-manager name: kafka-manager spec: containers: - name: kafka-manager image: harbor.zq.com/infra/kafka-manager:latest ports: - containerPort: 9000 protocol: TCP env: - name: ZK_HOSTS value: zk1.od.com:2181 - name: APPLICATION_SECRET value: letmein imagePullPolicy: IfNotPresent imagePullSecrets: - name: harbor restartPolicy: Always terminationGracePeriodSeconds: 30 securityContext: runAsUser: 0 schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 revisionHistoryLimit: 7 progressDeadlineSeconds: 600 EOF 4.3.2 准备svc资源清单 cat \u003eservice.yaml \u003c\u003c'EOF' kind: Service apiVersion: v1 metadata: name: kafka-manager namespace: infra spec: ports: - protocol: TCP port: 9000 targetPort: 9000 selector: app: kafka-manager EOF 4.3.3 准备ingress资源清单 cat \u003eingress.yaml \u003c\u003c'EOF' kind: Ingress apiVersion: extensions/v1beta1 metadata: name: kafka-manager namespace: infra spec: rules: - host: km.zq.com http: paths: - path: / backend: serviceName: kafka-manager servicePort: 9000 EOF 4.3.4 应用资源配置清单 任意一台运算节点上： kubectl apply -f http://k8s-yaml.od.com/kafka-manager/deployment.yaml kubectl apply -f http://k8s-yaml.od.com/kafka-manager/service.yaml kubectl apply -f http://k8s-yaml.od.com/kafka-manager/ingress.yaml 4.3.5 解析域名 HDSS7-11.host.com上 ~]# vim /var/named/zq.com.zone km A 10.4.7.10 ~]# systemctl restart named ~]# dig -t A km.od.com @10.4.7.11 +short 10.4.7.10 4.3.6 浏览器访问 http://km.zq.com 添加集群 查看集群信息 ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:4:3","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"5 部署filebeat 官方下载地址 运维主机HDSS7-200.host.com上： ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:5:0","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"5.1 制作docker镜像 mkdir /data/dockerfile/filebeat cd /data/dockerfile/filebeat 5.1.1 准备Dockerfile cat \u003eDockerfile \u003c\u003c'EOF' FROM debian:jessie # 如果更换版本,需在官网下载同版本LINUX64-BIT的sha替换FILEBEAT_SHA1 ENV FILEBEAT_VERSION=7.5.1 \\ FILEBEAT_SHA1=daf1a5e905c415daf68a8192a069f913a1d48e2c79e270da118385ba12a93aaa91bda4953c3402a6f0abf1c177f7bcc916a70bcac41977f69a6566565a8fae9c RUN set -x \u0026\u0026 \\ apt-get update \u0026\u0026 \\ apt-get install -y wget \u0026\u0026 \\ wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-${FILEBEAT_VERSION}-linux-x86_64.tar.gz -O /opt/filebeat.tar.gz \u0026\u0026 \\ cd /opt \u0026\u0026 \\ echo \"${FILEBEAT_SHA1} filebeat.tar.gz\" | sha512sum -c - \u0026\u0026 \\ tar xzvf filebeat.tar.gz \u0026\u0026 \\ cd filebeat-* \u0026\u0026 \\ cp filebeat /bin \u0026\u0026 \\ cd /opt \u0026\u0026 \\ rm -rf filebeat* \u0026\u0026 \\ apt-get purge -y wget \u0026\u0026 \\ apt-get autoremove -y \u0026\u0026 \\ apt-get clean \u0026\u0026 rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* COPY filebeat.yaml /etc/ COPY docker-entrypoint.sh / ENTRYPOINT [\"/bin/bash\",\"/docker-entrypoint.sh\"] EOF 5.1.2 准备filebeat配置文件 cat \u003e/etc/filebeat.yaml \u003c\u003c EOF filebeat.inputs: - type: log fields_under_root: true fields: topic: logm-PROJ_NAME paths: - /logm/*.log - /logm/*/*.log - /logm/*/*/*.log - /logm/*/*/*/*.log - /logm/*/*/*/*/*.log scan_frequency: 120s max_bytes: 10485760 multiline.pattern: 'MULTILINE' multiline.negate: true multiline.match: after multiline.max_lines: 100 - type: log fields_under_root: true fields: topic: logu-PROJ_NAME paths: - /logu/*.log - /logu/*/*.log - /logu/*/*/*.log - /logu/*/*/*/*.log - /logu/*/*/*/*/*.log - /logu/*/*/*/*/*/*.log output.kafka: hosts: [\"10.4.7.11:9092\"] topic: k8s-fb-ENV-%{[topic]} version: 2.0.0 # kafka版本超过2.0，默认写2.0.0 required_acks: 0 max_message_bytes: 10485760 EOF 5.1.3 准备启动脚本 cat \u003edocker-entrypoint.sh \u003c\u003c'EOF' #!/bin/bash ENV=${ENV:-\"test\"} # 定义日志收集的环境 PROJ_NAME=${PROJ_NAME:-\"no-define”} # 定义项目名称 MULTILINE=${MULTILINE:-\"^\\d{2}\"} # 多行匹配，以2个数据开头的为一行，反之 # 替换配置文件中的内容 sed -i 's#PROJ_NAME#${PROJ_NAME}#g' /etc/filebeat.yaml sed -i 's#MULTILINE#${MULTILINE}#g' /etc/filebeat.yaml sed -i 's#ENV#${ENV}#g' /etc/filebeat.yaml if [[ \"$1\" == \"\" ]]; then exec filebeat -c /etc/filebeat.yaml else exec \"$@\" fi EOF 5.1.4 构建镜像 docker build . -t harbor.od.com/infra/filebeat:v7.5.1 docker push harbor.od.com/infra/filebeat:v7.5.1 ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:5:1","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"5.2 以边车模式运行POD 5.2.1 准备资源配置清单 使用dubbo-demo-consumer的镜像,以边车模式运行filebeat ]# vim /data/k8s-yaml/test/dubbo-demo-consumer/deployment.yaml kind: Deployment apiVersion: extensions/v1beta1 metadata: name: dubbo-demo-consumer namespace: test labels: name: dubbo-demo-consumer spec: replicas: 1 selector: matchLabels: name: dubbo-demo-consumer template: metadata: labels: app: dubbo-demo-consumer name: dubbo-demo-consumer annotations: blackbox_path: \"/hello?name=health\" blackbox_port: \"8080\" blackbox_scheme: \"http\" prometheus_io_scrape: \"true\" prometheus_io_port: \"12346\" prometheus_io_path: \"/\" spec: containers: - name: dubbo-demo-consumer image: harbor.zq.com/app/dubbo-tomcat-web:apollo_200513_1808 ports: - containerPort: 8080 protocol: TCP - containerPort: 20880 protocol: TCP env: - name: JAR_BALL value: dubbo-client.jar - name: C_OPTS value: -Denv=fat -Dapollo.meta=http://config-test.zq.com imagePullPolicy: IfNotPresent #--------新增内容-------- volumeMounts: - mountPath: /opt/tomcat/logs name: logm - name: filebeat image: harbor.zq.com/infra/filebeat:v7.5.1 imagePullPolicy: IfNotPresent env: - name: ENV value: test # 测试环境 - name: PROJ_NAME value: dubbo-demo-web # 项目名 volumeMounts: - mountPath: /logm name: logm volumes: - emptyDir: {} #随机在宿主机找目录创建,容器删除时一起删除 name: logm #--------新增结束-------- imagePullSecrets: - name: harbor restartPolicy: Always terminationGracePeriodSeconds: 30 securityContext: runAsUser: 0 schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 revisionHistoryLimit: 7 progressDeadlineSeconds: 600 5.2.2 应用资源清单 任意node节点 kubectl apply -f http://k8s-yaml.od.com/test/dubbo-demo-consumer/deployment.yaml ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:5:2","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"5.2.3 验证 浏览器访问http://km.zq.com,看到kafaka-manager里，topic打进来，即为成功 进入dubbo-demo-consumer的容器中,查看logm目录下是否有日志 kubectl -n test exec -it dobbo...... -c filebeat /bin/bash ls /logm # -c参数指定pod中的filebeat容器 # /logm是filebeat容器挂载的目录 ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:5:3","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"6 部署logstash 运维主机HDSS7-200.host.com上： ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:6:0","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"6.1 准备docker镜像 6.1.1 下载官方镜像 docker pull logstash:6.8.6 docker tag d0a2dac51fcb harbor.od.com/infra/logstash:v6.8.6 docker push harbor.zq.com/infra/logstash:v6.8.6 6.1.2 准备配置文件 准备目录 mkdir /etc/logstash/ 创建test.conf cat \u003e/etc/logstash/logstash-test.conf \u003c\u003c'EOF' input { kafka { bootstrap_servers =\u003e \"10.4.7.11:9092\" client_id =\u003e \"10.4.7.200\" consumer_threads =\u003e 4 group_id =\u003e \"k8s_test\" # 为test组 topics_pattern =\u003e \"k8s-fb-test-.*\" # 只收集k8s-fb-test开头的topics } } filter { json { source =\u003e \"message\" } } output { elasticsearch { hosts =\u003e [\"10.4.7.12:9200\"] index =\u003e \"k8s-test-%{+YYYY.MM.DD}\" } } EOF 创建prod.conf cat \u003e/etc/logstash/logstash-prod.conf \u003c\u003c'EOF' input { kafka { bootstrap_servers =\u003e \"10.4.7.11:9092\" client_id =\u003e \"10.4.7.200\" consumer_threads =\u003e 4 group_id =\u003e \"k8s_prod\" topics_pattern =\u003e \"k8s-fb-prod-.*\" } } filter { json { source =\u003e \"message\" } } output { elasticsearch { hosts =\u003e [\"10.4.7.12:9200\"] index =\u003e “k8s-prod-%{+YYYY.MM.DD}\" } } EOF ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:6:1","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"6.2 启动logstash 6.2.1 启动测试环境的logstash docker run -d \\ --restart=always \\ --name logstash-test \\ -v /etc/logstash:/etc/logstash \\ -f /etc/logstash/logstash-test.conf \\ harbor.od.com/infra/logstash:v6.8.6 ~]# docker ps -a|grep logstash 6.2.2 查看es是否接收数据 ~]# curl http://10.4.7.12:9200/_cat/indices?v health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open k8s-test-2020.01.07 mFEQUyKVTTal8c97VsmZHw 5 0 12 0 78.4kb 78.4kb 6.2.3 启动正式环境的logstash docker run -d \\ --restart=always \\ --name logstash-prod \\ -v /etc/logstash:/etc/logstash \\ -f /etc/logstash/logstash-prod.conf \\ harbor.od.com/infra/logstash:v6.8.6 ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:6:2","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"7 部署Kibana 运维主机HDSS7-200.host.com上： ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:7:0","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"7.1 准备相关资源 7.1.1 准备docker镜像 kibana官方镜像下载地址 docker pull kibana:6.8.6 docker tag adfab5632ef4 harbor.od.com/infra/kibana:v6.8.6 docker push harbor.zq.com/infra/kibana:v6.8.6 准备目录 mkdir /data/k8s-yaml/kibana cd /data/k8s-yaml/kibana 7.1.3 准备dp资源清单 cat \u003edeployment.yaml \u003c\u003c'EOF' kind: Deployment apiVersion: extensions/v1beta1 metadata: name: kibana namespace: infra labels: name: kibana spec: replicas: 1 selector: matchLabels: name: kibana template: metadata: labels: app: kibana name: kibana spec: containers: - name: kibana image: harbor.zq.com/infra/kibana:v6.8.6 imagePullPolicy: IfNotPresent ports: - containerPort: 5601 protocol: TCP env: - name: ELASTICSEARCH_URL value: http://10.4.7.12:9200 imagePullSecrets: - name: harbor securityContext: runAsUser: 0 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 revisionHistoryLimit: 7 progressDeadlineSeconds: 600 EOF 7.1.4 准备svc资源清单 cat \u003eservice.yaml \u003c\u003c'EOF' kind: Service apiVersion: v1 metadata: name: kibana namespace: infra spec: ports: - protocol: TCP port: 5601 targetPort: 5601 selector: app: kibana EOF 7.1.5 准备ingress资源清单 cat \u003eingress.yaml \u003c\u003c'EOF' kind: Ingress apiVersion: extensions/v1beta1 metadata: name: kibana namespace: infra spec: rules: - host: kibana.zq.com http: paths: - path: / backend: serviceName: kibana servicePort: 5601 EOF ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:7:1","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"7.2 应用资源 7.2.1 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/kibana/deployment.yaml kubectl apply -f http://k8s-yaml.zq.com/kibana/service.yaml kubectl apply -f http://k8s-yaml.zq.com/kibana/ingress.yaml 7.2.2 解析域名 ~]# vim /var/named/od.com.zone kibana A 10.4.7.10 ~]# systemctl restart named ~]# dig -t A kibana.od.com @10.4.7.11 +short 10.4.7.10 7.2.3 浏览器访问 访问http://kibana.zq.com ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:7:2","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"7.3 kibana的使用 选择区域 项目 用途 @timestamp 对应日志的时间戳 og.file.path 对应日志文件名 message 对应日志内容 时间选择器 选择日志时间 快速时间 绝对时间 相对时间 环境选择器 选择对应环境的日志 k8s-test-* k8s-prod-* 项目选择器 对应filebeat的PROJ_NAME值 Add a fillter topic is ${PROJ_NAME} dubbo-demo-service dubbo-demo-web 关键字选择器 exception error ","date":"2020-10-01","objectID":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/:7:3","tags":["K8S","转载"],"title":"16_K8S_监控实战-ELK收集K8S内应用日志","uri":"/16_k8s_%E7%9B%91%E6%8E%A7%E5%AE%9E%E6%88%98-elk%E6%94%B6%E9%9B%86k8s%E5%86%85%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"},{"categories":["转载","K8S"],"content":"转载，原为老男孩教育视频内容","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"17_K8S_集成实战-使用spinnaker进行自动化部署 ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:0:0","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"1 spinnaker概述和选型 ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:1:0","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"1.1 概述 1.1.1 主要功能 Spinnaker是一个开源的多云持续交付平台，提供快速、可靠、稳定的软件变更服务。主要包含两类功能：集群管理和部署管理 1.1.2 集群管理 集群管理主要用于管理云资源，Spinnaker所说的”云“可以理解成AWS，即主要是laaS的资源，比如OpenStak，Google云，微软云等，后来还支持了容器与Kubernetes，但是管理方式还是按照管理基础设施的模式来设计的。 1.1.3 部署管理 管理部署流程是Spinnaker的核心功能，使用minio作为持久化层，同时对接jenkins流水线创建的镜像，部署到Kubernetes集群中去，让服务真正运行起来。 1.1.4 逻辑架构图 Spinnaker自己就是Spinnake一个微服务,由若干组件组成，整套逻辑架构图如下： Deck是基于浏览器的UI。 Gate是API网关。 Spinnaker UI和所有api调用程序都通过Gate与Spinnaker进行通信。 Clouddriver负责管理云平台，并为所有部署的资源编制索引/缓存。 Front50用于管理数据持久化，用于保存应用程序，管道，项目和通知的元数据。 Igor用于通过Jenkins和Travis CI等系统中的持续集成作业来触发管道，并且它允许在管道中使用Jenkins / Travis阶段。 Orca是编排引擎。它处理所有临时操作和流水线。 Rosco是管理调度虚拟机。 Kayenta为Spinnaker提供自动化的金丝雀分析。 Fiat 是Spinnaker的认证服务。 Echo是信息通信服务。 它支持发送通知（例如，Slack，电子邮件，SMS），并处理来自Github之类的服务中传入的Webhook。 ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:1:1","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"1.2 部署选型 Spinnaker官网 Spinnaker包含组件众多,部署相对复杂,因此官方提供的脚手架工具halyard,但是可惜里面涉及的部分镜像地址被墙 Armory发行版 基于Spinnaker,众多公司开发了开发第三方发行版来简化Spinnaker的部署工作,例如我们要用的Armory发行版 Armory也有自己的脚手架工具,虽然相对halyard更简化了,但仍然部分被墙 因此我们部署的方式是手动交付Spinnaker的Armory发行版 ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:1:2","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"2 部署spinnaker第一部分 ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:2:0","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"2.1 spinnaker之minio部署 2.1.1 准备minio镜像 docker pull minio/minio:latest docker tag 533fee13ab07 harbor.zq.com/armory/minio:latest docker push harbor.od.com/armory/minio:latest 准备目录 mkdir -p /data/nfs-volume/minio mkdir -p /data/k8s-yaml/armory/minio cd /data/k8s-yaml/armory/minio 2.1.2 准备dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' kind: Deployment apiVersion: apps/v1 kind: Deployment metadata: labels: name: minio name: minio namespace: armory spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 7 selector: matchLabels: name: minio template: metadata: labels: app: minio name: minio spec: containers: - name: minio image: harbor.zq.com/armory/minio:latest imagePullPolicy: IfNotPresent ports: - containerPort: 9000 protocol: TCP args: - server - /data env: - name: MINIO_ACCESS_KEY value: admin - name: MINIO_SECRET_KEY value: admin123 readinessProbe: failureThreshold: 3 httpGet: path: /minio/health/ready port: 9000 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 volumeMounts: - mountPath: /data name: data imagePullSecrets: - name: harbor volumes: - nfs: server: ops-200.host.com path: /data/nfs-volume/minio name: data EOF 2.1.3 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: minio namespace: armory spec: ports: - port: 80 protocol: TCP targetPort: 9000 selector: app: minio EOF 2.1.4 准备ingress资源清单 cat \u003eingress.yaml \u003c\u003c'EOF' kind: Ingress apiVersion: extensions/v1beta1 metadata: name: minio namespace: armory spec: rules: - host: minio.zq.com http: paths: - path: / backend: serviceName: minio servicePort: 80 EOF 2.1.5 应用资源配置清单 任意node节点 创建namespace和secret kubectl create namespace armory kubectl create secret docker-registry harbor \\ --docker-server=harbor.zq.com \\ --docker-username=admin \\ --docker-password=Harbor12345 \\ -n armory 应用清单 kubectl apply -f http://k8s-yaml.zq.com/armory/minio/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/armory/minio/svc.yaml kubectl apply -f http://k8s-yaml.zq.com/armory/minio/ingress.yaml 2.1.6 访问验证 访问http://minio.zq.com,用户名密码为:admin/admin123 如果访问并登陆成功,表示minio部署成功 ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:2:1","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"2.2 spinnaker之redis部署 2.2.1 准备镜像好目录 docker pull redis:4.0.14 docker tag 6e221e67453d harbor.zq.com/armory/redis:v4.0.14 docker push harbor.od.com/armory/redis:v4.0.14 准备目录 mkdir -p /data/k8s-yaml/armory/redis cd /data/k8s-yaml/armory/redis 2.2.2 准备dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' kind: Deployment apiVersion: apps:v1 metadata: labels: name: redis name: redis namespace: armory spec: replicas: 1 revisionHistoryLimit: 7 selector: matchLabels: name: redis template: metadata: labels: app: redis name: redis spec: containers: - name: redis image: harbor.zq.com/armory/redis:v4.0.14 imagePullPolicy: IfNotPresent ports: - containerPort: 6379 protocol: TCP imagePullSecrets: - name: harbor EOF 2.2.3 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: redis namespace: armory spec: ports: - port: 6379 protocol: TCP targetPort: 6379 selector: app: redis EOF 2.3.4 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/armory/redis/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/armory/redis/svc.yaml ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:2:2","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"3 部署spinnaker之CloudDriver CloudDriver是整套spinnaker部署中最难的部分,因此单独写一章来说明 ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:3:0","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"3.1 部署准备工作 3.1.1 准备镜像和目录 docker pull armory/spinnaker-clouddriver-slim:release-1.11.x-bee52673a docker tag f1d52d01e28d harbor.od.com/armory/clouddriver:v1.11.x docker push harbor.od.com/armory/clouddriver:v1.11.x 准备目录 mkdir /data/k8s-yaml/armory/clouddriver cd /data/k8s-yaml/armory/clouddriver 3.1.2 准备minio的secret 准备配置文件 cat \u003ecredentials \u003c\u003c'EOF' [default] aws_access_key_id=admin aws_secret_access_key=admin123 EOF NODE节点创建secret wget http://k8s-yaml.od.com/armory/clouddriver/credentials kubectl create secret generic credentials \\ --from-file=./credentials \\ -n armory # 也可以不急于配置文件,直接命令行创建 kubectl create secret generic credentials \\ --aws_access_key_id=admin \\ --aws_secret_access_key=admin123 \\ -n armory 3.1.3 签发证书与私钥 cd /opt/certs cp client-csr.json admin-csr.json sed -i 's##cluster-admin#g' admin-csr.json cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=client \\ admin-csr.json |cfssl-json -bare admin ls admin* 3.1.3 分发证书 在任意node节点 cd /opt/certs scp hdss7-200:/opt/certs/ca.pem . scp hdss7-200:/opt/certs/admin.pem . scp hdss7-200:/opt/certs/admin-key.pem . 3.1.4 创建用户 # 4步法创建用户 kubectl config set-cluster myk8s \\ --certificate-authority=./ca.pem \\ --embed-certs=true --server=https://192.168.1.10:7443 \\ --kubeconfig=config kubectl config set-credentials cluster-admin \\ --client-certificate=./admin.pem \\ --client-key=./admin-key.pem \\ --embed-certs=true --kubeconfig=config kubectl config set-context myk8s-context \\ --cluster=myk8s \\ --user=cluster-admin \\ --kubeconfig=config kubectl config use-context myk8s-context \\ --kubeconfig=config # 集群角色绑定 kubectl create clusterrolebinding myk8s-admin \\ --clusterrole=cluster-admin \\ --user=cluster-admin 3.1.5 使用config创建cm资源 cp config default-kubeconfig kubectl create cm default-kubeconfig --from-file=default-kubeconfig -n armory ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:3:1","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"3.2 创建并应用资源清单 回到7.200管理机 cd /data/k8s-yaml/armory/clouddriver 3.2.1 创建环境变量配置 cat \u003evim init-env.yaml \u003c\u003c'EOF' kind: ConfigMap apiVersion: v1 metadata: name: init-env namespace: armory data: API_HOST: http://spinnaker.zq.com/api ARMORY_ID: c02f0781-92f5-4e80-86db-0ba8fe7b8544 ARMORYSPINNAKER_CONF_STORE_BUCKET: armory-platform ARMORYSPINNAKER_CONF_STORE_PREFIX: front50 ARMORYSPINNAKER_GCS_ENABLED: \"false\" ARMORYSPINNAKER_S3_ENABLED: \"true\" AUTH_ENABLED: \"false\" AWS_REGION: us-east-1 BASE_IP: 127.0.0.1 CLOUDDRIVER_OPTS: -Dspring.profiles.active=armory,configurator,local CONFIGURATOR_ENABLED: \"false\" DECK_HOST: http://spinnaker.zq.com ECHO_OPTS: -Dspring.profiles.active=armory,configurator,local GATE_OPTS: -Dspring.profiles.active=armory,configurator,local IGOR_OPTS: -Dspring.profiles.active=armory,configurator,local PLATFORM_ARCHITECTURE: k8s REDIS_HOST: redis://redis:6379 SERVER_ADDRESS: 0.0.0.0 SPINNAKER_AWS_DEFAULT_REGION: us-east-1 SPINNAKER_AWS_ENABLED: \"false\" SPINNAKER_CONFIG_DIR: /home/spinnaker/config SPINNAKER_GOOGLE_PROJECT_CREDENTIALS_PATH: \"\" SPINNAKER_HOME: /home/spinnaker SPRING_PROFILES_ACTIVE: armory,configurator,local EOF 3.2.2 创建组件配置文件 cat \u003ecustom-config.yaml \u003c\u003c'EOF' kind: ConfigMap apiVersion: v1 metadata: name: custom-config namespace: armory data: clouddriver-local.yml: | kubernetes: enabled: true accounts: - name: cluster-admin serviceAccount: false dockerRegistries: - accountName: harbor namespace: [] namespaces: - test - prod kubeconfigFile: /opt/spinnaker/credentials/custom/default-kubeconfig primaryAccount: cluster-admin dockerRegistry: enabled: true accounts: - name: harbor requiredGroupMembership: [] providerVersion: V1 insecureRegistry: true address: http://harbor.zq.com username: admin password: Harbor12345 primaryAccount: harbor artifacts: s3: enabled: true accounts: - name: armory-config-s3-account apiEndpoint: http://minio apiRegion: us-east-1 gcs: enabled: false accounts: - name: armory-config-gcs-account custom-config.json: \"\" echo-configurator.yml: | diagnostics: enabled: true front50-local.yml: | spinnaker: s3: endpoint: http://minio igor-local.yml: | jenkins: enabled: true masters: - name: jenkins-admin address: http://jenkins.zq.com username: admin password: admin123 primaryAccount: jenkins-admin nginx.conf: | gzip on; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript application/vnd.ms-fontobject application/x-font-ttf font/opentype image/svg+xml image/x-icon; server { listen 80; location / { proxy_pass http://armory-deck/; } location /api/ { proxy_pass http://armory-gate:8084/; } rewrite ^/login(.*)$ /api/login$1 last; rewrite ^/auth(.*)$ /api/auth$1 last; } spinnaker-local.yml: | services: igor: enabled: true EOF 3.2.3 创建默认配置文件 注意: 此配置文件超长,是用armory部署工具部署好后,基本不需要改动 cat \u003edefault-config.yaml \u003c\u003c'EOF' kind: ConfigMap apiVersion: v1 metadata: name: default-config namespace: armory data: barometer.yml: | server: port: 9092 spinnaker: redis: host: ${services.redis.host} port: ${services.redis.port} clouddriver-armory.yml: | aws: defaultAssumeRole: role/${SPINNAKER_AWS_DEFAULT_ASSUME_ROLE:SpinnakerManagedProfile} accounts: - name: default-aws-account accountId: ${SPINNAKER_AWS_DEFAULT_ACCOUNT_ID:none} client: maxErrorRetry: 20 serviceLimits: cloudProviderOverrides: aws: rateLimit: 15.0 implementationLimits: AmazonAutoScaling: defaults: rateLimit: 3.0 AmazonElasticLoadBalancing: defaults: rateLimit: 5.0 security.basic.enabled: false management.security.enabled: false clouddriver-dev.yml: | serviceLimits: defaults: rateLimit: 2 clouddriver.yml: | server: port: ${services.clouddriver.port:7002} address: ${services.clouddriver.host:localhost} redis: connection: ${REDIS_HOST:redis://localhost:6379} udf: enabled: ${services.clouddriver.aws.udf.enabled:true} udfRoot: /opt/spinnaker/config/udf defaultLegacyUdf: false default: account: env: ${providers.aws.primaryCredentials.name} aws: enabled: ${providers.aws.enabled:false}","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:3:2","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"4 部署spinnaker第三部分 ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:4:0","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"4.1 spinnaker之front50部署 mkdir /data/k8s-yaml/armory/front50 cd /data/k8s-yaml/armory/front50 4.1.1 准备镜像 docker pull armory/spinnaker-front50-slim:release-1.8.x-93febf2 docker tag 0d353788f4f2 harbor.zq.com/armory/front50:v1.8.x docker push harbor.zq.com/armory/front50:v1.8.x 4.1.2 准备dp资源清单 cat \u003cdp.yaml \u003c\u003c'EOF' apiVersion: apps/v1 kind: Deployment metadata: labels: app: armory-front50 name: armory-front50 namespace: armory spec: replicas: 1 revisionHistoryLimit: 7 selector: matchLabels: app: armory-front50 template: metadata: annotations: artifact.spinnaker.io/location: '\"armory\"' artifact.spinnaker.io/name: '\"armory-front50\"' artifact.spinnaker.io/type: '\"kubernetes/deployment\"' moniker.spinnaker.io/application: '\"armory\"' moniker.spinnaker.io/cluster: '\"front50\"' labels: app: armory-front50 spec: containers: - name: armory-front50 image: harbor.od.com/armory/front50:v1.8.x imagePullPolicy: IfNotPresent command: - bash - -c args: - bash /opt/spinnaker/config/default/fetch.sh \u0026\u0026 cd /home/spinnaker/config \u0026\u0026 /opt/front50/bin/front50 ports: - containerPort: 8080 protocol: TCP env: - name: JAVA_OPTS value: -javaagent:/opt/front50/lib/jamm-0.2.5.jar -Xmx1000M envFrom: - configMapRef: name: init-env livenessProbe: failureThreshold: 3 httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 600 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 180 periodSeconds: 5 successThreshold: 8 timeoutSeconds: 1 volumeMounts: - mountPath: /etc/podinfo name: podinfo - mountPath: /home/spinnaker/.aws name: credentials - mountPath: /opt/spinnaker/config/default name: default-config - mountPath: /opt/spinnaker/config/custom name: custom-config imagePullSecrets: - name: harbor volumes: - configMap: defaultMode: 420 name: custom-config name: custom-config - configMap: defaultMode: 420 name: default-config name: default-config - name: credentials secret: defaultMode: 420 secretName: credentials - downwardAPI: defaultMode: 420 items: - fieldRef: apiVersion: v1 fieldPath: metadata.labels path: labels - fieldRef: apiVersion: v1 fieldPath: metadata.annotations path: annotations name: podinfo EOF 4.1.3 创建svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: armory-front50 namespace: armory spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: armory-front50 EOF 4.1.4 应用资源清单 kubectl apply -f http://k8s-yaml.zq.com/armory/front50/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/armory/front50/svc.yaml 验证 ~]# docker ps -qa|grep minio b71a5af3c57e ~]# docker exec -it b71a5af3c57e sh / # curl armory-front50:8080/health {\"status\":\"UP\"} ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:4:1","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"4.2 spinnaker之orca部署 mkdir /data/k8s-yaml/armory/orca cd /data/k8s-yaml/armory/orca 4.2.1 准备docker镜像 docker pull docker.io/armory/spinnaker-orca-slim:release-1.8.x-de4ab55 docker tag 5103b1f73e04 harbor.zq.com/armory/orca:v1.8.x docker push harbor.zq.com/armory/orca:v1.8.x 4.2.2 准备dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' apiVersion: apps/v1 kind: Deployment metadata: labels: app: armory-orca name: armory-orca namespace: armory spec: replicas: 1 revisionHistoryLimit: 7 selector: matchLabels: app: armory-orca template: metadata: annotations: artifact.spinnaker.io/location: '\"armory\"' artifact.spinnaker.io/name: '\"armory-orca\"' artifact.spinnaker.io/type: '\"kubernetes/deployment\"' moniker.spinnaker.io/application: '\"armory\"' moniker.spinnaker.io/cluster: '\"orca\"' labels: app: armory-orca spec: containers: - name: armory-orca image: harbor.od.com/armory/orca:v1.8.x imagePullPolicy: IfNotPresent command: - bash - -c args: - bash /opt/spinnaker/config/default/fetch.sh \u0026\u0026 cd /home/spinnaker/config \u0026\u0026 /opt/orca/bin/orca ports: - containerPort: 8083 protocol: TCP env: - name: JAVA_OPTS value: -Xmx1000M envFrom: - configMapRef: name: init-env livenessProbe: failureThreshold: 5 httpGet: path: /health port: 8083 scheme: HTTP initialDelaySeconds: 600 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /health port: 8083 scheme: HTTP initialDelaySeconds: 180 periodSeconds: 3 successThreshold: 5 timeoutSeconds: 1 volumeMounts: - mountPath: /etc/podinfo name: podinfo - mountPath: /opt/spinnaker/config/default name: default-config - mountPath: /opt/spinnaker/config/custom name: custom-config imagePullSecrets: - name: harbor volumes: - configMap: defaultMode: 420 name: custom-config name: custom-config - configMap: defaultMode: 420 name: default-config name: default-config - downwardAPI: defaultMode: 420 items: - fieldRef: apiVersion: v1 fieldPath: metadata.labels path: labels - fieldRef: apiVersion: v1 fieldPath: metadata.annotations path: annotations name: podinfo EOF 4.2.3 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: armory-orca namespace: armory spec: ports: - port: 8083 protocol: TCP targetPort: 8083 selector: app: armory-orca EOF 4.2.4 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/armory/orca/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/armory/orca/svc.yaml 检查 ~]# docker exec -it b71a5af3c57e sh / # curl armory-orca:8083/health {\"status\":\"UP\"} ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:4:2","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"4.3 spinnaker之echo部署 mkdir /data/k8s-yaml/armory/echo cd /data/k8s-yaml/armory/echo 4.3.1 准备docker镜像 docker pull docker.io/armory/echo-armory:c36d576-release-1.8.x-617c567 docker tag 415efd46f474 harbor.od.com/armory/echo:v1.8.x docker push harbor.od.com/armory/echo:v1.8.x 4.3.2 准备dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' apiVersion: apps/v1 kind: Deployment metadata: labels: app: armory-echo name: armory-echo namespace: armory spec: replicas: 1 revisionHistoryLimit: 7 selector: matchLabels: app: armory-echo template: metadata: annotations: artifact.spinnaker.io/location: '\"armory\"' artifact.spinnaker.io/name: '\"armory-echo\"' artifact.spinnaker.io/type: '\"kubernetes/deployment\"' moniker.spinnaker.io/application: '\"armory\"' moniker.spinnaker.io/cluster: '\"echo\"' labels: app: armory-echo spec: containers: - name: armory-echo image: harbor.od.com/armory/echo:v1.8.x imagePullPolicy: IfNotPresent command: - bash - -c args: - bash /opt/spinnaker/config/default/fetch.sh \u0026\u0026 cd /home/spinnaker/config \u0026\u0026 /opt/echo/bin/echo ports: - containerPort: 8089 protocol: TCP env: - name: JAVA_OPTS value: -javaagent:/opt/echo/lib/jamm-0.2.5.jar -Xmx1000M envFrom: - configMapRef: name: init-env livenessProbe: failureThreshold: 3 httpGet: path: /health port: 8089 scheme: HTTP initialDelaySeconds: 600 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /health port: 8089 scheme: HTTP initialDelaySeconds: 180 periodSeconds: 3 successThreshold: 5 timeoutSeconds: 1 volumeMounts: - mountPath: /etc/podinfo name: podinfo - mountPath: /opt/spinnaker/config/default name: default-config - mountPath: /opt/spinnaker/config/custom name: custom-config imagePullSecrets: - name: harbor volumes: - configMap: defaultMode: 420 name: custom-config name: custom-config - configMap: defaultMode: 420 name: default-config name: default-config - downwardAPI: defaultMode: 420 items: - fieldRef: apiVersion: v1 fieldPath: metadata.labels path: labels - fieldRef: apiVersion: v1 fieldPath: metadata.annotations path: annotations name: podinfo EOF 4.3.3 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: armory-echo namespace: armory spec: ports: - port: 8089 protocol: TCP targetPort: 8089 selector: app: armory-echo EOF 4.3.4 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/armory/echo/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/armory/echo/svc.yaml 检查 ~]# docker exec -it b71a5af3c57e sh / # curl armory-echo:8089/health {\"status\":\"UP\"} ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:4:3","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"4.4 spinnaker之igor部署 mkdir /data/k8s-yaml/armory/igor cd /data/k8s-yaml/armory/igor 4.4.1 准备docker镜像 docker pull docker.io/armory/spinnaker-igor-slim:release-1.8-x-new-install-healthy-ae2b329 docker tag 23984f5b43f6 harbor.zq.com/armory/igor:v1.8.x docker push harbor.zq.com/armory/igor:v1.8.x 4.4.2 准备dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' apiVersion: apps/v1 kind: Deployment metadata: labels: app: armory-igor name: armory-igor namespace: armory spec: replicas: 1 revisionHistoryLimit: 7 selector: matchLabels: app: armory-igor template: metadata: annotations: artifact.spinnaker.io/location: '\"armory\"' artifact.spinnaker.io/name: '\"armory-igor\"' artifact.spinnaker.io/type: '\"kubernetes/deployment\"' moniker.spinnaker.io/application: '\"armory\"' moniker.spinnaker.io/cluster: '\"igor\"' labels: app: armory-igor spec: containers: - name: armory-igor image: harbor.od.com/armory/igor:v1.8.x imagePullPolicy: IfNotPresent command: - bash - -c args: - bash /opt/spinnaker/config/default/fetch.sh \u0026\u0026 cd /home/spinnaker/config \u0026\u0026 /opt/igor/bin/igor ports: - containerPort: 8088 protocol: TCP env: - name: IGOR_PORT_MAPPING value: -8088:8088 - name: JAVA_OPTS value: -Xmx1000M envFrom: - configMapRef: name: init-env livenessProbe: failureThreshold: 3 httpGet: path: /health port: 8088 scheme: HTTP initialDelaySeconds: 600 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /health port: 8088 scheme: HTTP initialDelaySeconds: 180 periodSeconds: 5 successThreshold: 5 timeoutSeconds: 1 volumeMounts: - mountPath: /etc/podinfo name: podinfo - mountPath: /opt/spinnaker/config/default name: default-config - mountPath: /opt/spinnaker/config/custom name: custom-config imagePullSecrets: - name: harbor securityContext: runAsUser: 0 volumes: - configMap: defaultMode: 420 name: custom-config name: custom-config - configMap: defaultMode: 420 name: default-config name: default-config - downwardAPI: defaultMode: 420 items: - fieldRef: apiVersion: v1 fieldPath: metadata.labels path: labels - fieldRef: apiVersion: v1 fieldPath: metadata.annotations path: annotations name: podinfo EOF 4.4.3 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: armory-igor namespace: armory spec: ports: - port: 8088 protocol: TCP targetPort: 8088 selector: app: armory-igor EOF 4.4.4 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/armory/igor/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/armory/igor/svc.yaml 检查 ~]# docker exec -it b71a5af3c57e sh / # curl armory-igor:8088/health {\"status\":\"UP\"} ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:4:4","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"4.5 spinnaker之gate部署 mkdir /data/k8s-yaml/armory/gate cd /data/k8s-yaml/armory/gate 4.5.1 准备docker镜像 docker pull docker.io/armory/gate-armory:dfafe73-release-1.8.x-5d505ca docker tag b092d4665301 harbor.zq.com/armory/gate:v1.8.x docker push harbor.zq.com/armory/gate:v1.8.x 4.5.2 准备dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' apiVersion: apps/v1 kind: Deployment metadata: labels: app: armory-gate name: armory-gate namespace: armory spec: replicas: 1 revisionHistoryLimit: 7 selector: matchLabels: app: armory-gate template: metadata: annotations: artifact.spinnaker.io/location: '\"armory\"' artifact.spinnaker.io/name: '\"armory-gate\"' artifact.spinnaker.io/type: '\"kubernetes/deployment\"' moniker.spinnaker.io/application: '\"armory\"' moniker.spinnaker.io/cluster: '\"gate\"' labels: app: armory-gate spec: containers: - name: armory-gate image: harbor.od.com/armory/gate:v1.8.x imagePullPolicy: IfNotPresent command: - bash - -c args: - bash /opt/spinnaker/config/default/fetch.sh gate \u0026\u0026 cd /home/spinnaker/config \u0026\u0026 /opt/gate/bin/gate ports: - containerPort: 8084 name: gate-port protocol: TCP - containerPort: 8085 name: gate-api-port protocol: TCP env: - name: GATE_PORT_MAPPING value: -8084:8084 - name: GATE_API_PORT_MAPPING value: -8085:8085 - name: JAVA_OPTS value: -Xmx1000M envFrom: - configMapRef: name: init-env livenessProbe: exec: command: - /bin/bash - -c - wget -O - http://localhost:8084/health || wget -O - https://localhost:8084/health failureThreshold: 5 initialDelaySeconds: 600 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 readinessProbe: exec: command: - /bin/bash - -c - wget -O - http://localhost:8084/health?checkDownstreamServices=true\u0026downstreamServices=true || wget -O - https://localhost:8084/health?checkDownstreamServices=true\u0026downstreamServices=true failureThreshold: 3 initialDelaySeconds: 180 periodSeconds: 5 successThreshold: 10 timeoutSeconds: 1 volumeMounts: - mountPath: /etc/podinfo name: podinfo - mountPath: /opt/spinnaker/config/default name: default-config - mountPath: /opt/spinnaker/config/custom name: custom-config imagePullSecrets: - name: harbor securityContext: runAsUser: 0 volumes: - configMap: defaultMode: 420 name: custom-config name: custom-config - configMap: defaultMode: 420 name: default-config name: default-config - downwardAPI: defaultMode: 420 items: - fieldRef: apiVersion: v1 fieldPath: metadata.labels path: labels - fieldRef: apiVersion: v1 fieldPath: metadata.annotations path: annotations name: podinfo EOF 4.5.3 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: armory-gate namespace: armory spec: ports: - name: gate-port port: 8084 protocol: TCP targetPort: 8084 - name: gate-api-port port: 8085 protocol: TCP targetPort: 8085 selector: app: armory-gate EOF 4.5.4 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/armory/gate/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/armory/gate/svc.yaml 检查 bin]# docker exec -it b71a5af3c57e sh / # curl armory-gate:8084/health {\"status\":\"UP\"} ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:4:5","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"4.6 spinnaker之deck部署 mkdir /data/k8s-yaml/armory/deck cd /data/k8s-yaml/armory/deck 4.6.1 准备docker镜像 docker pull docker.io/armory/deck-armory:d4bf0cf-release-1.8.x-0a33f94 docker tag 9a87ba3b319f harbor.od.com/armory/deck:v1.8.x docker push harbor.od.com/armory/deck:v1.8.x 4.6.2 准备dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' apiVersion: apps/v1 kind: Deployment metadata: labels: app: armory-deck name: armory-deck namespace: armory spec: replicas: 1 revisionHistoryLimit: 7 selector: matchLabels: app: armory-deck template: metadata: annotations: artifact.spinnaker.io/location: '\"armory\"' artifact.spinnaker.io/name: '\"armory-deck\"' artifact.spinnaker.io/type: '\"kubernetes/deployment\"' moniker.spinnaker.io/application: '\"armory\"' moniker.spinnaker.io/cluster: '\"deck\"' labels: app: armory-deck spec: containers: - name: armory-deck image: harbor.od.com/armory/deck:v1.8.x imagePullPolicy: IfNotPresent command: - bash - -c args: - bash /opt/spinnaker/config/default/fetch.sh \u0026\u0026 /entrypoint.sh ports: - containerPort: 9000 protocol: TCP envFrom: - configMapRef: name: init-env livenessProbe: failureThreshold: 3 httpGet: path: / port: 9000 scheme: HTTP initialDelaySeconds: 180 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 5 httpGet: path: / port: 9000 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 3 successThreshold: 5 timeoutSeconds: 1 volumeMounts: - mountPath: /etc/podinfo name: podinfo - mountPath: /opt/spinnaker/config/default name: default-config - mountPath: /opt/spinnaker/config/custom name: custom-config imagePullSecrets: - name: harbor volumes: - configMap: defaultMode: 420 name: custom-config name: custom-config - configMap: defaultMode: 420 name: default-config name: default-config - downwardAPI: defaultMode: 420 items: - fieldRef: apiVersion: v1 fieldPath: metadata.labels path: labels - fieldRef: apiVersion: v1 fieldPath: metadata.annotations path: annotations name: podinfo EOF 4.6.3 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: armory-deck namespace: armory spec: ports: - port: 80 protocol: TCP targetPort: 9000 selector: app: armory-deck EOF 4.6.4 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/armory/deck/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/armory/deck/svc.yaml 检查 ~]# docker exec -it b71a5af3c57e sh / # curl armory-igor:8088/health {\"status\":\"UP\"} ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:4:6","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["转载","K8S"],"content":"4.7 spinnaker之nginx部署 mkdir /data/k8s-yaml/armory/nginx cd /data/k8s-yaml/armory/nginx 4.7.1 准备docker镜像 docker pull nginx:1.12.2 docker tag 4037a5562b03 harbor.od.com/armory/nginx:v1.12.2 docker push harbor.od.com/armory/nginx:v1.12.2 4.7.2 准备dp资源清单 cat \u003edp.yaml \u003c\u003c'EOF' apiVersion: apps/v1 kind: Deployment metadata: labels: app: armory-nginx name: armory-nginx namespace: armory spec: replicas: 1 revisionHistoryLimit: 7 selector: matchLabels: app: armory-nginx template: metadata: annotations: artifact.spinnaker.io/location: '\"armory\"' artifact.spinnaker.io/name: '\"armory-nginx\"' artifact.spinnaker.io/type: '\"kubernetes/deployment\"' moniker.spinnaker.io/application: '\"armory\"' moniker.spinnaker.io/cluster: '\"nginx\"' labels: app: armory-nginx spec: containers: - name: armory-nginx image: harbor.od.com/armory/nginx:v1.12.2 imagePullPolicy: Always command: - bash - -c args: - bash /opt/spinnaker/config/default/fetch.sh nginx \u0026\u0026 nginx -g 'daemon off;' ports: - containerPort: 80 name: http protocol: TCP - containerPort: 443 name: https protocol: TCP - containerPort: 8085 name: api protocol: TCP livenessProbe: failureThreshold: 3 httpGet: path: / port: 80 scheme: HTTP initialDelaySeconds: 180 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: / port: 80 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 3 successThreshold: 5 timeoutSeconds: 1 volumeMounts: - mountPath: /opt/spinnaker/config/default name: default-config - mountPath: /etc/nginx/conf.d name: custom-config imagePullSecrets: - name: harbor volumes: - configMap: defaultMode: 420 name: custom-config name: custom-config - configMap: defaultMode: 420 name: default-config name: default-config EOF 4.6.3 准备svc资源清单 cat \u003esvc.yaml \u003c\u003c'EOF' apiVersion: v1 kind: Service metadata: name: armory-nginx namespace: armory spec: ports: - name: http port: 80 protocol: TCP targetPort: 80 - name: https port: 443 protocol: TCP targetPort: 443 - name: api port: 8085 protocol: TCP targetPort: 8085 selector: app: armory-nginx EOF 4.6.4 准备ingress资源清单 cat \u003eingress.yaml \u003c\u003c'EOF' kind: IngressRoute metadata: labels: app: spinnaker web: spinnaker.od.com name: spinnaker-route namespace: armory spec: entryPoints: - web routes: - match: Host(`spinnaker.od.com`) kind: Rule services: - name: armory-nginx port: 80 EOF 4.6.5 应用资源配置清单 kubectl apply -f http://k8s-yaml.zq.com/armory/nginx/dp.yaml kubectl apply -f http://k8s-yaml.zq.com/armory/nginx/svc.yaml kubectl apply -f http://k8s-yaml.zq.com/armory/nginx/ingress.yaml 检查 ","date":"2020-10-01","objectID":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/:4:7","tags":["K8S","转载"],"title":"17_K8S_集成实战-使用spinnaker进行自动化部署","uri":"/17_k8s_%E9%9B%86%E6%88%90%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8spinnaker%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"categories":["share"],"content":"电视点播及直接分享 迫于目前视频网站平台资源过于分散，需要购买多平台才能看到所有资源。最主要的原因是穷。所以在网上搜索了找到了多种方式可以免费看到最新的视频资源。所有资源都来源于互联网，如有侵权，请联系scemsjyd@163.com删除。 ","date":"2021-01-24","objectID":"/tv-share/:0:0","tags":["tv"],"title":"TV-Share","uri":"/tv-share/"},{"categories":["share"],"content":"1、点播软件 猫影视 设置配置接口地址 接口地址： http://gg.gg/pglblb https://wds.ecsxs.com/212757.json http://tyzx.weetai.cn/mtv.json 今日影视 biubiuTV 密码：auwz ","date":"2021-01-24","objectID":"/tv-share/:0:1","tags":["tv"],"title":"TV-Share","uri":"/tv-share/"},{"categories":["share"],"content":"2、直播软件 超级ITV 无广告，无购物台 火星直播 有购物台 DIYP影音经典版 长按屏幕右侧呼出设置，点击【接口设置】-点击【节目路线】即可开启内置路线。 大视界TV2022免密版 电视家3.0无广告纯净版免登录 电视家极速版纯净版 ","date":"2021-01-24","objectID":"/tv-share/:0:2","tags":["tv"],"title":"TV-Share","uri":"/tv-share/"},{"categories":["share"],"content":"3、手机类 火龙果影视_2.8.9纯净版 大师兄影视 android ios：安装『猴子探站』，搜索：『中华人民共和国万岁』 ","date":"2021-01-24","objectID":"/tv-share/:0:3","tags":["tv"],"title":"TV-Share","uri":"/tv-share/"},{"categories":["share"],"content":"4、在线类 大师兄影视 影视工厂 哔嘀影视 ","date":"2021-01-24","objectID":"/tv-share/:0:4","tags":["tv"],"title":"TV-Share","uri":"/tv-share/"},{"categories":["spring"],"content":"SpringMVC ","date":"2016-10-27","objectID":"/01_springmvc_flow/:0:0","tags":["springmvc"],"title":"01_springmvc_flow","uri":"/01_springmvc_flow/"},{"categories":["spring"],"content":"SpringMVC流程图 ","date":"2016-10-27","objectID":"/01_springmvc_flow/:1:0","tags":["springmvc"],"title":"01_springmvc_flow","uri":"/01_springmvc_flow/"},{"categories":["spring"],"content":"SpringMVC 九大组件 翻看SpringMVC源码会发现 DispatcherServlet 用于处理上传请求。处理方法是将普通的request包装成MultipartHttpServletRequest，后者可以直接调用getFile方法获取File，如果上传多个文件，还可以调用getFileMap得到FileName-\u003eFile结构的Map。此组件中一共有三个方法，作用分别是判断是不是上传请求，将request包装成MultipartHttpServletRequest、处理完后清理上传过程中产生的临时资源。 解析视图需要两个参数：一是视图名，另一个是Locale。视图名是处理器返回的，Locale是从哪里来的？这就是LocaleResolver要做的事情。LocaleResolver用于从request解析出Locale，Locale就是zh-cn之类，表示一个区域，有了这个就可以对不同区域的用户显示不同的结果。SpringMVC主要有两个地方用到了Locale：一是ViewResolver视图解析的时候；二是用到国际化资源或者主题的时候。 用于解析主题。SpringMVC中一个主题对应一个properties文件，里面存放着跟当前主题相关的所有资源、如图片、css样式等。SpringMVC的主题也支持国际化，同一个主题不同区域也可以显示不同的风格。SpringMVC中跟主题相关的类有 ThemeResolver、ThemeSource和Theme。主题是通过一系列资源来具体体现的，要得到一个主题的资源，首先要得到资源的名称，这是ThemeResolver的工作。然后通过主题名称找到对应的主题（可以理解为一个配置）文件，这是ThemeSource的工作。最后从主题中获取资源就可以了。 是用来查找Handler的。在SpringMVC中会有很多请求，每个请求都需要一个Handler处理，具体接收到一个请求之后使用哪个Handler进行处理呢？这就是HandlerMapping需要做的事。 从名字上看，它就是一个适配器。因为SpringMVC中的Handler可以是任意的形式，只要能处理请求就ok，但是Servlet需要的处理方法的结构却是固定的，都是以request和response为参数的方法。如何让固定的Servlet处理方法调用灵活的Handler来进行处理呢？这就是HandlerAdapter要做的事情。 小结：Handler是用来干活的工具；HandlerMapping用于根据需要干的活找到相应的工具；HandlerAdapter是使用工具干活的人。 其它组件都是用来干活的。在干活的过程中难免会出现问题，出问题后怎么办呢？这就需要有一个专门的角色对异常情况进行处理，在SpringMVC中就是HandlerExceptionResolver。具体来说，此组件的作用是根据异常设置ModelAndView，之后再交给render方法进行渲染。 ViewName是根据ViewName查找View，但有的Handler处理完后并没有设置View也没有设置ViewName，这时就需要从request获取ViewName了，如何从request中获取ViewName就是RequestToViewNameTranslator要做的事情了。RequestToViewNameTranslator在Spring MVC容器里只可以配置一个，所以所有request到ViewName的转换规则都要在一个Translator里面全部实现。 ViewResolver用来将String类型的视图名和Locale解析为View类型的视图。View是用来渲染页面的，也就是将程序返回的参数填入模板里，生成html（也可能是其它类型）文件。这里就有两个关键问题：使用哪个模板？用什么技术（规则）填入参数？这其实是ViewResolver主要要做的工作，ViewResolver需要找到渲染所用的模板和所用的技术（也就是视图的类型）进行渲染，具体的渲染过程则交由不同的视图自己完成。 用来管理FlashMap的，FlashMap主要用在redirect中传递参数。 ","date":"2016-10-27","objectID":"/01_springmvc_flow/:2:0","tags":["springmvc"],"title":"01_springmvc_flow","uri":"/01_springmvc_flow/"},{"categories":["spring"],"content":"DispatcherServlet Init dispatcherServlet是springmvc的核心 ","date":"2016-10-28","objectID":"/02_springmvc_init/:0:0","tags":["springmvc"],"title":"02_springmvc_init","uri":"/02_springmvc_init/"},{"categories":["spring"],"content":"servlet的生命周期 ==servlet==的三个重要方法init service destroy 1.被创建：执行init方法，只执行一次 　1.1 Servlet什么时候被创建？ 　1.2 默认情况下，第一次被访问时，Servlet被创建，然后执行init方法； 　1.3 可以配置执行Servlet的创建时机； 2.提供服务：执行service方法，执行多次 3.被销毁：当Servlet服务器正常关闭时，执行destroy方法，只执行一次 ","date":"2016-10-28","objectID":"/02_springmvc_init/:1:0","tags":["springmvc"],"title":"02_springmvc_init","uri":"/02_springmvc_init/"},{"categories":["spring"],"content":"init dispatcherServlet的init做了什么？ 最重要的方法：initServletBean() 到此SpringMVC的初始化基本结束。 总结： 完成上下文springmvc的上下文配置 初始化九大组件的策略配置 ","date":"2016-10-28","objectID":"/02_springmvc_init/:2:0","tags":["springmvc"],"title":"02_springmvc_init","uri":"/02_springmvc_init/"},{"categories":["spring"],"content":"service service是servlet的业务处理核心，此处又做了些什么？ DispatcherServlet的service主要业务处理方法在doDispatch中 ","date":"2016-10-28","objectID":"/02_springmvc_init/:3:0","tags":["springmvc"],"title":"02_springmvc_init","uri":"/02_springmvc_init/"},{"categories":["spring"],"content":"HandlerMapping HandlerMapping接口负责根据request请求找到对应的Handler处理器及Interceptor拦截器，并将它们封装在HandlerExecutionChain对象内，返回给中央调度器。 HandlerMapping接口只有一个方法： @Nullable HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception; 这里我们主要讲清楚两个问题： HandlerMapping初始化 HandlerMapping的唯一方法getHandler ","date":"2016-10-29","objectID":"/03_springmvc_handlermapping/:0:0","tags":["springmvc"],"title":"03_springmvc_handlermapping","uri":"/03_springmvc_handlermapping/"},{"categories":["spring"],"content":"HandlerMapping初始化 我们在第02_DispatcherServlet里已经看到过了Springmvc初始化handlerMapping策略的方法：initHandlerMappings 此处可以看到将HandlerMapping的实现类封装到了handlerMappings属性中。 那HandlerMapping的实现类是在什么时候实例化的，并且里面有哪些东西呢？带着这个疑问我们往下走。 这里我们以RequestMappingHandlerMapping为例： 在非SpringBoot的环境下，我们以前写spring的启动配置dispatchServlet.xml的时候总会加一行 \u003cmvc:annotation-driven/\u003e 这个配置会为我们初始化三个类 RequestMappingHandlerMapping RequestMappingHandlerAdapter ExceptionHandlerExceptionResolver 但是在SpringBoot环境下，配置都是自动化添加的，那我们看一下spring-boot-autoconfigure的spring.factories，再搜索一下web。我们能在其中找到下面这个配置类： org.springframework.boot.autoconfigure.web.servlet.WebMvcAutoConfiguration 这个类中有一个静态类EnableWebMvcConfiguration的目录结构如下： 在其中，我们可以看到有一个方法createRequestMappingHandlerMapping。 ","date":"2016-10-29","objectID":"/03_springmvc_handlermapping/:1:0","tags":["springmvc"],"title":"03_springmvc_handlermapping","uri":"/03_springmvc_handlermapping/"},{"categories":["spring"],"content":"InitializingBean接口 在上面的RequestMappingHandlerMapping类图中，我们看到该类实现了InitializingBean接口，接口方法实现如下： 方法中实例化了一个BuilderConfiguration对象，并为该对象设置了一些路径抓取器，路径方法匹配器等。最后还需要调用父类的方法 该方法比较重要，看名字可以猜测是初始化HandlerMethods用的。方法实现如下： 第一步是遍历AplicationContext中的所有Bean，只要不是以SCOPED_TARGET_NAME_PREFIX（private static final String SCOPED_TARGET_NAME_PREFIX = “scopedTarget.\";）开头就调用processCandidatebean方法，方法如下： 拿到Bean的类型，调用isHandler(beanType)方法，该方法如下： 看到两个非常熟悉的注解@Controller和@RequestMapping。 如果该Bean标注了以上两个注解，那么调用detectHandlerMethos(beanName)，方法如下： 看方法描述可知，该方法在指定的bean中寻找handler methods。 我们先来看看该类中第一个重要的方法getMappingForMethod(method, userType)，方法如下： 通过方法或者通过类级别标注的RequestMapping注解创建RequestMappingInfo， 这里可以看出RequestMappingInfo主要存放的就是RequestMapping注解标注的方法的相关信息请求信息。到这里RequestMappingInfo已经构造完成。然后我们回到之前的方法，在遍历完标注了@Controller或者@RequestMapping的类的方法并且生成了对应的RequestMappingInfo之后调用registerHandlerMethod 至此，Controller中的HandlerMethods方法遍历查找并且注册到了RequestMapptinHandlerMapping中的mappingRegistry属性中。第一步结束。 我们再回到初始化之前，看 initHandlerMethods 接下来又做了什么? 翻看第6张图，得到接下来调用 handlerMethodsInitialized 方法： 从该方法看到handlerMethods初始化结束之后并没有做其他特别的事，但是有一行注释，我们发现。handlerMethods包括了两部分：springmvc自动监测到的和用户显示通过registerMapping（）方法添加的。 到这里为止，InitializingBean方法执行结束。 ","date":"2016-10-29","objectID":"/03_springmvc_handlermapping/:1:1","tags":["springmvc"],"title":"03_springmvc_handlermapping","uri":"/03_springmvc_handlermapping/"},{"categories":["spring"],"content":"ApplicationContextAware接口 回到之前看的类结构图，得知该还继承了ApplicationObjectSupport类，而该类又实现了ApplicationContextAware接口。那我们再来看看该接口方法做了什么。 从该方法得知：该类对子类暴露了一个方法initApplicationContext，我们从RequestMappingHandlerMapping的父类AbstractHandlerMapping中看到以下实现： 我们看到这里内部执行了三个方法， extendInterceptors(this.interceptors) 该方法体内部为空，给子类使用 detectMappedInterceptors(this.adaptedInterceptors) 从所有Bean对象中找出MappedInterceptor类的的Bean，添加到this.this.adaptedInterceptors中。 initInterceptors() 将默认interceptors拦截器放入this.adaptedInterceptors中， 比如SpringBoot配置类EnableWebMvcConfiguration默认添加了两个拦截器ConversionServiceExposingInterceptor ResourceUrlProviderExposingInterceptor @Bean @Primary @Override public RequestMappingHandlerMapping requestMappingHandlerMapping( @Qualifier(\"mvcContentNegotiationManager\") ContentNegotiationManager contentNegotiationManager, @Qualifier(\"mvcConversionService\") FormattingConversionService conversionService, @Qualifier(\"mvcResourceUrlProvider\") ResourceUrlProvider resourceUrlProvider) { // Must be @Primary for MvcUriComponentsBuilder to work return super.requestMappingHandlerMapping(contentNegotiationManager, conversionService, resourceUrlProvider); } ","date":"2016-10-29","objectID":"/03_springmvc_handlermapping/:1:2","tags":["springmvc"],"title":"03_springmvc_handlermapping","uri":"/03_springmvc_handlermapping/"},{"categories":["spring"],"content":"HandlerInterceptor HandlerInterceptor接口主要有三个方法，三个方法会在dispatcherServlet执行过程中调用 preHandle：执行HandlerAdaptor的handle方法前。 default boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { return true; } preHandle：执行HandlerAdaptor的handle方法后。 default void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception { } afterCompletion：doDispatch方法执行完成前，即使抛出异常也会执行。 default void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception { } 至此，ApplicationContextAware接口方法initApplicationContext执行结束 ","date":"2016-10-29","objectID":"/03_springmvc_handlermapping/:1:3","tags":["springmvc"],"title":"03_springmvc_handlermapping","uri":"/03_springmvc_handlermapping/"},{"categories":["spring"],"content":"getHandler方法 HandlerExecutionChain getHandler(HttpServletRequest request) 我们来看第一个方法getHandlerInternal(request) 通过UrlPathHelper从request获取请求的path，然后根据path调用lookupHandlerMethod()方法获取处理这个request的HandlerMethod。 lookupHandlerMethod方法作用：查找当前请求的最佳匹配处理程序方法，如果找到多个匹配项，则选择最佳匹配项。这个方法的作用也比较明确，就不多说了。 回到之前，找到HandlerMethod，如果没有找到，返回默认的Handler。接下来是最核心的方法getHandlerExecutionChain(handler, request) 从此方法可以看出通过HandlerMethod new了一个HandlerExecutionChain对象。然后将属性adaptedInterceptors中的HandlerInterceptor添加到HandlerExecutionChain中，形成了调用执行链。 其中一个包括includePatterns和excludePatterns字符串集合并带有MappedInterceptor的类。 很明显，就是对于某些地址做特殊包括和排除的拦截器。 接下来，判断请求或者Handler是否是CROS请求，如果是，则添加 chain.addInterceptor(0, new CorsInterceptor(config)); 到此，getHandler执行结束。 ","date":"2016-10-29","objectID":"/03_springmvc_handlermapping/:2:0","tags":["springmvc"],"title":"03_springmvc_handlermapping","uri":"/03_springmvc_handlermapping/"},{"categories":["spring"],"content":"使用 ","date":"2016-10-29","objectID":"/03_springmvc_handlermapping/:3:0","tags":["springmvc"],"title":"03_springmvc_handlermapping","uri":"/03_springmvc_handlermapping/"},{"categories":["spring"],"content":"基础用法 实现HandlerInterceptor接口 package com.example.springmvcexample.handlerMapping; import lombok.extern.slf4j.Slf4j; import org.springframework.web.servlet.HandlerInterceptor; import org.springframework.web.servlet.ModelAndView; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; @Slf4j public class LogHandlerInterceptor implements HandlerInterceptor { public LogHandlerInterceptor() { log.info(\"LogHandlerInterceptor 构造方法被调用\"); } @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { log.info(\"preHandle hanlder = {}\", handler); return true; } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { log.info(\"postHandle hanlder = {}, modelAndView = {}\", handler, modelAndView); } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { log.info(\"postHandle hanlder = {}, Exception = {}\", handler, ex); } } 注意： 实现HandlerInterceptor接口的实现类使用@Component注解之后，仍然无法使用，因为该实例没有被放入AbstractHandlerMapping的adaptedInterceptors属性中。 继承WebMvcConfigurationSupport重写addInterceptors方法，将自定义的拦截器放入adaptedInterceptors中。 package com.example.springmvcexample.handlerMapping; import org.springframework.context.annotation.Configuration; import org.springframework.web.servlet.config.annotation.InterceptorRegistry; import org.springframework.web.servlet.config.annotation.WebMvcConfigurationSupport; import org.springframework.web.servlet.handler.MappedInterceptor; @Configuration public class WebMvcConfig extends WebMvcConfigurationSupport { @Override protected void addInterceptors(InterceptorRegistry registry) { super.addInterceptors(registry); // 第一种直接添加自定义的拦截器 // registry.addInterceptor(new LogHandlerInterceptor()); // 如果需要对指定url的请求调用拦截器，使用MappedInterceptor String[] includes = new String[]{\"/hello/{name}\"}; String[] excludes = new String[]{\"/echo/{name}\"}; registry.addInterceptor(new MappedInterceptor(includes, excludes, new LogHandlerInterceptor())); } } 如果是异步请求使用WebRequestInterceptor，这里不作具体描述。 ","date":"2016-10-29","objectID":"/03_springmvc_handlermapping/:3:1","tags":["springmvc"],"title":"03_springmvc_handlermapping","uri":"/03_springmvc_handlermapping/"},{"categories":["spring"],"content":"高级用法 自定义HandlerMapping 参考spring boot actuator 的 WebMvcEndpointHandlerMapping ","date":"2016-10-29","objectID":"/03_springmvc_handlermapping/:3:2","tags":["springmvc"],"title":"03_springmvc_handlermapping","uri":"/03_springmvc_handlermapping/"},{"categories":["spring"],"content":"总结 RequestMappingHandlerMapping初始化过程中会遍历所有的Bean，找到注解了@Controller或者@RequestMapping的类，通过反射找到所有的注解了@RequestMapping的方法，将其信息封装到一个RequestMapingInfo类对象中；最后将Handler（可以理解为Controller）、Method（注解标记的方法）、RequestMappingInfo三者注册到mappingRegistry的registry属性中。 执行doDispatch方法内部调用getHandler方法将HandlerInterceptor实现类封装得到HandlerExecutionChain对象。 得到HandlerExecutionChain对象之后调用HandlerInterceptor的preHandle方法 调用HandlerAdapter的handle方法后调用HandlerInterceptor的postHandle方法 在doDispatch方法调用结束前调用HandlerInterceptor的afterCompletion方法（异常处理之后，preHandle返回false也会执行） ","date":"2016-10-29","objectID":"/03_springmvc_handlermapping/:4:0","tags":["springmvc"],"title":"03_springmvc_handlermapping","uri":"/03_springmvc_handlermapping/"},{"categories":["spring"],"content":"HandlerAdapter HandlerAdapter是处理器适配器，Spring MVC通过HandlerAdapter来实际调用处理函数。它是SpringMvc处理流程的第二步,当HandlerMapping获取了定位请求处理器Handler，DispatcherServlet会将得到的Handler告知HandlerAdapter，HandlerAdapter再根据请求去定位请求的具体处理方法是哪一个。 HandlerAdapter定义了如何处理请求的策略，通过请求url、请求Method和处理器的requestMapping定义，最终确定使用处理类的哪个方法来处理请求，并检查处理类相应处理方法的参数以及相关的Annotation配置，确定如何转换需要的参数传入调用方法，并最终调用返回ModelAndView。 DispatcherServlet中根据HandlerMapping找到对应的handler method后，首先检查当前工程中注册的所有可用的handlerAdapter，根据handlerAdapter中的supports方法找到可以使用的handlerAdapter。 通过调用handlerAdapter中的handler方法来处理及准备handler method的参数及annotation(这就是spring mvc如何将request中的参数变成handle method中的输入参数的地方)，最终调用实际的handler method。 handlerAdapter这个类的作用就是接过handlermapping解析请求得到的handler对象。在更精确的定位到能够执行请求的方法。 initStrategies调用initHandlerAdapters ❶从Spring的上下文环境中获取实现了HandlerAdapter接口的实现类，默认会有以下实现类： RequestMappingHandlerAdapter HandlerFunctionAdapter HttpRequestHandlerAdapter SimpleControllerHandlerAdapter ❷对❶中查找到的实现类排序 ❸如果配置了detectAllHandlerAdapters属性为false，则从Spring上下文中获取一个beanName = handlerAdapter的实例 ❹如果前几步都没有获取到HandlerAdapter的实现类，则从dispatcherServlet.properties中获取默认的实现类。 org.springframework.web.servlet.HandlerAdapter=org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter,\\ org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter,\\ org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter,\\ org.springframework.web.servlet.function.support.HandlerFunctionAdapter RequestMappingHandlerAdapterUML类图，在没有自定义特殊情况下，该类便是HandlerAdapter的主要实现类。以下我们便以该类来讲解。 ","date":"2016-10-30","objectID":"/04_springmvc_handleradapter/:0:0","tags":["springmvc"],"title":"04_springmvc_handleradapter","uri":"/04_springmvc_handleradapter/"},{"categories":["spring"],"content":"InitializationBean 由以上类图可知：RequestMappingHandlerAdapter实现了InitializationBean接口，所以我们看一眼该接口方法做了什么？ afterPropertiesSet ","date":"2016-10-30","objectID":"/04_springmvc_handleradapter/:1:0","tags":["springmvc"],"title":"04_springmvc_handleradapter","uri":"/04_springmvc_handleradapter/"},{"categories":["spring"],"content":"❶ InitControllerAdviceCache 遍历Spring上下文实例，找出@ControllerAdvice、@ModelAttribute、@InitBinder和实现了RequestBodyAdvice或者ResponseBodyAdvice相关的处理方法，并缓存起来。具体如下： 从Spring上下文找到@ControllerAdvice注解的类 遍历1.中找到的标记了@ControllerAdvice的类中找到注解了@ModelAttribute的方法 遍历1.中找到的标记了@ControllerAdvice的类中找到注解了@InitBinder的方法 遍历1.中找到的标记了@ControllerAdvice的类中找到实现了RequestBodyAdvice和ResponseBodyAdvice的实现 ","date":"2016-10-30","objectID":"/04_springmvc_handleradapter/:1:1","tags":["springmvc"],"title":"04_springmvc_handleradapter","uri":"/04_springmvc_handleradapter/"},{"categories":["spring"],"content":"❷ 参数解析器 解析特定注解的参数 参数校验 获取默认的参数解析器，针对各种类型参数具体解析器如下： 由该方法可知，参数解析器包含了4大块：基于注解的，基于类型的，自定义的，其他 所以接口都实现了HandlerMethodArgumentResolver接口。该接口有两个方法，一个判断是否支持该类型参数，一个用于解析参数。 RequestParamMethodArgumentResolver 主要用来解析@RequestParam注解的参数 RequestParamMapMethodArgumentResolver 用来解析@RequestParam注解并参数类型为Map的参数，并且requestParam.name为空 @Override public boolean supportsParameter(MethodParameter parameter) { RequestParam requestParam = parameter.getParameterAnnotation(RequestParam.class); return (requestParam != null \u0026\u0026 Map.class.isAssignableFrom(parameter.getParameterType()) \u0026\u0026 !StringUtils.hasText(requestParam.name())); PathVariableMethodArgumentResolver 用来解析@PathVariable注解的参数；参数类型是Map，并且pathVariable.value()存在的参数 @Override public boolean supportsParameter(MethodParameter parameter) { if (!parameter.hasParameterAnnotation(PathVariable.class)) { return false; } if (Map.class.isAssignableFrom(parameter.nestedIfOptional().getNestedParameterType())) { PathVariable pathVariable = parameter.getParameterAnnotation(PathVariable.class); return (pathVariable != null \u0026\u0026 StringUtils.hasText(pathVariable.value())); } return true; } PathVariableMapMethodArgumentResolver 用来解析@PathVariable注解的参数类型是Map，并且pathVariable.value()不存在的参数 @Override public boolean supportsParameter(MethodParameter parameter) { PathVariable ann = parameter.getParameterAnnotation(PathVariable.class); return (ann != null \u0026\u0026 Map.class.isAssignableFrom(parameter.getParameterType()) \u0026\u0026 !StringUtils.hasText(ann.value())); } MatrixVariableMethodArgumentResolver 用于解析@MatrixVariable注解的参数；*参数类型是Map，并且matrixVariable.name存在的参数，也能被解析，但是没有默认的数据绑定器，所以会报错。 @Override public boolean supportsParameter(MethodParameter parameter) { if (!parameter.hasParameterAnnotation(MatrixVariable.class)) { return false; } if (Map.class.isAssignableFrom(parameter.nestedIfOptional().getNestedParameterType())) { MatrixVariable matrixVariable = parameter.getParameterAnnotation(MatrixVariable.class); return (matrixVariable != null \u0026\u0026 StringUtils.hasText(matrixVariable.name())); } return true; } 示例 /** * 请求示例： GET http://localhost:8080/mv1/123;q=123/456;q=456 * 正常响应 */ @RequestMapping(\"/mv1/{x}/{y}\") public String matrixVairable1( @PathVariable String x, @PathVariable String y, @MatrixVariable(name = \"q\", pathVar = \"x\") int q1, @MatrixVariable(name = \"q\", pathVar = \"y\") int q2) { return String.format(\"x = %s, y = %s, q1 = %s, q2 = %s\", x, y, q1, q2); } /** * 请求示例： GET http://localhost:8080/mv2/q=123/q=456 * 正常响应 */ @RequestMapping(\"/mv2/{a}/{b}\") public String matrixVairable2( @MatrixVariable(name = \"q\", pathVar = \"a\") int q1, @MatrixVariable(name = \"q\", pathVar = \"b\") int q2) { return String.format(\"a = %s, b = %s\", q1, q2); } /** * 请求示例： GET http://localhost:8080/mv4/a=123/b=456; * 结果报错：Cannot convert value of type 'java.lang.String' to required type 'java.util.Map': no matching editors or conversion strategy found */ @RequestMapping(\"/mv4/{a}/{b}\") public String matrixVairable4( @MatrixVariable(name = \"a\", pathVar = \"a\") Map\u003cString, String\u003e m1, @MatrixVariable(name = \"b\", pathVar = \"b\") Map\u003cString, String\u003e m2) throws JsonProcessingException { ObjectMapper objectMapper = new ObjectMapper(); return String.format(\"a = %s, b = %s\", objectMapper.writeValueAsString(m1), objectMapper.writeValueAsString(m2)); } MatrixVariableMapMethodArgumentResolver 用于解析@MatrixVariable注解的参数类型是Map，并且matrixVariable.name不存在的参数 @Override public boolean supportsParameter(MethodParameter parameter) { MatrixVariable matrixVariable = parameter.getParameterAnnotation(MatrixVariable.class); return (matrixVariable != null \u0026\u0026 Map.class.isAssignableFrom(parameter.getParameterType()) \u0026\u0026 !StringUtils.hasText(matrixVariable.name())); } 示例 /** * 请求示例： GET http://localhost:8080/mv3/a1=123;a2=321/b1=456;b2=654 */ @RequestMapping(\"/mv3/{a}/{b}\") public String matrixVairable3( @MatrixVariable(pathVar = \"a\") Map\u003cString, String\u003e m1, @MatrixVariable(pathVar = \"b\") Map\u003cString, String\u003e m2) throws JsonProcessingException { ObjectMapper objectMapper = new ObjectMapper(); return Str","date":"2016-10-30","objectID":"/04_springmvc_handleradapter/:1:2","tags":["springmvc"],"title":"04_springmvc_handleradapter","uri":"/04_springmvc_handleradapter/"},{"categories":["spring"],"content":"❸InitBinder参数解析器 调用@InitBinder注解的方法，解析被注解的方法的参数。 支持的参数解析器如下： /** * Return the list of argument resolvers to use for {@code @InitBinder} * methods including built-in and custom resolvers. */ private List\u003cHandlerMethodArgumentResolver\u003e getDefaultInitBinderArgumentResolvers() { List\u003cHandlerMethodArgumentResolver\u003e resolvers = new ArrayList\u003c\u003e(); // Annotation-based argument resolution resolvers.add(new RequestParamMethodArgumentResolver(getBeanFactory(), false)); resolvers.add(new RequestParamMapMethodArgumentResolver()); resolvers.add(new PathVariableMethodArgumentResolver()); resolvers.add(new PathVariableMapMethodArgumentResolver()); resolvers.add(new MatrixVariableMethodArgumentResolver()); resolvers.add(new MatrixVariableMapMethodArgumentResolver()); resolvers.add(new ExpressionValueMethodArgumentResolver(getBeanFactory())); resolvers.add(new SessionAttributeMethodArgumentResolver()); resolvers.add(new RequestAttributeMethodArgumentResolver()); // Type-based argument resolution resolvers.add(new ServletRequestMethodArgumentResolver()); resolvers.add(new ServletResponseMethodArgumentResolver()); // Custom arguments if (getCustomArgumentResolvers() != null) { resolvers.addAll(getCustomArgumentResolvers()); } // Catch-all resolvers.add(new RequestParamMethodArgumentResolver(getBeanFactory(), true)); return resolvers; } 调用流程： Controller标注的@RequestMapping方法的参数解析时，比如：RequestParamMethodArgumentResolver解析器。有如下代码： ...省略 if (binderFactory != null) { // 创建WebDataBinder对象，其中调用@InitBinder注解的方法 WebDataBinder binder = binderFactory.createBinder(webRequest, null, namedValueInfo.name); try { arg = binder.convertIfNecessary(arg, parameter.getParameterType(), parameter); } catch (ConversionNotSupportedException ex) { throw new MethodArgumentConversionNotSupportedException(arg, ex.getRequiredType(), namedValueInfo.name, parameter, ex.getCause()); } catch (TypeMismatchException ex) { throw new MethodArgumentTypeMismatchException(arg, ex.getRequiredType(), namedValueInfo.name, parameter, ex.getCause()); } } ...省略 public final WebDataBinder createBinder( NativeWebRequest webRequest, @Nullable Object target, String objectName) throws Exception { WebDataBinder dataBinder = createBinderInstance(target, objectName, webRequest); if (this.initializer != null) { // 为initbinder方法添加消息转换大，属性编辑器等 this.initializer.initBinder(dataBinder, webRequest); } // 调用@InitBinder方法 initBinder(dataBinder, webRequest); return dataBinder; } 调用@ControllerAdvice中@InitBinder的全局方法和@Controller中的单属于每个controller的@InitBinder方法 public void initBinder(WebDataBinder dataBinder, NativeWebRequest request) throws Exception { for (InvocableHandlerMethod binderMethod : this.binderMethods) { if (isBinderMethodApplicable(binderMethod, dataBinder)) { // 调用@initBinder注解的方法 Object returnValue = binderMethod.invokeForRequest(request, null, dataBinder); if (returnValue != null) { throw new IllegalStateException( \"@InitBinder methods must not return a value (should be void): \" + binderMethod); } } } } 调用参数解析器解析initBinder参数 protected Object[] getMethodArgumentValues(NativeWebRequest request, @Nullable ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception { ... 省略 // 此处resolvers便是afterPropertiesSet中调用getDefaultInitBinderArgumentResolvers() if (!this.resolvers.supportsParameter(parameter)) { throw new IllegalStateException(formatArgumentError(parameter, \"No suitable resolver\")); } try { args[i] = this.resolvers.resolveArgument(parameter, mavContainer, request, this.dataBinderFactory); } ... 省略 } ","date":"2016-10-30","objectID":"/04_springmvc_handleradapter/:1:3","tags":["springmvc"],"title":"04_springmvc_handleradapter","uri":"/04_springmvc_handleradapter/"},{"categories":["spring"],"content":"❹返回值解析器 处理返回值 源码位置： public void invokeAndHandle(ServletWebRequest webRequest, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception { // ①调用handle，Controller方法的处理函数得到返回值 Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs); setResponseStatus(webRequest); if (returnValue == null) { if (isRequestNotModified(webRequest) || getResponseStatus() != null || mavContainer.isRequestHandled()) { disableContentCachingIfNecessary(webRequest); mavContainer.setRequestHandled(true); return; } } else if (StringUtils.hasText(getResponseStatusReason())) { mavContainer.setRequestHandled(true); return; } mavContainer.setRequestHandled(false); Assert.state(this.returnValueHandlers != null, \"No return value handlers\"); try { // ②处理返回值 this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest); } catch (Exception ex) { if (logger.isTraceEnabled()) { logger.trace(formatErrorForReturnValue(returnValue), ex); } throw ex; } } ①处调用请求的处理，②处调用返回值处理逻辑。 返回值处理逻辑如下： /** * Iterate over registered {@link HandlerMethodReturnValueHandler HandlerMethodReturnValueHandlers} and invoke the one that supports it. * @throws IllegalStateException if no suitable {@link HandlerMethodReturnValueHandler} is found. */ @Override public void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception { // 获取处理返回值的解析器 HandlerMethodReturnValueHandler handler = selectHandler(returnValue, returnType); if (handler == null) { throw new IllegalArgumentException(\"Unknown return value type: \" + returnType.getParameterType().getName()); } // 根据返回的对应解析器处理返回值 handler.handleReturnValue(returnValue, returnType, mavContainer, webRequest); } ","date":"2016-10-30","objectID":"/04_springmvc_handleradapter/:1:4","tags":["springmvc"],"title":"04_springmvc_handleradapter","uri":"/04_springmvc_handleradapter/"},{"categories":["spring"],"content":"handle流程 handle方法是HandleAdapter的核心方法，Controller中业务的处理逻辑也是在此方法中被调用 RequestMappingHandlerAdapter中的handle方法如下： /** * This implementation expects the handler to be an {@link HandlerMethod}. */ @Override @Nullable public final ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { return handleInternal(request, response, (HandlerMethod) handler); } ![image-20200725153200873](/Users/Adam.Jin/Library/Mobile Documents/com~apple~CloudDocs/笔记/技术/Java/springmvc/04_HandlerAdapter.assets/image-20200725153200873.png) invokeHandlerMethod方法： ❶、获取DataBinderFactory代码如下： ❷、获取ModelFactory的逻辑与DataBinderFacotory差不多 ❼、参数名解析参考https://blog.csdn.net/qq271859852/article/details/84963672 ❽、填充model数据，包括SessionAtrribute中的数据。 接着调用invokeAndHandle方法 ","date":"2016-10-30","objectID":"/04_springmvc_handleradapter/:2:0","tags":["springmvc"],"title":"04_springmvc_handleradapter","uri":"/04_springmvc_handleradapter/"},{"categories":["spring"],"content":"HandlerExceptionResolver Spring的处理器异常解析器HandlerExceptionResolver接口的实现负责处理各类控制器执行过程中出现的异常 public interface HandlerExceptionResolver { @Nullable ModelAndView resolveException( HttpServletRequest request, HttpServletResponse response, @Nullable Object handler, Exception ex); } ","date":"2016-11-01","objectID":"/05_springmvc_handlerexceptionresolver/:0:0","tags":["springmvc"],"title":"05_springmvc_handlerExceptionResolver","uri":"/05_springmvc_handlerexceptionresolver/"},{"categories":["spring"],"content":"初始化 初始化过程比较简单，在DispatcherServlet类initStrategies方法中调用initHandlerExceptionResolvers获取所有实现了HandlerExceptionResolver接口的实例 /** * Initialize the HandlerExceptionResolver used by this class. * \u003cp\u003eIf no bean is defined with the given name in the BeanFactory for this namespace, * we default to no exception resolver. */ private void initHandlerExceptionResolvers(ApplicationContext context) { this.handlerExceptionResolvers = null; if (this.detectAllHandlerExceptionResolvers) { // Find all HandlerExceptionResolvers in the ApplicationContext, including ancestor contexts. // 找到所有实现了HandlerExceptionResolver接口 Map\u003cString, HandlerExceptionResolver\u003e matchingBeans = BeanFactoryUtils .beansOfTypeIncludingAncestors(context, HandlerExceptionResolver.class, true, false); if (!matchingBeans.isEmpty()) { this.handlerExceptionResolvers = new ArrayList\u003c\u003e(matchingBeans.values()); // We keep HandlerExceptionResolvers in sorted order. AnnotationAwareOrderComparator.sort(this.handlerExceptionResolvers); } } else { try { HandlerExceptionResolver her = context.getBean(HANDLER_EXCEPTION_RESOLVER_BEAN_NAME, HandlerExceptionResolver.class); this.handlerExceptionResolvers = Collections.singletonList(her); } catch (NoSuchBeanDefinitionException ex) { // Ignore, no HandlerExceptionResolver is fine too. } } // Ensure we have at least some HandlerExceptionResolvers, by registering // default HandlerExceptionResolvers if no other resolvers are found. // 获取默认的异常解析器 if (this.handlerExceptionResolvers == null) { this.handlerExceptionResolvers = getDefaultStrategies(context, HandlerExceptionResolver.class); if (logger.isTraceEnabled()) { logger.trace(\"No HandlerExceptionResolvers declared in servlet '\" + getServletName() + \"': using default strategies from DispatcherServlet.properties\"); } } } ","date":"2016-11-01","objectID":"/05_springmvc_handlerexceptionresolver/:1:0","tags":["springmvc"],"title":"05_springmvc_handlerExceptionResolver","uri":"/05_springmvc_handlerexceptionresolver/"},{"categories":["spring"],"content":"处理逻辑 在处理请求过程中，当发生了异常，被try…catch抓到之后，赋值给了dispatchException变量，然后在processDispatchResult方法中，判断exception是否为空，非空即表示存在异常，调用异常处理解析器（方法：processHandlerException）处理异常，返回ModelAndView private void processDispatchResult(HttpServletRequest request, HttpServletResponse response, @Nullable HandlerExecutionChain mappedHandler, @Nullable ModelAndView mv, @Nullable Exception exception) throws Exception { boolean errorView = false; if (exception != null) { if (exception instanceof ModelAndViewDefiningException) { logger.debug(\"ModelAndViewDefiningException encountered\", exception); mv = ((ModelAndViewDefiningException) exception).getModelAndView(); } else { Object handler = (mappedHandler != null ? mappedHandler.getHandler() : null); // 处理异常 mv = processHandlerException(request, response, handler, exception); errorView = (mv != null); } } // Did the handler return a view to render? if (mv != null \u0026\u0026 !mv.wasCleared()) { render(mv, request, response); if (errorView) { WebUtils.clearErrorRequestAttributes(request); } } ...省略 } 处理异常 protected ModelAndView processHandlerException(HttpServletRequest request, HttpServletResponse response, @Nullable Object handler, Exception ex) throws Exception { ... 省略 // Check registered HandlerExceptionResolvers... ModelAndView exMv = null; if (this.handlerExceptionResolvers != null) { for (HandlerExceptionResolver resolver : this.handlerExceptionResolvers) { // 调用异常处理解析器处理异常 exMv = resolver.resolveException(request, response, handler, ex); if (exMv != null) { break; } } } ...省略 throw ex; } ","date":"2016-11-01","objectID":"/05_springmvc_handlerexceptionresolver/:2:0","tags":["springmvc"],"title":"05_springmvc_handlerExceptionResolver","uri":"/05_springmvc_handlerexceptionresolver/"},{"categories":["spring"],"content":"ExceptionHandlerMethodResolver 异常解析器中默认最常用的，也是工作中使用最多的，就是该类。主要处理@ExceptionHandler注解 该类继承和实现的接口如下图： 由上图可知，该类主要实现了HandlerExeptionResolver用于解析异常，并且实现了@InitializationBean接口。 @InitializationBean @Override public void afterPropertiesSet() { // Do this first, it may add ResponseBodyAdvice beans // ① initExceptionHandlerAdviceCache(); // ② if (this.argumentResolvers == null) { List\u003cHandlerMethodArgumentResolver\u003e resolvers = getDefaultArgumentResolvers(); this.argumentResolvers = new HandlerMethodArgumentResolverComposite().addResolvers(resolvers); } // ③ if (this.returnValueHandlers == null) { List\u003cHandlerMethodReturnValueHandler\u003e handlers = getDefaultReturnValueHandlers(); this.returnValueHandlers = new HandlerMethodReturnValueHandlerComposite().addHandlers(handlers); } } ① 、遍历所有标注了ControllerAdvice的类，并从标注了该注解的类下找到所有标注了@ExceptionHandler的方法。具体代码如下： private void initExceptionHandlerAdviceCache() { ... 省略 // 遍历Spring上下文，找出所有@ControllerAdvice的Bean List\u003cControllerAdviceBean\u003e adviceBeans = ControllerAdviceBean.findAnnotatedBeans(getApplicationContext()); for (ControllerAdviceBean adviceBean : adviceBeans) { Class\u003c?\u003e beanType = adviceBean.getBeanType(); if (beanType == null) { throw new IllegalStateException(\"Unresolvable type for ControllerAdviceBean: \" + adviceBean); } // 循环所有@ControllerAdvice的Bean，构造ExceptionHandlerMethodResolver实例用于缓存处理异常的方法 ExceptionHandlerMethodResolver resolver = new ExceptionHandlerMethodResolver(beanType); // 如果该@ControllerAdvice中没有@ExceptionHandler，则丢弃刚new的实例 if (resolver.hasExceptionMappings()) { this.exceptionHandlerAdviceCache.put(adviceBean, resolver); } // 如果标注了@ControllerAdvice的类实现了ResponseBodyAdvice，放到responseBodyAdvice属性中 if (ResponseBodyAdvice.class.isAssignableFrom(beanType)) { this.responseBodyAdvice.add(adviceBean); } } ... 省略 } ②、获取异常处理解析器中，用于异常处理方法的参数解析器 ③、获取异常处理解析器中，用于异常处理方法的返回值解析器 接下来的流程和HandlerAdapter的处理逻辑差不多 @Override @Nullable protected ModelAndView doResolveHandlerMethodException(HttpServletRequest request, HttpServletResponse response, @Nullable HandlerMethod handlerMethod, Exception exception) { // 获取异常处理方法 ServletInvocableHandlerMethod exceptionHandlerMethod = getExceptionHandlerMethod(handlerMethod, exception); if (exceptionHandlerMethod == null) { return null; } // 设置异常处理方法的参数解析器 if (this.argumentResolvers != null) { exceptionHandlerMethod.setHandlerMethodArgumentResolvers(this.argumentResolvers); } // 设置异常处理方法的返回值解析器 if (this.returnValueHandlers != null) { exceptionHandlerMethod.setHandlerMethodReturnValueHandlers(this.returnValueHandlers); } ServletWebRequest webRequest = new ServletWebRequest(request, response); ModelAndViewContainer mavContainer = new ModelAndViewContainer(); try { if (logger.isDebugEnabled()) { logger.debug(\"Using @ExceptionHandler \" + exceptionHandlerMethod); } Throwable cause = exception.getCause(); if (cause != null) { // Expose cause as provided argument as well // 调用异常解析方法 exceptionHandlerMethod.invokeAndHandle(webRequest, mavContainer, exception, cause, handlerMethod); } else { // Otherwise, just the given exception as-is // 调用异常解析方法 exceptionHandlerMethod.invokeAndHandle(webRequest, mavContainer, exception, handlerMethod); } } catch (Throwable invocationEx) { // Any other than the original exception (or its cause) is unintended here, // probably an accident (e.g. failed assertion or the like). if (invocationEx != exception \u0026\u0026 invocationEx != exception.getCause() \u0026\u0026 logger.isWarnEnabled()) { logger.warn(\"Failure in @ExceptionHandler \" + exceptionHandlerMethod, invocationEx); } // Continue with default processing of the original exception... return null; } if (mavContainer.isRequestHandled()) { return new ModelAndView(); } else { ModelMap model = mavContainer.getModel(); HttpStatus status = mavContainer.getStatus(); ModelAndView mav = new ModelAndView(mavContainer.getViewName(), model, status); mav.setViewName(mavContainer.getViewName()); if (!mavContainer.isViewReference()) { mav.setView((View) mavContainer.getView()); } if (model instanceof RedirectAttributes) {","date":"2016-11-01","objectID":"/05_springmvc_handlerexceptionresolver/:2:1","tags":["springmvc"],"title":"05_springmvc_handlerExceptionResolver","uri":"/05_springmvc_handlerexceptionresolver/"},{"categories":["spring"],"content":"参数校验器 Spring MVC中的参数校验器并没有自己大量从新开发，而是使用了Hibernate-Validator，而Hibernate-Validator实现了JSR-303的所有功能。在SpringMVC中有两个地方可以于用参数校验。 ","date":"2016-11-01","objectID":"/06_springmvc_validator/:0:0","tags":["springmvc"],"title":"06_springmvc_validator","uri":"/06_springmvc_validator/"},{"categories":["spring"],"content":"1、Bean Validator 此处的Bean Validator指JSR-303（JSR是Java Specification Requests的缩写，意思是Java 规范提案，其中303号内容指提供一套基于注解的校验规范[This JSR will define a meta-data model and API for JavaBeanTM validation based on annotations, with overrides and extended meta-data through the use of XML validation descriptors.]）。Hibernate Validator是它最出名的实现，也是目前世界上使用最广的校验器实现。Hibernate Validator 提供了 JSR 303 规范中所有内置 constraint 的实现，除此之外还有一些附加的 constraint。 Annotation 注解说明 备注 DecimalMax 元素必须是一个数字，其值必须小于或等于指定的最大值 DecimalMin 元素必须是一个数字，其值必须大于或等于指定的最小值 Pattern 必须与指定的正则表达式匹配 Email 检查给定的字符序列（例如字符串）是否是格式正确的电子邮件地址 Max 元素必须是一个数字，其值必须小于或等于指定的最大值 Min 元素必须是一个数字，其值必须大于或等于指定的最小值 AssertFalse 元素的值必须为false AssertTrue 元素的值必须为true Digits 元素必须是可接受范围内的数字 NegativeOrZero 元素必须为严格的负数（即0视为无效值） NotBlank 删除任何前导或尾随空格后，检查字符序列是否不为空 NotEmpty 元素必须不为null且不为empty NotNull 元素必须不为null Null 元素必须为null PositiveOrZero 元素必须为正数或0 Positive 元素必须为严格的正数（即0视为无效值） Size 元素大小必须在指定的边界（包括在内）之间。 Future 元素必须是将来的瞬间，日期或时间 FutureOrPresent 元素必须是当前或将来的瞬间，日期或时间 Past 元素必须是过去的瞬间，日期或时间 PastOrPresent 元素必须是过去或现在的瞬间，日期或时间 ","date":"2016-11-01","objectID":"/06_springmvc_validator/:1:0","tags":["springmvc"],"title":"06_springmvc_validator","uri":"/06_springmvc_validator/"},{"categories":["spring"],"content":"2、Hibernate Validator编程式校验 SpringBoot \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-validation\u003c/artifactId\u003e \u003c/dependency\u003e 直接导入 \u003cdependency\u003e \u003cgroupId\u003ejavax.validation\u003c/groupId\u003e \u003cartifactId\u003evalidation-api\u003c/artifactId\u003e \u003cversion\u003e2.0.0.Final\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.hibernate.validator\u003c/groupId\u003e \u003cartifactId\u003ehibernate-validator\u003c/artifactId\u003e \u003cversion\u003e6.1.5.Final\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e ","date":"2016-11-01","objectID":"/06_springmvc_validator/:2:0","tags":["springmvc"],"title":"06_springmvc_validator","uri":"/06_springmvc_validator/"},{"categories":["spring"],"content":"2.1、普通对象校验 public class User { @NotNull private String name; @Min(value=\"1\") private String age; //... } ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); validator = factory.getValidator(); Set\u003cConstraintViolation\u003cCar\u003e\u003e constraintViolations = validator.validate( department ); assertEquals( 1, constraintViolations.size() ); assertEquals( \"must not be null\", constraintViolations.iterator().next().getMessage() ); Validation类是Bean Validation的入口点，buildDefaultValidatorFactory()方法基于默认的Bean Validation提供程序构建并返回ValidatorFactory实例。使用默认验证提供程序解析程序逻辑解析提供程序列表。代码上等同于Validation.byDefaultProvider().configure().buildValidatorFactory()。 之后调用该ValidatorFactory.getValidator()返回一个校验器实例，使用这个校验器的validate方法对目标对象的属性进行校验，返回一个ConstraintViolation集合。ConstraintViolation用于描述约束违规。 此对象公开约束违规上下文以及描述违规的消息。 ","date":"2016-11-01","objectID":"/06_springmvc_validator/:2:1","tags":["springmvc"],"title":"06_springmvc_validator","uri":"/06_springmvc_validator/"},{"categories":["spring"],"content":"2.2、分组校验 首先需要在constraint注解上指定groups属性，这个属性是一个class对象数组，再调用javax.validation.Validator接口的validate方法的时候将第二个参数groups传入class数组元素之一就可以针对这个这个group的校验规则生效。 ","date":"2016-11-01","objectID":"/06_springmvc_validator/:2:2","tags":["springmvc"],"title":"06_springmvc_validator","uri":"/06_springmvc_validator/"},{"categories":["spring"],"content":"HandlerMethodArgumentResolver 在04_HandlerAdaptor那一章节中我们已经讲过参数解析器了，在参数解析过程中，SpringMVC会对参数进行校验。 我们这里以RequestResponseBodyMethodProcessor来举例，该类实现了HandlerMethodArgumentResolver接口，用于处理@RequestBody标记的参数类型。 ","date":"2016-11-01","objectID":"/06_springmvc_validator/:3:0","tags":["springmvc"],"title":"06_springmvc_validator","uri":"/06_springmvc_validator/"},{"categories":["spring"],"content":"validateIfApplicable 该方法便是用于参数的校验，具体逻辑如下： ","date":"2016-11-01","objectID":"/06_springmvc_validator/:3:1","tags":["springmvc"],"title":"06_springmvc_validator","uri":"/06_springmvc_validator/"},{"categories":["spring"],"content":"ValidationAutoConfiguration @RestController @Validated public class TestController { @RequestMapping(\"/test\") public String test(@RequestParam(\"age\") @Max(200) Integer age) { return String.format(\"age = %s\", age); } } spring驱动类：ValidationAutoConfiguration MethodValidationPostProcessor 校验器 https://blog.csdn.net/roberts939299/article/details/73730410 ","date":"2016-11-01","objectID":"/06_springmvc_validator/:4:0","tags":["springmvc"],"title":"06_springmvc_validator","uri":"/06_springmvc_validator/"},{"categories":["工具"],"content":"java解密代码如下： // // Source code recreated from a .class file by IntelliJ IDEA // (powered by Fernflower decompiler) // package com.test; import org.apache.commons.codec.digest.DigestUtils; import javax.crypto.Cipher; import javax.crypto.SecretKey; import javax.crypto.SecretKeyFactory; import javax.crypto.spec.DESKeySpec; import java.io.ByteArrayOutputStream; import java.io.DataOutputStream; import java.io.IOException; import java.io.UnsupportedEncodingException; import java.security.SecureRandom; import java.util.Random; public class Test { // private static final String OO0O00OO00OOOOOO0000O0O000O0O0000OOOO00OO000OO0000OO000000O00O0O000OO00O0O000O00O0O00OOOOOO0OOO0 = \"DES\"; public static long number = 3680984568597093857L; private static int num = 8; public static void main(String[] args) throws Exception { String content = \"sdsfew1tf45r1g3\"; String s1 = encode(content); String s2 = decode(s1); System.out.println(s1); System.out.println(s2); } public static String decode(String data) throws IOException, Exception { if (data == null) { return null; } else { String rs = \"\"; if (!fff(data)) { byte[] buf = ggg(data); byte[] head = new byte[num]; System.arraycopy(buf, 0, head, 0, head.length); byte[] d = new byte[buf.length - head.length]; System.arraycopy(buf, head.length, d, 0, d.length); byte[] bt = jjj(d, kkk(head)); rs = new String(bt); } return rs; } } public static String encode(String content) throws Exception { byte[] head = aaa(num); byte[] d = bbb(content.getBytes(\"utf-8\"), head); byte[] result = new byte[head.length + d.length]; System.arraycopy(head, 0, result, 0, head.length); System.arraycopy(d, 0, result, head.length, d.length); String rs = ccc(result); return rs; } static byte[] aaa(int len) { byte[] data = new byte[len]; for(int i = 0; i \u003c len; ++i) { data[i] = (byte)(new Random()).nextInt(127); } return data; } public static byte[] bbb(byte[] data, byte[] head) throws Exception { SecureRandom sr = new SecureRandom(); DESKeySpec dks = new DESKeySpec(kkk(head)); SecretKeyFactory keyFactory = SecretKeyFactory.getInstance(\"DES\"); SecretKey securekey = keyFactory.generateSecret(dks); Cipher cipher = Cipher.getInstance(\"DES\"); cipher.init(1, securekey, sr); return cipher.doFinal(data); } public static String ccc(byte[] byteData) throws UnsupportedEncodingException { return ddd(byteData, \"UTF-8\"); } public static String ddd(byte[] byteData, String encoding) throws UnsupportedEncodingException { if (byteData == null) { throw new IllegalArgumentException(\"byteData cannot be null\"); } else { return new String(eee(byteData), encoding); } } public static final byte[] eee(byte[] byteData) { if (byteData == null) { throw new IllegalArgumentException(\"byteData cannot be null\"); } else { byte[] byteDest = new byte[(byteData.length + 2) / 3 * 4]; int iSrcIdx = 0; int iDestIdx; for(iDestIdx = 0; iSrcIdx \u003c byteData.length - 2; iSrcIdx += 3) { byteDest[iDestIdx++] = (byte)(byteData[iSrcIdx] \u003e\u003e\u003e 2 \u0026 63); byteDest[iDestIdx++] = (byte)(byteData[iSrcIdx + 1] \u003e\u003e\u003e 4 \u0026 15 | byteData[iSrcIdx] \u003c\u003c 4 \u0026 63); byteDest[iDestIdx++] = (byte)(byteData[iSrcIdx + 2] \u003e\u003e\u003e 6 \u0026 3 | byteData[iSrcIdx + 1] \u003c\u003c 2 \u0026 63); byteDest[iDestIdx++] = (byte)(byteData[iSrcIdx + 2] \u0026 63); } if (iSrcIdx \u003c byteData.length) { byteDest[iDestIdx++] = (byte)(byteData[iSrcIdx] \u003e\u003e\u003e 2 \u0026 63); if (iSrcIdx \u003c byteData.length - 1) { byteDest[iDestIdx++] = (byte)(byteData[iSrcIdx + 1] \u003e\u003e\u003e 4 \u0026 15 | byteData[iSrcIdx] \u003c\u003c 4 \u0026 63); byteDest[iDestIdx++] = (byte)(byteData[iSrcIdx + 1] \u003c\u003c 2 \u0026 63); } else { byteDest[iDestIdx++] = (byte)(byteData[iSrcIdx] \u003c\u003c 4 \u0026 63); } } for(iSrcIdx = 0; iSrcIdx \u003c iDestIdx; ++iSrcIdx) { if (byteDest[iSrcIdx] \u003c 26) { byteDest[iSrcIdx] = (byte)(byteDest[iSrcIdx] + 65); } else if (byteDest[iSrcIdx] \u003c 52) { byteDest[iSrcIdx] = (byte)(byteDest[iSrcIdx] + 97 - 26); } else if (byteDest[iSrcIdx] \u003c 62) { byteDest[iSrcIdx] = (byte)(byteDest[iSrcIdx] + 48 - 52); } else if (byteDest[iSrcIdx] \u003c 63) { byteDest[iSrcIdx] = 43; } else { byteDest[iSrcIdx] = 47; } } while(iSrcIdx \u003c","date":"2016-10-27","objectID":"/finalshell%E8%A7%A3%E5%AF%86/:0:0","tags":["finalshell"],"title":"01_finalshell密码破解","uri":"/finalshell%E8%A7%A3%E5%AF%86/"},{"categories":["工具"],"content":"navicat 解密 步骤如下 打开链接：https://tool.lu/coderunner/ 粘贴以下代码，修改倒数第二行 \u003c?php namespace FatSmallTools; class NavicatPassword { protected $version = 0; protected $aesKey = 'libcckeylibcckey'; protected $aesIv = 'libcciv libcciv '; protected $blowString = '3DC5CA39'; protected $blowKey = null; protected $blowIv = null; public function __construct($version = 12) { $this-\u003eversion = $version; $this-\u003eblowKey = sha1('3DC5CA39', true); $this-\u003eblowIv = hex2bin('d9c7c3c8870d64bd'); } public function encrypt($string) { $result = FALSE; switch ($this-\u003eversion) { case 11: $result = $this-\u003eencryptEleven($string); break; case 12: $result = $this-\u003eencryptTwelve($string); break; default: break; } return $result; } protected function encryptEleven($string) { $round = intval(floor(strlen($string) / 8)); $leftLength = strlen($string) % 8; $result = ''; $currentVector = $this-\u003eblowIv; for ($i = 0; $i \u003c $round; $i++) { $temp = $this-\u003eencryptBlock($this-\u003exorBytes(substr($string, 8 * $i, 8), $currentVector)); $currentVector = $this-\u003exorBytes($currentVector, $temp); $result .= $temp; } if ($leftLength) { $currentVector = $this-\u003eencryptBlock($currentVector); $result .= $this-\u003exorBytes(substr($string, 8 * $i, $leftLength), $currentVector); } return strtoupper(bin2hex($result)); } protected function encryptBlock($block) { return openssl_encrypt($block, 'BF-ECB', $this-\u003eblowKey, OPENSSL_RAW_DATA|OPENSSL_NO_PADDING); } protected function decryptBlock($block) { return openssl_decrypt($block, 'BF-ECB', $this-\u003eblowKey, OPENSSL_RAW_DATA|OPENSSL_NO_PADDING); } protected function xorBytes($str1, $str2) { $result = ''; for ($i = 0; $i \u003c strlen($str1); $i++) { $result .= chr(ord($str1[$i]) ^ ord($str2[$i])); } return $result; } protected function encryptTwelve($string) { $result = openssl_encrypt($string, 'AES-128-CBC', $this-\u003eaesKey, OPENSSL_RAW_DATA, $this-\u003eaesIv); return strtoupper(bin2hex($result)); } public function decrypt($string) { $result = FALSE; switch ($this-\u003eversion) { case 11: $result = $this-\u003edecryptEleven($string); break; case 12: $result = $this-\u003edecryptTwelve($string); break; default: break; } return $result; } protected function decryptEleven($upperString) { $string = hex2bin(strtolower($upperString)); $round = intval(floor(strlen($string) / 8)); $leftLength = strlen($string) % 8; $result = ''; $currentVector = $this-\u003eblowIv; for ($i = 0; $i \u003c $round; $i++) { $encryptedBlock = substr($string, 8 * $i, 8); $temp = $this-\u003exorBytes($this-\u003edecryptBlock($encryptedBlock), $currentVector); $currentVector = $this-\u003exorBytes($currentVector, $encryptedBlock); $result .= $temp; } if ($leftLength) { $currentVector = $this-\u003eencryptBlock($currentVector); $result .= $this-\u003exorBytes(substr($string, 8 * $i, $leftLength), $currentVector); } return $result; } protected function decryptTwelve($upperString) { $string = hex2bin(strtolower($upperString)); return openssl_decrypt($string, 'AES-128-CBC', $this-\u003eaesKey, OPENSSL_RAW_DATA, $this-\u003eaesIv); } } use FatSmallTools\\NavicatPassword; //需要指定版本，11或12 $navicatPassword = new NavicatPassword(12); //$navicatPassword = new NavicatPassword(11); //解密 //$decode = $navicatPassword-\u003edecrypt('15057D7BA390'); $decode = $navicatPassword-\u003edecrypt('266523D8991D6B48575B4C9F92F40BA6742967A9315D95CD4F1FEDB356C99FFC'); echo $decode.\"\\n\"; ","date":"2018-12-27","objectID":"/navicat%E8%A7%A3%E5%AF%86/:0:0","tags":["navicat"],"title":"02_navicat密码破解","uri":"/navicat%E8%A7%A3%E5%AF%86/"},{"categories":["测试工具"],"content":"测试工具gatling（加特林） 在学习Webflux响应式编程的过程中偶然听到了gatling这个负载测试工具，并且看着很简单。之前有听说过loadrunner和jmeter，并且使用过wrk这个小工具，但是没有一个详细完整的报告。因此看到这个工具的时候，就花了点时间在网上找资料学习了一下。这个文档只是为了记录我的学习过程。我是开发人员，因此不会关注太细，如有问题，请指正。 ","date":"2019-10-27","objectID":"/gatling/:0:0","tags":["test"],"title":"gatling测试工具用法介绍","uri":"/gatling/"},{"categories":["测试工具"],"content":"1、使用方式一 下载 Download - Gatling Load and Performance testing 目录结构 bin //命令 conf //配置文件 lib //类库 results //测试之后生成的报告地址 target 测试脚本编译目录 user-files //脚本目录 resource 脚本数据资源文件 simulations 脚本文件，脚本下文件目录以package方式 下载完成之后simulations下有样例文件user-files/simulations/computerdatabase/BasicSimulation.scala 执行样例 sh gatling.sh 执行之后可以选择需要执行的脚本。最后会在results下生成测试报告 ","date":"2019-10-27","objectID":"/gatling/:1:0","tags":["test"],"title":"gatling测试工具用法介绍","uri":"/gatling/"},{"categories":["测试工具"],"content":"2、使用方式二 方式二使用的maven进行测试，个人觉得这种方式更适合，代码可调试。比下载工具方式更简单 使用idea下载scala插件 下载完成新建maven项目,如图配置 最新参见:Maven Repository: io.gatling.highcharts » gatling-highcharts-maven-archetype GroupId:io.gatling.highcharts ArtifactId:gatling-highcharts-maven.archetype Version:3.0.2 ├── pom.xml ├── src │ └── test │ ├── resources │ │ ├── bodies │ │ ├── data │ │ ├── gatling.conf │ │ ├── logback.xml │ │ └── recorder.conf │ └── scala │ ├── BasicSimulation.scala //源码文件 │ ├── Engine.scala //执行文件 │ ├── IDEPathHelper.scala │ └── Recorder.scala └── target 不使用artifact直接使用插件 如下： \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eio.gatling.highcharts\u003c/groupId\u003e \u003cartifactId\u003egatling-charts-highcharts\u003c/artifactId\u003e \u003cversion\u003e3.0.2\u003c/version\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eio.gatling\u003c/groupId\u003e \u003cartifactId\u003egatling-maven-plugin\u003c/artifactId\u003e \u003cversion\u003e3.0.2\u003c/version\u003e \u003cconfiguration\u003e \u003csimulationsFolder\u003esrc/main/java\u003c/simulationsFolder\u003e \u003csimulationClass\u003ecom.scemsjyd.BaseSimulation\u003c/simulationClass\u003e \u003c/configuration\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cphase\u003etest\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003eexecute\u003c/goal\u003e \u003c/goals\u003e \u003cconfiguration\u003e \u003cjvmArgs\u003e \u003cjvmArg\u003e-Dgatling.http.ahc.connectTimeout=6000000\u003c/jvmArg\u003e \u003cjvmArg\u003e-Dgatling.http.ahc.requestTimeout=6000000\u003c/jvmArg\u003e \u003cjvmArg\u003e-Dgatling.http.ahc.sslSessionTimeout=6000000\u003c/jvmArg\u003e \u003cjvmArg\u003e-Dgatling.http.ahc.pooledConnectionIdleTimeout=6000000\u003c/jvmArg\u003e \u003cjvmArg\u003e-Dgatling.http.ahc.readTimeout=6000000\u003c/jvmArg\u003e \u003c/jvmArgs\u003e \u003c/configuration\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e ","date":"2019-10-27","objectID":"/gatling/:2:0","tags":["test"],"title":"gatling测试工具用法介绍","uri":"/gatling/"},{"categories":["spring"],"content":"Spring kafka 要点 以下内容记录了一些工作中遇到的kafka的要点（个人认为） ","date":"2019-10-27","objectID":"/springdata_kafka/:0:0","tags":["kafka"],"title":"SpringData_kafka","uri":"/springdata_kafka/"},{"categories":["spring"],"content":"一、kafka消费者 ","date":"2019-10-27","objectID":"/springdata_kafka/:1:0","tags":["kafka"],"title":"SpringData_kafka","uri":"/springdata_kafka/"},{"categories":["spring"],"content":"1.1、源码分析 1.1.1、 @EnableKafka 作用：kafka开启入口 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Import(KafkaBootstrapConfiguration.class) // 最重要的入口配置 public @interface EnableKafka { } 1.1.1.1、 KafkaBootstrapConfiguration 作用：kafka启动配置类，该类主要实例化以下两个Bean 1.1.1.1.1、 KafkaListenerAnnotationBeanPostProcessor 作用：实现BeanPostProcessor接口，重写方法 1.1.1.1.1.1、 postProcessAfterInitialization @Override public Object postProcessAfterInitialization(final Object bean, final String beanName) throws BeansException { if (!this.nonAnnotatedClasses.contains(bean.getClass())) { Class\u003c?\u003e targetClass = AopUtils.getTargetClass(bean); // 找到标记在类上的@KafkaListener注解 Collection\u003cKafkaListener\u003e classLevelListeners = findListenerAnnotations(targetClass); final boolean hasClassLevelListeners = classLevelListeners.size() \u003e 0; final List\u003cMethod\u003e multiMethods = new ArrayList\u003c\u003e(); // 找到标记在方法上的@KafkaListener注解 Map\u003cMethod, Set\u003cKafkaListener\u003e\u003e annotatedMethods = MethodIntrospector.selectMethods(targetClass, new MethodIntrospector.MetadataLookup\u003cSet\u003cKafkaListener\u003e\u003e() { @Override public Set\u003cKafkaListener\u003e inspect(Method method) { Set\u003cKafkaListener\u003e listenerMethods = findListenerAnnotations(method); return (!listenerMethods.isEmpty() ? listenerMethods : null); } }); if (hasClassLevelListeners) { Set\u003cMethod\u003e methodsWithHandler = MethodIntrospector.selectMethods(targetClass, (ReflectionUtils.MethodFilter) method -\u003e AnnotationUtils.findAnnotation(method, KafkaHandler.class) != null); multiMethods.addAll(methodsWithHandler); } if (annotatedMethods.isEmpty()) { this.nonAnnotatedClasses.add(bean.getClass()); if (this.logger.isTraceEnabled()) { this.logger.trace(\"No @KafkaListener annotations found on bean type: \" + bean.getClass()); } } else { // Non-empty set of methods for (Map.Entry\u003cMethod, Set\u003cKafkaListener\u003e\u003e entry : annotatedMethods.entrySet()) { Method method = entry.getKey(); for (KafkaListener listener : entry.getValue()) { // 重要的方法，处理kafkaListener processKafkaListener(listener, method, bean, beanName); } } if (this.logger.isDebugEnabled()) { this.logger.debug(annotatedMethods.size() + \" @KafkaListener methods processed on bean '\" + beanName + \"': \" + annotatedMethods); } } if (hasClassLevelListeners) { processMultiMethodListeners(classLevelListeners, multiMethods, bean, beanName); } } return bean; } 1.1.1.1.1.2、processKafkaListener protected void processKafkaListener(KafkaListener kafkaListener, Method method, Object bean, String beanName) { Method methodToUse = checkProxy(method, bean); // new endpoint实例，表示一个kafkaListener切入点 MethodKafkaListenerEndpoint\u003cK, V\u003e endpoint = new MethodKafkaListenerEndpoint\u003c\u003e(); endpoint.setMethod(methodToUse); // 处理Listener processListener(endpoint, kafkaListener, bean, methodToUse, beanName); } 1.1.1.1.1.3、processListener protected void processListener(MethodKafkaListenerEndpoint\u003c?, ?\u003e endpoint, KafkaListener kafkaListener, Object bean, Object adminTarget, String beanName) { String beanRef = kafkaListener.beanRef(); if (StringUtils.hasText(beanRef)) { this.listenerScope.addListener(beanRef, bean); } endpoint.setBean(bean); endpoint.setMessageHandlerMethodFactory(this.messageHandlerMethodFactory); endpoint.setId(getEndpointId(kafkaListener)); endpoint.setGroupId(getEndpointGroupId(kafkaListener, endpoint.getId())); endpoint.setTopicPartitions(resolveTopicPartitions(kafkaListener)); endpoint.setTopics(resolveTopics(kafkaListener)); endpoint.setTopicPattern(resolvePattern(kafkaListener)); endpoint.setClientIdPrefix(resolveExpressionAsString(kafkaListener.clientIdPrefix(), \"clientIdPrefix\")); String group = kafkaListener.containerGroup(); ... String concurrency = kafkaListener.concurrency(); ... resolveKafkaProperties(endpoint, kafkaListener.properties()); // 设置 KafkaListenerContainerFactory KafkaListenerContainerFactory\u003c?\u003e factory = null; String containerFactoryBeanName = resolve(kafkaListener.containerFactory()); ... endpoint.setBeanFactory(this.beanFactory); ... // 将endpoint 登记到 KafkaListenerEndpointRegistrar 中，前面一大段代码都是设置en","date":"2019-10-27","objectID":"/springdata_kafka/:1:1","tags":["kafka"],"title":"SpringData_kafka","uri":"/springdata_kafka/"},{"categories":["spring"],"content":"1.1.3、 KafkaMessageListenerContainer KafkaMessageListenerContainer 该类封装了KafkaConsumer ，主要作用是连接kafka，并且poll数据，然后根据配置处理数据。 run 方法 @Override public void run() { this.consumerThread = Thread.currentThread(); if (this.genericListener instanceof ConsumerSeekAware) { ((ConsumerSeekAware) this.genericListener).registerSeekCallback(this); } if (this.transactionManager != null) { ProducerFactoryUtils.setConsumerGroupId(this.consumerGroupId); } this.count = 0; this.last = System.currentTimeMillis(); // 初始消费者线程绑定分区 initAsignedPartitions(); while (isRunning()) { try { // 拉取数据并且调用listener注解的业务方法处理数据 pollAndInvoke(); } catch (@SuppressWarnings(UNUSED) WakeupException e) { // Ignore, we're stopping } catch (NoOffsetForPartitionException nofpe) { this.fatalError = true; ListenerConsumer.this.logger.error(\"No offset and no reset policy\", nofpe); break; } catch (Exception e) { handleConsumerException(e); } catch (Error e) { // NOSONAR - rethrown Runnable runnable = KafkaMessageListenerContainer.this.emergencyStop; if (runnable != null) { runnable.run(); } this.logger.error(\"Stopping container due to an Error\", e); wrapUp(); throw e; } } wrapUp(); } pollAndInvoke 方法 protected void pollAndInvoke() { // 非自动提交并且(ack == COUNT || COUNT_TIME)，处理co if (!this.autoCommit \u0026\u0026 !this.isRecordAck) { // 该方法会提交ack，但是会判断是否该线程消费者线程，还会判断ack mode.只有非手动提交的这里才会提交。并且注意，提交线程一但提交，因为是多线程消费，会出现消费顺序不一致。 processCommits(); } // seek 指定消费者偏移量 processSeeks(); checkPaused(); // 开始拉取数据，指定超时时间 ConsumerRecords\u003cK, V\u003e records = this.consumer.poll(this.pollTimeout); this.lastPoll = System.currentTimeMillis(); checkResumed(); debugRecords(records); if (records != null \u0026\u0026 records.count() \u003e 0) { if (this.containerProperties.getIdleEventInterval() != null) { this.lastReceive = System.currentTimeMillis(); } // 调用@KafkaListener注解的业务代码，方法内部会判断是否有事务 invokeListener(records); } else { checkIdle(); } } ","date":"2019-10-27","objectID":"/springdata_kafka/:1:2","tags":["kafka"],"title":"SpringData_kafka","uri":"/springdata_kafka/"},{"categories":["spring"],"content":"1.2 要点 多线徎多记录消费顺序会不一致，手动提交偏移量会导致数据数据丢失 一个@KafkaListener会启动concurrency个消费者；concurrency应该小于等于partitions数。 ","date":"2019-10-27","objectID":"/springdata_kafka/:2:0","tags":["kafka"],"title":"SpringData_kafka","uri":"/springdata_kafka/"},{"categories":["spring"],"content":"引用 [1] spring-kafka源码解析 https://blog.csdn.net/qq_26323323/article/details/84938892 ","date":"2019-10-27","objectID":"/springdata_kafka/:3:0","tags":["kafka"],"title":"SpringData_kafka","uri":"/springdata_kafka/"},{"categories":["中间件"],"content":"sentry配置 使用sentry 进行异常报警 sentry安装 sentry 目前推荐使用docker安装，docker-compose启动 官方安装链接：https://docs.sentry.io/server/installation 添加maven依赖 \u003cdependency\u003e \u003cgroupId\u003ecom.getsentry.raven\u003c/groupId\u003e \u003cartifactId\u003eraven-logback\u003c/artifactId\u003e \u003cversion\u003e8.0.3\u003c/version\u003e \u003c/dependency\u003e 配置logback \u003cappender name=\"Sentry\" class=\"com.getsentry.raven.logback.SentryAppender\"\u003e \u003cdsn\u003ehttps://username:password@sentry.abc.com/117\u003c/dsn\u003e \u003cfilter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"\u003e \u003clevel\u003eERROR\u003c/level\u003e \u003c/filter\u003e \u003c/appender\u003e ","date":"2018-11-27","objectID":"/sentry/:0:0","tags":["sentry"],"title":"sentry在java中用法","uri":"/sentry/"},{"categories":["中间件"],"content":"skywalking-agent源码分析 ","date":"2018-09-27","objectID":"/skywalking/:0:0","tags":["apm"],"title":"skywalking源码分析","uri":"/skywalking/"},{"categories":["中间件"],"content":"执行顺序图 该顺序图大体内容都有了，缺少最后一步的BootService的生命周期调用，即下面的接口。在这个接口中会有一些服务调用，比如向Gprc注册发送的流程* public interface BootService { void prepare() throws Throwable; void boot() throws Throwable; void onComplete() throws Throwable; void shutdown() throws Throwable; } ","date":"2018-09-27","objectID":"/skywalking/:1:0","tags":["apm"],"title":"skywalking源码分析","uri":"/skywalking/"},{"categories":["中间件"],"content":"分析 分析中主要讲class Transformer implements AgentBuilder.Transformer这个实现类中的transform方法 transform方法中调用pluginFinder的find方法。 @Override public DynamicType.Builder\u003c?\u003e transform(DynamicType.Builder\u003c?\u003e builder, TypeDescription typeDescription, ClassLoader classLoader, JavaModule module) { List\u003cAbstractClassEnhancePluginDefine\u003e pluginDefines = pluginFinder.find(typeDescription, classLoader); if (pluginDefines.size() \u003e 0) { DynamicType.Builder\u003c?\u003e newBuilder = builder; EnhanceContext context = new EnhanceContext(); for (AbstractClassEnhancePluginDefine define : pluginDefines) { DynamicType.Builder\u003c?\u003e possibleNewBuilder = define.define(typeDescription, newBuilder, classLoader, context); if (possibleNewBuilder != null) { newBuilder = possibleNewBuilder; } } if (context.isEnhanced()) { logger.debug(\"Finish the prepare stage for {}.\", typeDescription.getName()); } return newBuilder; } logger.debug(\"Matched class {}, but ignore by finding mechanism.\", typeDescription.getTypeName()); return builder; } 找到所有的AbstractClassEnhancePluginDefine实现类，在迭代所有子类，调用子类的define方法。在define方法中最主要的方法是enhance。该方法是抽象方法，由子类ClassEnhancePluginDefine中调用enhanceClass和enhanceInstance。 @Override protected DynamicType.Builder\u003c?\u003e enhance(TypeDescription typeDescription, DynamicType.Builder\u003c?\u003e newClassBuilder, ClassLoader classLoader, EnhanceContext context) throws PluginException { newClassBuilder = this.enhanceClass(typeDescription, newClassBuilder, classLoader); newClassBuilder = this.enhanceInstance(typeDescription, newClassBuilder, classLoader, context); return newClassBuilder; } 调用enhanceClass方法。该方法第一步也是最主要的一步调用getStaticMethodsInterceptPoints。这个方法也是抽象方法。这个方法由具体的插件实现。比如LoadBalancedConnectionProxyInstrumentation。这个类中会调用getConstructorsInterceptPoints来获取哪些方法是需要被拦截的。并且通过getMethodsInterceptor方法返回具体的实现类。这个方法类似AOP做切面处理。 private DynamicType.Builder\u003c?\u003e enhanceClass(TypeDescription typeDescription, DynamicType.Builder\u003c?\u003e newClassBuilder, ClassLoader classLoader) throws PluginException { StaticMethodsInterceptPoint[] staticMethodsInterceptPoints = getStaticMethodsInterceptPoints(); String enhanceOriginClassName = typeDescription.getTypeName(); if (staticMethodsInterceptPoints == null || staticMethodsInterceptPoints.length == 0) { return newClassBuilder; } for (StaticMethodsInterceptPoint staticMethodsInterceptPoint : staticMethodsInterceptPoints) { String interceptor = staticMethodsInterceptPoint.getMethodsInterceptor(); if (StringUtil.isEmpty(interceptor)) { throw new EnhanceException(\"no StaticMethodsAroundInterceptor define to enhance class \" + enhanceOriginClassName); } if (staticMethodsInterceptPoint.isOverrideArgs()) { newClassBuilder = newClassBuilder.method(isStatic().and(staticMethodsInterceptPoint.getMethodsMatcher())) .intercept( MethodDelegation.withDefaultConfiguration() .withBinders( Morph.Binder.install(OverrideCallable.class) ) .to(new StaticMethodsInterWithOverrideArgs(interceptor)) ); } else { newClassBuilder = newClassBuilder.method(isStatic().and(staticMethodsInterceptPoint.getMethodsMatcher())) .intercept( MethodDelegation.withDefaultConfiguration() .to(new StaticMethodsInter(interceptor)) ); } } return newClassBuilder; } LoadBalancedConnectionProxyInstrumentation的实现方法 @Override protected StaticMethodsInterceptPoint[] getStaticMethodsInterceptPoints() { return new StaticMethodsInterceptPoint[] { new StaticMethodsInterceptPoint() { @Override public ElementMatcher\u003cMethodDescription\u003e getMethodsMatcher() { return named(\"createProxyInstance\"); } //返回具体的拦截器实现类 @Override public String getMethodsInterceptor() { return METHOD_INTERCEPTOR; } @Override public boolean isOverrideArgs() { return false; } } }; } 调用enhanceInstance方法。这个方法中最主要的是调用下面的两个方法，一个是对构造器进行切面拦截，另一个是对实例对象中的方法进行拦截。比如InvocableHandlerInstrumentation这个实现类是对Springmvc中InvocableHandlerMethod的invokeForRequest进行拦截，具体的拦截器类是org.apache.skywalking.apm.plugin.spring.mvc.commons.interceptor.InvokeForRequestInterceptor。 ConstructorInterceptPoint[] constructorInterceptPoints = getConstructorsInterceptPoints(); InstanceMethodsInt","date":"2018-09-27","objectID":"/skywalking/:2:0","tags":["apm"],"title":"skywalking源码分析","uri":"/skywalking/"},{"categories":["中间件"],"content":"总结重要实现及接口 该流程过程中主要是在enhance方法内进程代理（拦截器）的创建。具体的拦截器接口有InstanceConstructorInterceptor,InstanceMethodsAroundInterceptor,StaticMethodsAroundInterceptor,然后在上面三个接口的子类中有些会实现EnhancedInstance接口进行动态属性添加。而最主要的三个接口的实现类的调用是通过newClassBuilder.method(junction).intercept方法内部调用的。 如下图： ","date":"2018-09-27","objectID":"/skywalking/:3:0","tags":["apm"],"title":"skywalking源码分析","uri":"/skywalking/"},{"categories":["java基础"],"content":"ClassLoader 介绍 ","date":"2017-10-27","objectID":"/classloader/:0:0","tags":["classloader"],"title":"classloader简要概述","uri":"/classloader/"},{"categories":["java基础"],"content":"什么是ClassLoader java源码编译出来是一个个的.class文件，而ClassLoader的作用是将一个个的.class文件加载到jvm中。 ","date":"2017-10-27","objectID":"/classloader/:1:0","tags":["classloader"],"title":"classloader简要概述","uri":"/classloader/"},{"categories":["java基础"],"content":"ClassLoader加载机制 Java中默认提供了三个ClassLoader BootStrap ClassLoader Extension ClassLoader App ClassLoader ","date":"2017-10-27","objectID":"/classloader/:2:0","tags":["classloader"],"title":"classloader简要概述","uri":"/classloader/"},{"categories":["java基础"],"content":"BootStrap ClassLoader 启动类加载器 作用：java中是最顶层的加载器，负责加载jdk的核心类库：rt.jar、resources.jar、charsets.jar public class BootStrapTest { public static void main(String[] args) { URL[] urls = sun.misc.Launcher.getBootstrapClassPath().getURLs(); for (int i = 0; i \u003c urls.length; i++) { System.out.println(urls[i].toExternalForm()); } } } 以上程序可以得到BootStrap ClassLoader从哪些地址加载了哪些jar包 file://Users/xxx/java/jdk1.8.0_60/jre/lib/resources.jar file://Users/xxx/java/jdk1.8.0_60/jre/lib/rt.jar file://Users/xxx/java/jdk1.8.0_60/jre/lib/sunrsasign.jar file://Users/xxx/java/jdk1.8.0_60/jre/lib/jsse.jar file://Users/xxx/java/jdk1.8.0_60/jre/lib/charsets.jar file://Users/xxx/java/jdk1.8.0_60/jre/lib/jfr.jar file://Users/xxx/java/jdk1.8.0_60/jre/classes 该结果和System.out.println(System.getProperty(\"sun.boot.class.path\"));输出结果一致 ","date":"2017-10-27","objectID":"/classloader/:2:1","tags":["classloader"],"title":"classloader简要概述","uri":"/classloader/"},{"categories":["java基础"],"content":"Extension ClassLoader 扩展类加载器 作用：负责加载java的扩展类库，默认加载$JAVA_HOME/jre/lib/ext/目录下的所有jar ","date":"2017-10-27","objectID":"/classloader/:2:2","tags":["classloader"],"title":"classloader简要概述","uri":"/classloader/"},{"categories":["java基础"],"content":"App ClassLoader 系统类加载器 作用：负责加载classpath目录下的所有jar和class文件。主要负责加载程序员自己编码的java应用代码 可以通过ClassLoader.getSystemClassLoader()方法获取 除了以上三种加载器之外，程序员可以自己实现自定义加载器，方式：继承java.lang.ClassLoader类。比如使用该方式可以对源class文件进行混淆加密，通过自定义ClassLoader进行解密 默认三种加载器之前存在父子关系（注意不是继承）AppClassLoader -\u003e ExtensionClassLoader -\u003e BootStrapClassLoader 。通过getParent()方法获取父类加载器（父类加载器使用包含关系引用） ","date":"2017-10-27","objectID":"/classloader/:2:3","tags":["classloader"],"title":"classloader简要概述","uri":"/classloader/"},{"categories":["java基础"],"content":"ClassLoader加载原理 摘自https://zhuanlan.zhihu.com/p/25493756 ","date":"2017-10-27","objectID":"/classloader/:3:0","tags":["classloader"],"title":"classloader简要概述","uri":"/classloader/"},{"categories":["java基础"],"content":"原理介绍 ClassLoader使用的是双亲委托模型来搜索类的，每个ClassLoader实例都有一个父类加载器的引用（不是继承的关系，是一个包含的关系），虚拟机内置的类加载器（Bootstrap ClassLoader）本身没有父类加载器，但可以用作其它ClassLoader实例的的父类加载器。当一个ClassLoader实例需要加载某个类时，它会试图亲自搜索某个类之前，先把这个任务委托给它的父类加载器，这个过程是由上至下依次检查的，首先由最顶层的类加载器Bootstrap ClassLoader试图加载，如果没加载到，则把任务转交给Extension ClassLoader试图加载，如果也没加载到，则转交给App ClassLoader 进行加载，如果它也没有加载得到的话，则返回给委托的发起者，由它到指定的文件系统或网络等URL中加载该类。如果它们都没有加载到这个类时，则抛出ClassNotFoundException异常。否则将这个找到的类生成一个类的定义，并将它加载到内存当中，最后返回这个类在内存中的Class实例对象。 ","date":"2017-10-27","objectID":"/classloader/:3:1","tags":["classloader"],"title":"classloader简要概述","uri":"/classloader/"},{"categories":["java基础"],"content":"为什么使用双亲委托模型？ 因为这样可以避免重复加载，当父亲已经加载了该类的时候，就没有必要 ClassLoader再加载一次。考虑到安全因素，我们试想一下，如果不使用这种委托模式，那我们就可以随时使用自定义的String来动态替代java核心api中定义的类型，这样会存在非常大的安全隐患，而双亲委托的方式，就可以避免这种情况，因为String已经在启动时就被引导类加载器（Bootstrcp ClassLoader）加载，所以用户自定义的ClassLoader永远也无法加载一个自己写的String，除非你改变JDK中ClassLoader搜索类的默认算法。 ","date":"2017-10-27","objectID":"/classloader/:3:2","tags":["classloader"],"title":"classloader简要概述","uri":"/classloader/"},{"categories":["java基础"],"content":"但是JVM在搜索类的时候，又是如何判定两个class是相同的呢？ JVM在判定两个class是否相同时，不仅要判断两个类名是否相同，而且要判断是否由同一个类加载器实例加载的。只有两者同时满足的情况下，JVM才认为这两个class是相同的。就算两个class是同一份class字节码，如果被两个不同的ClassLoader实例所加载，JVM也会认为它们是两个不同class。比如网络上的一个Java类org.classloader.simple.NetClassLoaderSimple，javac编译之后生成字节码文件NetClassLoaderSimple.class，ClassLoaderA和ClassLoaderB这两个类加载器并读取了NetClassLoaderSimple.class文件，并分别定义出了java.lang.Class实例来表示这个类，对于JVM来说，它们是两个不同的实例对象，但它们确实是同一份字节码文件，如果试图将这个Class实例生成具体的对象进行转换时，就会抛运行时异常java.lang.ClassCaseException，提示这是两个不同的类型。现在通过实例来验证上述所描述的是否正确： ","date":"2017-10-27","objectID":"/classloader/:3:3","tags":["classloader"],"title":"classloader简要概述","uri":"/classloader/"},{"categories":["运维"],"content":"iptables 简单入门介绍 iptables 是组成Linux平台下的包过滤防火墙。提到iptables就不能不提到netfliter。这里可以简单理解iptables是客户端，而真正进行包过滤的是内核中的netfliter组件。 ","date":"2017-09-27","objectID":"/iptables/:0:0","tags":["iptables"],"title":"iptables入门","uri":"/iptables/"},{"categories":["运维"],"content":"一、网络基础知识 ","date":"2017-09-27","objectID":"/iptables/:1:0","tags":["iptables"],"title":"iptables入门","uri":"/iptables/"},{"categories":["运维"],"content":"1.1、网络分层模型 还有一个四层模型，是将五层模型中的数据链路及物理层合并为网络接口层(链路层) 物理层：主要负责在物理载体上的数据包传输，如 WiFi，以太网，光纤，电话线等。 数据链路层：主要负责链路层协议解析（主要为以太网帧）。 网络层：主要负责 IP 协议（包括 IPv4 和 IPv6）解析。 传输层：负责传输层协议解析（主要为 TCP，UDP 等） 应用层：传输层以上我们均归类为应用层，主要包括各类应用层协议，如我们常用的 HTTP，FTP，SMTP，DNS，DHCP 等。 ","date":"2017-09-27","objectID":"/iptables/:1:1","tags":["iptables"],"title":"iptables入门","uri":"/iptables/"},{"categories":["运维"],"content":"1.2、几种网络协议 TCP/IP 是互联网。≤相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。 ICMP：网际报文控制协议，比如常用的ping命令，traceroute命令 用于IP主机、路由器之间传递控制消息。控制消息是在网络通不通、主机是否可达、路由是否可用等网络本身的消息。这些控制消息虽然不传输用户数据，但是对于用户数据的传递起着重要的作用。 IGMP：互联网组管理协议。 IP组播通信的特点是报文从一个源发出，被转发到一组特定的接收者。但在组播通信模型中，发送者不关注接收者的位置信息，只是将数据发送到约定的目的组播地址。要使组播报文最终能够到达接收者，需要某种机制使连接接收者网段的组播路由器能够了解到该网段存在哪些组播接收者，同时保证接收者可以加入相应的组播组中。IGMP就是用来在接收者主机和与其所在网段直接相邻的组播路由器之间建立、维护组播组成员关系的协议。 ARP/RARP：地址解析协议/反地址解析协议。 根据IP地址获取物理地址/根据物理地址获取IP地址，同一局域网下网络传输使用。 TCP：传输控制协议 三次握手，四次挥手。面向有连接，可靠传输 UDP：用户数据报协议 无连接，不可靠 UDP TCP 是否连接 无连接 面向连接 是否可靠 不可靠传输，不使用流量控制和拥塞控制 可靠传输，使用流量控制和拥塞控制 连接对象个数 支持一对一，一对多，多对一和多对多交互通信 只能是一对一通信 传输方式 面向报文 面向字节流 首部开销 首部开销小，仅8字节 首部最小20字节，最大60字节 适用场景 适用于实时应用（IP电话、视频会议、直播等） 适用于要求可靠传输的应用，例如文件传输 ","date":"2017-09-27","objectID":"/iptables/:1:2","tags":["iptables"],"title":"iptables入门","uri":"/iptables/"},{"categories":["运维"],"content":"二、Iptables/netfliter 要学会使用iptables和理解netfliter，就必须弄懂数据包在设备上的传输流程，及在每一个阶段所能做的事。 ","date":"2017-09-27","objectID":"/iptables/:2:0","tags":["iptables"],"title":"iptables入门","uri":"/iptables/"},{"categories":["运维"],"content":"2.1、Packet传输流程图 ","date":"2017-09-27","objectID":"/iptables/:2:1","tags":["iptables"],"title":"iptables入门","uri":"/iptables/"},{"categories":["运维"],"content":"2.2、iptables 表（tables） filter：一般的过滤功能 nat：用于nat功能（端口映射，地址映射等） mangle：用于对特定数据包的修改 Raw：有限级最高，设置raw时一般是为了不再让iptables做数据包的链接跟踪处理，提高性能RAW 表只使用在PREROUTING链和OUTPUT链上,因为优先级最高，从而可以对收到的数据包在连接跟踪前进行处理。一但用户使用了RAW表,在某个链 上,RAW表处理完后,将跳过NAT表和 ip_conntrack处理,即不再做地址转换和数据包的链接跟踪处理了。RAW表可以应用在那些不需要做nat的情况下，以提高性能。如大量访问的web服务器，可以让80端口不再让iptables做数据包的链接跟踪处理，以提高用户的访问速度。 链（chains） PREROUTING：数据包进入路由表之前 INPUT：通过路由表后目的地为本机 FORWARD：通过路由表后，目的地不为本机 OUTPUT：由本机产生，向外转发 POSTROUTIONG：发送到网卡接口之前 规则（rules） *nat :PREROUTING ACCEPT [60:4250] :INPUT ACCEPT [31:1973] :OUTPUT ACCEPT [3:220] :POSTROUTING ACCEPT [3:220] -A PREROUTING -p tcp -m tcp --dport 8088 -j DNAT --to-destination 192.168.1.160:80 //PREROUTING规则都放在上面 -A PREROUTING -p tcp -m tcp --dport 33066 -j DNAT --to-destination 192.168.1.161:3306 -A POSTROUTING -d 192.168.1.160/32 -p tcp -m tcp --sport 80 -j SNAT --to-source 192.168.1.7 //POSTROUTING规则都放在下面 -A POSTROUTING -d 192.168.1.161/32 -p tcp -m tcp --sport 3306 -j SNAT --to-source 192.168.1.7 ..... *filter :INPUT ACCEPT [16:7159] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [715:147195] -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -p tcp -m state --state NEW -m tcp --dport 8088 -j ACCEPT -A INPUT -p tcp -m state --state NEW -m tcp --dport 33066 -j ACCEPT ","date":"2017-09-27","objectID":"/iptables/:2:2","tags":["iptables"],"title":"iptables入门","uri":"/iptables/"},{"categories":["运维"],"content":"2.3、使用 iptables [-t 表名] 命令选项 ［链名］ ［条件匹配］ ［-j 目标动作或跳转］ 查看iptables命令 iptables --help 2.3.1、操作filter表 禁用ping iptables -t filter -A INPUT -p icmp --icmp-type 8 -s 0.0.0.0/0 -j DROP 开通一段ip的端口 iptables -t filter -I YZW -m iprange --src-range 192.168.110.236-192.168.110.237 -p tcp -m multiport --dport 3011,3012,3301,8005,3302,3015,3016,20930 -j ACCEPT 保存iptables iptables-save \u003e /etc/sysconfig/iptables-yzw 2.3.2、操作nat表 比如访问本机（192.168.1.7）的8088端口转发到192.168.1.160的80端口； DNAT iptables -t nat -A PREROUTING -p tcp -m tcp --dport 8088 -j DNAT --to-destination 192.168.1.160:80 SNAT iptables -t nat -A POSTROUTING -d 192.168.1.160/32 -p tcp -m tcp --sport 80 -j SNAT --to-source 192.168.1.7 MASQUERADE iptables -t nat -A POSTROUTING -s 192.168.1.7/255.255.255.0 -o eth0 -j MASQUERADE ","date":"2017-09-27","objectID":"/iptables/:2:3","tags":["iptables"],"title":"iptables入门","uri":"/iptables/"},{"categories":["运维"],"content":"引用 【1】Iptables 规则用法小结 【2】状态机制 ","date":"2017-09-27","objectID":"/iptables/:3:0","tags":["iptables"],"title":"iptables入门","uri":"/iptables/"},{"categories":["java基础"],"content":"HashMap源码解析 ","date":"2016-04-27","objectID":"/hashmap/:0:0","tags":["hashmap"],"title":"HashMap源码分析","uri":"/hashmap/"},{"categories":["java基础"],"content":"构造方法 ==无参构造方法== /** * Constructs an empty \u003ctt\u003eHashMap\u003c/tt\u003e with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } 看方法注释可知：无参构造方法默认capacity = 16 、loadFactor = 0.75 ==带有初始容量的构造方法== /** * Constructs an empty \u003ctt\u003eHashMap\u003c/tt\u003e with the specified initial * capacity and the default load factor (0.75). * * @param initialCapacity the initial capacity. * @throws IllegalArgumentException if the initial capacity is negative. */ public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } 初始容量负数扔IllegalArgumentException异常，loadFactor = 0.75 ==带有初始容量及负载因子的构造方法== /** * Constructs an empty \u003ctt\u003eHashMap\u003c/tt\u003e with the specified initial * capacity and load factor. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity \u003c 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity \u003e MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor \u003c= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); } static final int MAXIMUM_CAPACITY = 1 \u003c\u003c 30; ==另一个Map作为参数== /** * Constructs a new \u003ctt\u003eHashMap\u003c/tt\u003e with the same mappings as the * specified \u003ctt\u003eMap\u003c/tt\u003e. The \u003ctt\u003eHashMap\u003c/tt\u003e is created with * default load factor (0.75) and an initial capacity sufficient to * hold the mappings in the specified \u003ctt\u003eMap\u003c/tt\u003e. * * @param m the map whose mappings are to be placed in this map * @throws NullPointerException if the specified map is null */ public HashMap(Map\u003c? extends K, ? extends V\u003e m) { this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); } ","date":"2016-04-27","objectID":"/hashmap/:1:0","tags":["hashmap"],"title":"HashMap源码分析","uri":"/hashmap/"},{"categories":["java基础"],"content":"tableSizeFor 返回大于输入参数且最近的2的整数次幂的数。比如10，则返回16。 参考https://www.jianshu.com/p/cbe3f22793be /** * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) { int n = cap - 1; n |= n \u003e\u003e\u003e 1; n |= n \u003e\u003e\u003e 2; n |= n \u003e\u003e\u003e 4; n |= n \u003e\u003e\u003e 8; n |= n \u003e\u003e\u003e 16; return (n \u003c 0) ? 1 : (n \u003e= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } 通过无符号右移再异或取到高位全是1，最后再加1. ","date":"2016-04-27","objectID":"/hashmap/:2:0","tags":["hashmap"],"title":"HashMap源码分析","uri":"/hashmap/"},{"categories":["java基础"],"content":"put 存放值 public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } ","date":"2016-04-27","objectID":"/hashmap/:3:0","tags":["hashmap"],"title":"HashMap源码分析","uri":"/hashmap/"},{"categories":["java基础"],"content":"hash(key) 求key的hash值 static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u003e\u003e\u003e 16); } (h = key.hashCode()) ^ (h »\u003e 16) 为什么需要h »\u003e 16位？扰动函数，减轻了哈希冲突 ","date":"2016-04-27","objectID":"/hashmap/:3:1","tags":["hashmap"],"title":"HashMap源码分析","uri":"/hashmap/"},{"categories":["java基础"],"content":"putValue ","date":"2016-04-27","objectID":"/hashmap/:3:2","tags":["hashmap"],"title":"HashMap源码分析","uri":"/hashmap/"},{"categories":["java基础"],"content":"resize ","date":"2016-04-27","objectID":"/hashmap/:3:3","tags":["hashmap"],"title":"HashMap源码分析","uri":"/hashmap/"},{"categories":["中间件"],"content":"《redis设计与实现》学习记录（一） 一直在使用redis，使用熟练，但是好像一直没有关注过底层如何实现，最近准备关注一下底层的数据结构，于是买了本《redis设计与实现》第二版，晚上睡前抽时间看看，让自己在脑海中对redis有一个更清晰的认识。以下内容是自己看书的一些记录。因此有些内容摘自书中。 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:0:0","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"数据结构 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:1:0","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"字符串 redis自定义了自己的字符串数据结构。（SDS=简单动态字符串） struct{ int len;//所保存的字符数长度 int free;//未使用字节数量 char buf[];//用于保存字符串，并且使用‘\\0’结尾，与c保持一致，方便使用c的部分函数 } 优势 获取字符串长度（STRLEN函数）时间复杂度为O(1)，因为有len这个属性。而C语言没有这个属性，要获取长度需要遍历数组。 杜绝缓冲区溢出或者泄露：当字符串拼接或者添加时，C语言默认字符数组容量足够，而SDS会默认检查free是否足够。 空间预分配：对字符串增加，如果SDS.len小于1M,分配free和len同样大小的空间；如果\u003e1M，将分配1M的free空间。 惰性空间释放：缩减SDS，并且立即释放内存，而是将释放大小增加到free空间。不用担心内存浪费，因为SDS提供了api在真正需要释放空间时执行。 目的：减小频繁的内存分配，即减小了程序的时间开销，增加性能。 二进制安全：C以‘\\0’为结尾，如果存储二进制图片等数据会认为‘\\0’即结束。但是SDS有一个len属性，会读取len属性大小（+1）的长度才会结束，即安全的二进制存储。 兼容部分C语言函数：SDS同样以‘\\0’结尾，即遵询了部分C的结构，可以重用部分C函数，不必重写。 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:1:1","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"链表 自定义链表结构： typedef struct listNode{ struct listNode *prev;//前置节点 struct listNode *next;//后置节点 void *value;//链表结点数据 }listNode; typedef struct list{ listNode *head;//头节点 listNode *tail;//尾节点 unsigned long len;//链表节点数量 void *(*dup)(void *ptr);//节点复制函数 void *(*free)(void *ptr);//节点释放函数 int (*match)(void *ptr,void *key);//节点对比函数 }list; 优势： 双端链表：获取前置节点与后置节点时间复杂度O(1)，通过prev和next指针。 无环：next=null即尾节点，prev=null即头节点 链表长度计数器：len属性获取节点长度时间复杂度O(1) 多态：可以根据value的类型为内置的三个函数指向具体类型的函数。如value是string,则三个函数为操作string的函数。 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:1:2","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"字典 redis字典使用哈希表为底层实现。一个哈希表里有多个hash节点，一个节点保存一个k-v（键值对） typedef struct dictht{ //dict hash table dictEntry **table;//哈希表 *数组* unsigned long size;//哈希表大小 unsigned long sizemask;//hash table大小掩码，计算索引值,总等于size-1,用户计算键放于table哪个索引上 unsigned long used;//hash table已使用节点大小 }dictht; typedef struct dictEntry{ void *key;//键 union{ void *val; uint64_tu64; int64_ts64; }v; //值 struct dictEntry *next; //指向下个哈希表节点，形成链表。拉链表解决hash冲突。 } typedef struct dict{ dictType *type; //类型特定函数 void *privdaa;//私有数据 dictht ht[2]; // 哈希表，大小为2，一个存储，另一个用于rehash时 in rehashidx; //rehash 索引，不rehash 值 = -1 } type属性和privdata属性，用于不同类型设置不同的函数 ht 大小为2，ht[1]只会在对ht[0]进行rehash时使用 使用了链地址法解决hash冲突dictEntry.next指针存储下一个节点 rehash 扩展操作：为ht[1]分配ht[0].used * 2的2^n 收缩操作：为ht[1]分配ht[0].used 的2^n 时机，以下任一个满足： 服务器没有执行BGSAVE或者BGREWRITEAOF，并且load_factor\u003e=1 正在执行BGSAVE或者BGREWRITEAOF，并且load_factor\u003e=5 load_factor = ht[0].used / ht[0].size Load_facotr \u003c 0.1自动执行收缩 rehash并不是一次性完成，而是多次，激进式完成。避免当数据量大时，计算量导致停止服务。 rehash时的查询先ht[0]再ht[1],新增直接操作ht[1] ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:1:3","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"跳跃表 跳跃表（skiplist）是一种有序数据结构。平均O(logN)、最坏O(N)时间复杂度。redis使用跳跃表作为有序集成键的底层实现之一。 typedef struct zskiplistNode{ struct zskiplistLevel{ struct zskiplistNode *forward;//前进指针 unsigned int span;//跨度 }level[]; //层 struct zskiplistNode *backward; double score;//分值 robj *obj; }zskiplistNode; typedef struct zskiplist{ struct skiplistNode *header,*tail;//头尾指针 unsigned long length; int level; } 跳跃表是有序集合的底层实现之一 zskiplist保存跳跃表信息，zskiplistNode保存节点信息 跳跃表按照分值大小排序 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:1:4","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"整数集合（intset） 整数集合是redis保存整数值的集合底层数据结构。 typedef struct intset{ uint32_t encoding;//编码方式 uint32_t length;//集合元素个数 int8_t contents[];//保存的元素 }intset; 虽然intset的content属性类型为int8_t，但是content并不保存int8_t类型的值，而是取决于encoding类型的值。 升级：当向contents添加一个类型比当前值的最大类型还大时，比如现在存放int16型的数据，但是下一个存放int32类型数据，那么intset集合会先进行升级。扩展contents的底层数据字节长度，再把之前的值改变成新的字节长度。最后更改encoding编码。因此intset添加元素的时间复杂度为O(N) 不支持降级 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:1:5","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"压缩列表 压缩列表是列表键及哈希键的底层实现之一，当列表键少量并且是小整数或者短字符时使用。 zlbytes表示压缩列表占用内存总字节数 zltail表示压缩列表表尾距离开始节点的偏移量 zllen表示节点数量 entry节点数据 zlend标记压缩列表结尾 压缩列表节点数据结构，即上图的entry节点： 描述： Previous_entry_length前一节点的长度，如果是\u003c 254字节，使用1字节长保存，如果\u003e=254字节，使用5字节保存，并且属性第一字节设置为OxFE=254。 encoding content内容的编码 content 真正保存的内容 影响 ： 压缩列表会引起连锁更新：因为previous_entry_length保存了前一个节点的长度。如果介于250~254之前，新增一个节点在前面并且是大于254的，接下来的节点的previous_entry_length会调整为5字节，又会影响下一个节点，连锁反应。 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:1:6","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"对象 redis并不直接使用上面的提到的数据结构来构建数据库。而是使用上面提到的数据结构构建了五种redis对象：字符串对象 列表对象 哈希对象 集合对象 有序集合对象。 使用对象的好处是可以对同一种对象底层使用不同的实现。并且根据对象类型执行不同的命令。优化对象在不同的场景下的使用效率。 redis对象实现了引用计数对内存进行回收。同时实现了对象的共享，以实现内存的节约。 redis对象带有访问时间记录信息，lru等属性，用于回收最近最少使用的对象。 typedef struct redisObject{ unsigned type:4; //对象类型 unsigned encodeing:4; //编码 void *ptr; //指向底层实现该对象的数据结构指针 }robj; type值有5种类型常量，分别对应redis的5种对象。 类型常量 对象的名称 REDIS_STRING 字符串对象 REDIS_LIST 列表对象 REDIS_HASH 哈希对象 REDIS_SET 集合对象 REDIS_ZSET 有序集合对象 可以使用type命令返回redis对象的值类型 set msg \"hello world\" type msg //返回string ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:2:0","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"编码和底层编码 对象的ptr指针指向了具体该对象的实现数据结构，而数据结构邮对象的encoding决定. 编码常量 编码对象底层数据结构 OBJECT ENCODING命令输出 REDIS_ENCODING_INT long类型整数 int REDIS_ENCODING_EMBSTR embtr编码的SDS embstr REDIS_ENCODING_RAW SDS raw REDIS_ENCODING_HT 字典 hashtable REDIS_ENCODING_LINKEDLIST 双端列表 linkedlist REDIS_ENCODING_ZIPLIST 压缩列表 ziplist REDIS_ENCODING_INTSET 整数集合 intset REDIS_ENCODING_SKIPLIST 跳跃表 skiplist ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:2:1","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"字符串对象 字符串对象编码可以是int、raw、embstr。 如果字符串对象保存的值类型为整数，那么encoding=REDIS_ENCODING_INT 如何值为字符串值，并且字符串值length \u003e 32 byte 那么encoding = REDIS_ENCODING_RAW。 相反如果length \u003c= 32 byte encoding = REDIS_ENCODING_EMBSTR 类型的编码不是永恒不变的，当原来保存的是int值，但是使用了APPEND函数添加了字符串，那么类型将变成raw。为什么不是embstr，是因为embstr没有修改函数，只有先将其转为raw才能执行修改操作。 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:2:2","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"列表对象 列表对象的编码可以是ziplist或者linkedlist 使用ziplist的条件如下： 列表对象保存的所有字符串元素长度 \u003c 64 byte 列表对象保存的元素数量 \u003c 512个 除此之外都使用linkedlist结构。 可以修改配置：list-max-ziplist-value 和list-max-ziplist-entries来修改上面的条件 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:2:3","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"哈希对象 哈希对象的编码是ziplist和hashtable 使用ziplist条件如下： 所有键值对的length \u003c 64 byte 所有键值对的数据\u003c 512 除此之外使用hashtable 可以修改配置：hash-max-ziplist-value 和hash-max-ziplist-entries来修改上面的条件 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:2:4","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"集合对象 集合对象使用的编码是：intset和hashtable 使用intset条件如下： 集合对象保存的所有元素都是整数 集合对象保存的元素个数 \u003c= 512 除此之外使用hashtable 可以修改配置：set-max-intset-value来修改上面的条件 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:2:5","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":["中间件"],"content":"有序集合对象 有序集合对象编码是：ziplist和skiplist 使用ziplist条件如下： 有序集合保存元素个数 \u003c 128 有序集合保存的所有元素长度 \u003c 64 byte 除此之外使用skiplist编码 可以修改配置：zset-max-ziplist-value 和zset-max-ziplist-entries来修改上面的条件 ","date":"2018-01-27","objectID":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/:2:6","tags":["redis"],"title":"01_redis设计与实现","uri":"/01_redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"categories":null,"content":"技能清单 熟练使用Java语言，熟悉golang和shell脚本开发 熟练掌握多线程开发，曾多次解决线上多线程问题 熟悉常见设计模式（单例、工厂、策略、模板、装饰、代理等） 熟悉主流web开发框架：springboot、mybatis、springmvc等 熟悉常见的中间件，比如redis、xxl-job、rabbitmq、kafka、elasticsearch等 熟悉微服务开发，dubbo、springcloud 熟悉JVM，能使用jdk提供的常见工具（jstack、jmap、jstat）并曾多次对线上问题进行排查和提供解决方案 熟悉Mysql，知道常见的SQL优化方式 熟悉HTML，CSS，JavaScript，vue，webpack等常用的web前端开发技术 熟悉docker及dockerfile的编写 熟练使用git，和jenkins配置及部署 了解k8s生态，大数据hadoop及其生态 学的太杂，还有很多… 联系方式 Email：scemsjyd@gmail.com Location：四川成都 ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"","uri":"/about/"}]